{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pymystem3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPLCYvireNU-",
        "outputId": "dcf74929-cc06-4282-bc7d-b4e1283846f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymystem3\n",
            "  Downloading pymystem3-0.2.0-py3-none-any.whl.metadata (5.5 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pymystem3) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2025.11.12)\n",
            "Downloading pymystem3-0.2.0-py3-none-any.whl (10 kB)\n",
            "Installing collected packages: pymystem3\n",
            "Successfully installed pymystem3-0.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from pymystem3 import Mystem\n",
        "import re\n",
        "import string\n",
        "\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
        "nltk.download('universal_tagset', quiet=True)\n",
        "\n",
        "mystem = Mystem()\n",
        "\n",
        "def read_file(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        return file.read()\n",
        "\n",
        "def clean_text(text, language):\n",
        "    \"\"\"Удаляет знаки препинания из текста\"\"\"\n",
        "    if language == 'english':\n",
        "        text = re.sub(r'[^\\w\\s\\']', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "    elif language == 'russian':\n",
        "        ru_punctuation = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~«»—…'\n",
        "        for char in ru_punctuation:\n",
        "            text = text.replace(char, ' ')\n",
        "        text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def tokenize_and_tag_english(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    tagged_tokens = nltk.pos_tag(tokens, tagset='universal')\n",
        "\n",
        "    result = []\n",
        "    for word, tag in tagged_tokens:\n",
        "        result.append(f\"{word.lower()}_{tag}\")\n",
        "\n",
        "    return result\n",
        "\n",
        "def tokenize_and_tag_russian(text):\n",
        "    analyzed = mystem.analyze(text)\n",
        "\n",
        "    result_tokens = []\n",
        "\n",
        "    for item in analyzed:\n",
        "        if item.get('text', '').strip() == '' or 'analysis' not in item:\n",
        "            continue\n",
        "        token_text = item['text'].lower()\n",
        "\n",
        "        if 'analysis' in item and item['analysis']:\n",
        "            first_analysis = item['analysis'][0]\n",
        "\n",
        "            if 'gr' in first_analysis:\n",
        "                pos_tag = first_analysis['gr'].split(',')[0].split('=')[0]\n",
        "\n",
        "                formatted_token = f\"{token_text}_{pos_tag}\"\n",
        "                result_tokens.append(formatted_token)\n",
        "            else:\n",
        "                result_tokens.append(token_text)\n",
        "        else:\n",
        "            result_tokens.append(token_text)\n",
        "\n",
        "    return result_tokens\n",
        "\n",
        "def process_text_file(file_path, language):\n",
        "    print(f\"\\nОбработка файла: {file_path}\")\n",
        "    print(f\"Язык: {language}\")\n",
        "\n",
        "    text = read_file(file_path)\n",
        "    print(f\"Длина исходного текста: {len(text)} символов\")\n",
        "    print(f\"Длина исходного текста: {len(text.split())} слов\")\n",
        "\n",
        "    # Очистка текста от знаков препинания\n",
        "    cleaned_text = clean_text(text, language)\n",
        "    print(f\"Длина текста после очистки: {len(cleaned_text)} символов\")\n",
        "    print(f\"Длина текста после очистки: {len(cleaned_text.split())} слов\")\n",
        "\n",
        "    if language == 'english':\n",
        "        tagged_tokens = tokenize_and_tag_english(cleaned_text)\n",
        "    elif language == 'russian':\n",
        "        tagged_tokens = tokenize_and_tag_russian(cleaned_text)\n",
        "    else:\n",
        "        print(f\"Неизвестный язык: {language}\")\n",
        "        return []\n",
        "\n",
        "    print(f\"Количество размеченных токенов: {len(tagged_tokens)}\")\n",
        "\n",
        "    print(\"\\nПервые 20 размеченных токенов:\")\n",
        "    for i, token in enumerate(tagged_tokens[:20]):\n",
        "        print(f\"{i+1:3}. {token}\")\n",
        "\n",
        "    analyze_pos_distribution(tagged_tokens)\n",
        "\n",
        "    return tagged_tokens\n",
        "\n",
        "def analyze_pos_distribution(tagged_tokens):\n",
        "    pos_counter = {}\n",
        "\n",
        "    for token in tagged_tokens:\n",
        "        if '_' in token:\n",
        "            pos = token.split('_')[-1]\n",
        "            pos_counter[pos] = pos_counter.get(pos, 0) + 1\n",
        "\n",
        "    print(\"\\nРаспределение частей речи:\")\n",
        "    total = len(tagged_tokens)\n",
        "    for pos, count in sorted(pos_counter.items(), key=lambda x: x[1], reverse=True):\n",
        "        percentage = (count / total) * 100\n",
        "        print(f\"{pos:15}: {count:4} ({percentage:5.1f}%)\")\n",
        "\n",
        "def save_results(tagged_tokens, output_file):\n",
        "    with open(output_file, 'w', encoding='utf-8') as file:\n",
        "        for token in tagged_tokens:\n",
        "            file.write(token + ' ')\n",
        "    print(f\"\\nРезультаты сохранены в файл: {output_file}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    english_tokens = process_text_file('англ.txt', 'english')\n",
        "    if english_tokens:\n",
        "        save_results(english_tokens, 'англ_tagged.txt')\n",
        "\n",
        "    russian_tokens = process_text_file('рус.txt', 'russian')\n",
        "    if russian_tokens:\n",
        "        save_results(russian_tokens, 'рус_tagged.txt')\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"СРАВНИТЕЛЬНЫЙ АНАЛИЗ\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    def count_unique_tokens(tagged_tokens):\n",
        "        unique = set()\n",
        "        for token in tagged_tokens:\n",
        "            word = token.split('_')[0] if '_' in token else token\n",
        "            unique.add(word)\n",
        "        return len(unique)\n",
        "\n",
        "    if english_tokens:\n",
        "        print(f\"\\nАнглийский текст:\")\n",
        "        print(f\"Всего токенов: {len(english_tokens)}\")\n",
        "        print(f\"Уникальных слов: {count_unique_tokens(english_tokens)}\")\n",
        "        print(f\"Средняя длина слова: {sum(len(t.split('_')[0]) for t in english_tokens)/len(english_tokens):.1f}\")\n",
        "\n",
        "    if russian_tokens:\n",
        "        print(f\"\\nРусский текст:\")\n",
        "        print(f\"Всего токенов: {len(russian_tokens)}\")\n",
        "        print(f\"Уникальных слов: {count_unique_tokens(russian_tokens)}\")\n",
        "        print(f\"Средняя длина слова: {sum(len(t.split('_')[0]) for t in russian_tokens)/len(russian_tokens):.1f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "md6BU7WOPNbZ",
        "outputId": "7a755d4b-ce13-4d5a-a125-8034031031f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Обработка файла: англ.txt\n",
            "Язык: english\n",
            "Длина исходного текста: 109935 символов\n",
            "Длина исходного текста: 15889 слов\n",
            "Длина текста после очистки: 107748 символов\n",
            "Длина текста после очистки: 16171 слов\n",
            "Количество размеченных токенов: 16239\n",
            "\n",
            "Первые 20 размеченных токенов:\n",
            "  1. fefu_ADJ\n",
            "  2. students_NOUN\n",
            "  3. win_VERB\n",
            "  4. four_NUM\n",
            "  5. awards_NOUN\n",
            "  6. at_ADP\n",
            "  7. the_DET\n",
            "  8. acrobatic_NOUN\n",
            "  9. rock_NOUN\n",
            " 10. n_CONJ\n",
            " 11. roll_NOUN\n",
            " 12. world_NOUN\n",
            " 13. cup_NOUN\n",
            " 14. in_ADP\n",
            " 15. kazakhstan_NOUN\n",
            " 16. the_DET\n",
            " 17. team_NOUN\n",
            " 18. of_ADP\n",
            " 19. far_NOUN\n",
            " 20. eastern_NOUN\n",
            "\n",
            "Распределение частей речи:\n",
            "NOUN           : 6703 ( 41.3%)\n",
            "ADP            : 2274 ( 14.0%)\n",
            "VERB           : 1961 ( 12.1%)\n",
            "DET            : 1691 ( 10.4%)\n",
            "ADJ            : 1629 ( 10.0%)\n",
            "CONJ           :  760 (  4.7%)\n",
            "ADV            :  340 (  2.1%)\n",
            "PRT            :  339 (  2.1%)\n",
            "PRON           :  316 (  1.9%)\n",
            "NUM            :  213 (  1.3%)\n",
            "X              :    9 (  0.1%)\n",
            ".              :    4 (  0.0%)\n",
            "\n",
            "Результаты сохранены в файл: англ_tagged.txt\n",
            "\n",
            "Обработка файла: рус.txt\n",
            "Язык: russian\n",
            "Длина исходного текста: 106019 символов\n",
            "Длина исходного текста: 12922 слов\n",
            "Длина текста после очистки: 103656 символов\n",
            "Длина текста после очистки: 13033 слов\n",
            "Количество размеченных токенов: 12808\n",
            "\n",
            "Первые 20 размеченных токенов:\n",
            "  1. студенты_S\n",
            "  2. двфу_S\n",
            "  3. завоевали_V\n",
            "  4. четыре_NUM\n",
            "  5. кубка_S\n",
            "  6. мира_S\n",
            "  7. по_PR\n",
            "  8. акробатическому_A\n",
            "  9. рок_S\n",
            " 10. н_S\n",
            " 11. роллу_S\n",
            " 12. в_PR\n",
            " 13. казахстане_S\n",
            " 14. сборная_S\n",
            " 15. дальневосточного_A\n",
            " 16. федерального_A\n",
            " 17. университета_S\n",
            " 18. двфу_S\n",
            " 19. и_CONJ\n",
            " 20. танцоры_S\n",
            "\n",
            "Распределение частей речи:\n",
            "S              : 5941 ( 46.4%)\n",
            "A              : 2322 ( 18.1%)\n",
            "PR             : 1378 ( 10.8%)\n",
            "V              : 1191 (  9.3%)\n",
            "CONJ           :  876 (  6.8%)\n",
            "APRO           :  324 (  2.5%)\n",
            "ADV            :  318 (  2.5%)\n",
            "SPRO           :  135 (  1.1%)\n",
            "PART           :   89 (  0.7%)\n",
            "ANUM           :   51 (  0.4%)\n",
            "NUM            :   46 (  0.4%)\n",
            "ADVPRO         :   42 (  0.3%)\n",
            "COM            :   24 (  0.2%)\n",
            "\n",
            "Результаты сохранены в файл: рус_tagged.txt\n",
            "\n",
            "============================================================\n",
            "СРАВНИТЕЛЬНЫЙ АНАЛИЗ\n",
            "============================================================\n",
            "\n",
            "Английский текст:\n",
            "Всего токенов: 16239\n",
            "Уникальных слов: 2598\n",
            "Средняя длина слова: 5.6\n",
            "\n",
            "Русский текст:\n",
            "Всего токенов: 12808\n",
            "Уникальных слов: 4060\n",
            "Средняя длина слова: 7.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}