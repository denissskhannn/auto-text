{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1: Векторизация текста с использованием Мешка слов (BoW) и TF-IDF**"
      ],
      "metadata": {
        "id": "76va5BKKNKcx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Создайте корпус из минимум 10 текстовых документов (можно взять новостные статьи, отзывы на товары, статьи по одной тематике).\n",
        "2. Реализуйте предобработку текста, включающую:\n",
        "* Приведение к нижнему регистру\n",
        "* Токенизацию\n",
        "* Удаление стоп-слов и пунктуации\n",
        "* Лемматизацию/стемминг\n",
        "3. Реализуйте модель Мешка слов (BoW) с использованием CountVectorizer из scikit-learn.\n",
        "4. Реализуйте модель TF-IDF с использованием TfidfVectorizer из scikit-learn.\n",
        "5. Найдите 10 самых значимых терминов для каждого документа по обоим подходам и сравните результаты.\n",
        "6. Визуализируйте сходство документов с помощью метрики косинусного расстояния для обоих подходов.\n",
        "7. Прокомментируйте разницу в результатах между BoW и TF-IDF"
      ],
      "metadata": {
        "id": "EFPwzZZ4NMIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYcQ1ic8u4p3",
        "outputId": "2350d1fd-ac53-43a8-b0e2-8764efdafaed"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7q9vo0RuMxPX"
      },
      "outputs": [],
      "source": [
        "# корпус из 10 отзывов на отель\n",
        "documents = [\n",
        "    \"comfortable, well-kept, and made us feel right at home. One of the highlights was the cooking experience: hands-on, fun, and thoughtfully guided. We not only enjoyed the process but also got to savour what we cooked, which made the experience even more special. A perfect blend of relaxation and memorable moments.\",\n",
        "    \"I can not fault this hotel in any way. We arrived 21st Dec 24 and we leave tomorrow 27th. From the moment twe arrived the staff have all been incredible. Special mention for daniel and apple. They are all so lovely and polite and can't do enough for you. The hotel is beautiful just like the pictures. Rooms are immaculate and serviced daily. Plenty of sun loungers for everyone. The food is fabulous and tasty and the coctails are lovely, we never had a bad meal and its so cheap to eat. The cocktails are literally the price of a beer in the uk. There is also happy hour 2-4-1 three tines a day and walk rpund with free canapes, ice lollies etc. We had a look at a bungalow and they were fab so booking one next time. Dinso is a haven of peace and tranquility in the madness of patong. I can highly recommend this place. We will 100% be returning.\",\n",
        "    \"Dinso feels like a true luxury jungle experience. The value meets the cost for the resort and feels fairly priced. It’s located near downtown Patong so quick access by a short drive to key areas and restaurants in the city. The rooms and public areas are well kept and clean. The environment is relaxing and the staff is very accommodating. While you’re there take advantage of the Spa. They have a large menu of treatments and service that goes above and beyond. If you hit the Spa, ask for Anna and Lucky.\",\n",
        "    \"Dinso the best!!!! Nice resort with the best service. Will recommend this resort to my friends as it is by far one of the friendliest staff ever!!\",\n",
        "    \"This is easily one of our favourite resorts so far. From the moment we arrived, we felt comfortable, relaxed, and able to properly switch off.\",\n",
        "    \"Staying at the Dinso Resort over Christmas has been an absolute pleasure. The breakfast buffet is very good, especially the fresh ripe fruit. The room is always kept clean and tidy. However, I would like to specifically mention the exceptional hospitality at this resort. The staff are super friendly and helpful; Deen and Arm, who work behind the bar by the pool, always welcome you with open smiles and constantly go the extra mile. Overall, it’s been an absolute pleasure and I would happily stay here again!\",\n",
        "    \"Had a really nice stay here, relaxed vibe, beautiful surroundings, and friendly staff all around. Deen stood out in particular: super chill, helpful, and always welcoming. He made the experience even better. Thanks, Deen!\",\n",
        "    \"I traveled with friends and I was able to make so comfortable and fun memories. In the resort, the manager of Goerge was so friendly that we were able to use the pool and bar!!\",\n",
        "    \"The beautiful hotel in the best location. I thank all the staff in the reception, restaurant, and other services for their warm welcome and excellent hospitality.\",\n",
        "    \"Love the warmth of staff member. Especially Mr. Deen has the best service for us. Loved it!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_stopwords = set(stopwords.words('english'))\n",
        "stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in english_stopwords and len(t) > 2]\n",
        "    stemmed = [stemmer.stem(t) for t in tokens]\n",
        "    return \" \".join(stemmed)\n",
        "\n",
        "preprocessed_documents = [preprocess_text(doc) for doc in documents]\n",
        "\n",
        "print(\"Предобработанные отзывы:\")\n",
        "for i, doc in enumerate(preprocessed_documents, 1):\n",
        "    print(f\"{i}: {doc}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT2-vH3Xvbyy",
        "outputId": "5c678a07-f273-443b-ea46-3ce402594e88"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Предобработанные отзывы:\n",
            "1: comfort wellkept made feel right home one highlight cook experi handson fun thought guid enjoy process also got savour cook made experi even special perfect blend relax memor moment\n",
            "2: fault hotel way arriv dec leav tomorrow moment twe arriv staff incred special mention daniel appl love polit cant enough hotel beauti like pictur room immacul servic daili plenti sun lounger everyon food fabul tasti coctail love never bad meal cheap eat cocktail liter price beer also happi hour three tine day walk rpund free canap ice lolli etc look bungalow fab book one next time dinso peac tranquil mad patong high recommend place return\n",
            "3: dinso feel like true luxuri jungl experi valu meet cost resort feel fair price locat near downtown patong quick access short drive key area restaur citi room public area well kept clean environ relax staff accommod take advantag spa larg menu treatment servic goe beyond hit spa ask anna lucki\n",
            "4: dinso best nice resort best servic recommend resort friend far one friendliest staff ever\n",
            "5: easili one favourit resort far moment arriv felt comfort relax abl proper switch\n",
            "6: stay dinso resort christma absolut pleasur breakfast buffet good especi fresh ripe fruit room alway kept clean tidi howev would like specif mention except hospit resort staff super friend help deen arm work behind bar pool alway welcom open smile constant extra mile overal absolut pleasur would happili stay\n",
            "7: realli nice stay relax vibe beauti surround friend staff around deen stood particular super chill help alway welcom made experi even better thank deen\n",
            "8: travel friend abl make comfort fun memori resort manag goerg friend abl use pool bar\n",
            "9: beauti hotel best locat thank staff recept restaur servic warm welcom excel hospit\n",
            "10: love warmth staff member especi deen best servic love\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count_vectorizer = CountVectorizer()\n",
        "bow_matrix = count_vectorizer.fit_transform(preprocessed_documents)\n",
        "bow_feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "bow_df = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=bow_feature_names,\n",
        "    index=[f\"Отзыв {i}\" for i in range(1, 11)]\n",
        ")\n",
        "\n",
        "print(\"\\nМатрица Bag of Words:\\n\")\n",
        "print(bow_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nUB7jhazYWb",
        "outputId": "e80f4195-2c10-4034-de13-a26ad1e32f13"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Матрица Bag of Words:\n",
            "\n",
            "          abl  absolut  access  accommod  advantag  also  alway  anna  appl  \\\n",
            "Отзыв 1     0        0       0         0         0     1      0     0     0   \n",
            "Отзыв 2     0        0       0         0         0     1      0     0     1   \n",
            "Отзыв 3     0        0       1         1         1     0      0     1     0   \n",
            "Отзыв 4     0        0       0         0         0     0      0     0     0   \n",
            "Отзыв 5     1        0       0         0         0     0      0     0     0   \n",
            "Отзыв 6     0        2       0         0         0     0      2     0     0   \n",
            "Отзыв 7     0        0       0         0         0     0      1     0     0   \n",
            "Отзыв 8     2        0       0         0         0     0      0     0     0   \n",
            "Отзыв 9     0        0       0         0         0     0      0     0     0   \n",
            "Отзыв 10    0        0       0         0         0     0      0     0     0   \n",
            "\n",
            "          area  ...  vibe  walk  warm  warmth  way  welcom  well  wellkept  \\\n",
            "Отзыв 1      0  ...     0     0     0       0    0       0     0         1   \n",
            "Отзыв 2      0  ...     0     1     0       0    1       0     0         0   \n",
            "Отзыв 3      2  ...     0     0     0       0    0       0     1         0   \n",
            "Отзыв 4      0  ...     0     0     0       0    0       0     0         0   \n",
            "Отзыв 5      0  ...     0     0     0       0    0       0     0         0   \n",
            "Отзыв 6      0  ...     0     0     0       0    0       1     0         0   \n",
            "Отзыв 7      0  ...     1     0     0       0    0       1     0         0   \n",
            "Отзыв 8      0  ...     0     0     0       0    0       0     0         0   \n",
            "Отзыв 9      0  ...     0     0     1       0    0       1     0         0   \n",
            "Отзыв 10     0  ...     0     0     0       1    0       0     0         0   \n",
            "\n",
            "          work  would  \n",
            "Отзыв 1      0      0  \n",
            "Отзыв 2      0      0  \n",
            "Отзыв 3      0      0  \n",
            "Отзыв 4      0      0  \n",
            "Отзыв 5      0      0  \n",
            "Отзыв 6      1      2  \n",
            "Отзыв 7      0      0  \n",
            "Отзыв 8      0      0  \n",
            "Отзыв 9      0      0  \n",
            "Отзыв 10     0      0  \n",
            "\n",
            "[10 rows x 197 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_documents)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_feature_names,\n",
        "    index=[f\"Review {i}\" for i in range(1, 11)]\n",
        ")\n",
        "\n",
        "print(\"\\nМатрица TF-IDF: \\n\")\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "flN05dLozpm0",
        "outputId": "2795e2b5-1d8c-42fc-fe9a-1469453b7433"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Матрица TF-IDF: \n",
            "\n",
            "                abl   absolut    access  accommod  advantag      also  \\\n",
            "Review 1   0.000000  0.000000  0.000000  0.000000  0.000000  0.158689   \n",
            "Review 2   0.000000  0.000000  0.000000  0.000000  0.000000  0.100404   \n",
            "Review 3   0.000000  0.000000  0.143292  0.143292  0.143292  0.000000   \n",
            "Review 4   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Review 5   0.275787  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Review 6   0.000000  0.282786  0.000000  0.000000  0.000000  0.000000   \n",
            "Review 7   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Review 8   0.459119  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Review 9   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Review 10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "              alway      anna     appl      area  ...      vibe     walk  \\\n",
            "Review 1   0.000000  0.000000  0.00000  0.000000  ...  0.000000  0.00000   \n",
            "Review 2   0.000000  0.000000  0.11811  0.000000  ...  0.000000  0.11811   \n",
            "Review 3   0.000000  0.143292  0.00000  0.286583  ...  0.000000  0.00000   \n",
            "Review 4   0.000000  0.000000  0.00000  0.000000  ...  0.000000  0.00000   \n",
            "Review 5   0.000000  0.000000  0.00000  0.000000  ...  0.000000  0.00000   \n",
            "Review 6   0.240394  0.000000  0.00000  0.000000  ...  0.000000  0.00000   \n",
            "Review 7   0.196240  0.000000  0.00000  0.000000  ...  0.230846  0.00000   \n",
            "Review 8   0.000000  0.000000  0.00000  0.000000  ...  0.000000  0.00000   \n",
            "Review 9   0.000000  0.000000  0.00000  0.000000  ...  0.000000  0.00000   \n",
            "Review 10  0.000000  0.000000  0.00000  0.000000  ...  0.000000  0.00000   \n",
            "\n",
            "               warm    warmth      way    welcom      well  wellkept  \\\n",
            "Review 1   0.000000  0.000000  0.00000  0.000000  0.000000  0.186673   \n",
            "Review 2   0.000000  0.000000  0.11811  0.000000  0.000000  0.000000   \n",
            "Review 3   0.000000  0.000000  0.00000  0.000000  0.143292  0.000000   \n",
            "Review 4   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
            "Review 5   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
            "Review 6   0.000000  0.000000  0.00000  0.105158  0.000000  0.000000   \n",
            "Review 7   0.000000  0.000000  0.00000  0.171687  0.000000  0.000000   \n",
            "Review 8   0.000000  0.000000  0.00000  0.000000  0.000000  0.000000   \n",
            "Review 9   0.335901  0.000000  0.00000  0.249820  0.000000  0.000000   \n",
            "Review 10  0.000000  0.369869  0.00000  0.000000  0.000000  0.000000   \n",
            "\n",
            "               work     would  \n",
            "Review 1   0.000000  0.000000  \n",
            "Review 2   0.000000  0.000000  \n",
            "Review 3   0.000000  0.000000  \n",
            "Review 4   0.000000  0.000000  \n",
            "Review 5   0.000000  0.000000  \n",
            "Review 6   0.141393  0.282786  \n",
            "Review 7   0.000000  0.000000  \n",
            "Review 8   0.000000  0.000000  \n",
            "Review 9   0.000000  0.000000  \n",
            "Review 10  0.000000  0.000000  \n",
            "\n",
            "[10 rows x 197 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Топ‑10 терминов по каждому отзыву\n",
        "def top_terms_per_doc(matrix, feature_names, top_n=10, is_tfidf=False):\n",
        "    tops = {}\n",
        "    for i in range(matrix.shape[0]):\n",
        "        row = matrix[i].toarray().flatten()\n",
        "        idx_sorted = np.argsort(row)[::-1]\n",
        "        top_idx = idx_sorted[:top_n]\n",
        "        if is_tfidf:\n",
        "            tops[i] = [(feature_names[j], round(row[j], 4)) for j in top_idx if row[j] > 0]\n",
        "        else:\n",
        "            tops[i] = [(feature_names[j], int(row[j])) for j in top_idx if row[j] > 0]\n",
        "    return tops\n",
        "\n",
        "bow_top_per_doc = top_terms_per_doc(bow_matrix, bow_feature_names)\n",
        "tfidf_top_per_doc = top_terms_per_doc(tfidf_matrix, tfidf_feature_names, is_tfidf=True)\n",
        "\n",
        "print(\"\\nТоп-10 BoW по отзывам:\")\n",
        "for i, terms in bow_top_per_doc.items():\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    for w, v in terms:\n",
        "        print(\"  -\", w, v)\n",
        "\n",
        "print(\"\\nТоп-10 TF-IDF по отзывам:\")\n",
        "for i, terms in tfidf_top_per_doc.items():\n",
        "    print(f\"\\nReview {i+1}:\")\n",
        "    for w, v in terms:\n",
        "        print(\"  -\", w, v)\n",
        "\n",
        "# Косинусное сходство\n",
        "bow_cosine = cosine_similarity(bow_matrix)\n",
        "tfidf_cosine = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"\\nКосинусное сходство BoW:\")\n",
        "print(pd.DataFrame(bow_cosine.round(3)))\n",
        "\n",
        "print(\"\\nКосинусное сходство TF-IDF:\")\n",
        "print(pd.DataFrame(tfidf_cosine.round(3)))\n",
        "\n",
        "# Сравнение топ‑10 по всему корпусу\n",
        "bow_sum = np.sum(bow_matrix.toarray(), axis=0)\n",
        "tfidf_sum = np.sum(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "bow_top_corpus = sorted(zip(bow_feature_names, bow_sum), key=lambda x: x[1], reverse=True)[:10]\n",
        "tfidf_top_corpus = sorted(zip(tfidf_feature_names, tfidf_sum), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"\\nТОП-10 BoW по корпусу:\")\n",
        "for i, (w, f) in enumerate(bow_top_corpus, 1):\n",
        "    print(f\"{i:2d}. {w:<15} {f}\")\n",
        "\n",
        "print(\"\\nТОП-10 TF-IDF по корпусу:\")\n",
        "for i, (w, s) in enumerate(tfidf_top_corpus, 1):\n",
        "    print(f\"{i:2d}. {w:<15} {s:.4f}\")\n",
        "\n",
        "bow_words = {w for w, _ in bow_top_corpus}\n",
        "tfidf_words = {w for w, _ in tfidf_top_corpus}\n",
        "common = bow_words & tfidf_words\n",
        "bow_only = bow_words - tfidf_words\n",
        "tfidf_only = tfidf_words - bow_words\n",
        "\n",
        "print(\"\\nОбщие слова (BoW и TF-IDF):\", \", \".join(sorted(common)))\n",
        "print(\"Только BoW:\", \", \".join(sorted(bow_only)))\n",
        "print(\"Только TF-IDF:\", \", \".join(sorted(tfidf_only)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nnoqWTGLzxfK",
        "outputId": "4e517084-959e-4fc3-97f4-aaa77e87f337"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Топ-10 BoW по отзывам:\n",
            "\n",
            "Review 1:\n",
            "  - made 2\n",
            "  - experi 2\n",
            "  - cook 2\n",
            "  - perfect 1\n",
            "  - wellkept 1\n",
            "  - process 1\n",
            "  - relax 1\n",
            "  - right 1\n",
            "  - savour 1\n",
            "  - comfort 1\n",
            "\n",
            "Review 2:\n",
            "  - love 2\n",
            "  - hotel 2\n",
            "  - arriv 2\n",
            "  - tranquil 1\n",
            "  - tomorrow 1\n",
            "  - price 1\n",
            "  - place 1\n",
            "  - plenti 1\n",
            "  - polit 1\n",
            "  - one 1\n",
            "\n",
            "Review 3:\n",
            "  - spa 2\n",
            "  - area 2\n",
            "  - feel 2\n",
            "  - true 1\n",
            "  - treatment 1\n",
            "  - restaur 1\n",
            "  - resort 1\n",
            "  - relax 1\n",
            "  - public 1\n",
            "  - take 1\n",
            "\n",
            "Review 4:\n",
            "  - resort 2\n",
            "  - best 2\n",
            "  - recommend 1\n",
            "  - staff 1\n",
            "  - servic 1\n",
            "  - friend 1\n",
            "  - friendliest 1\n",
            "  - dinso 1\n",
            "  - ever 1\n",
            "  - nice 1\n",
            "\n",
            "Review 5:\n",
            "  - proper 1\n",
            "  - relax 1\n",
            "  - resort 1\n",
            "  - one 1\n",
            "  - switch 1\n",
            "  - easili 1\n",
            "  - comfort 1\n",
            "  - arriv 1\n",
            "  - abl 1\n",
            "  - far 1\n",
            "\n",
            "Review 6:\n",
            "  - would 2\n",
            "  - resort 2\n",
            "  - pleasur 2\n",
            "  - stay 2\n",
            "  - alway 2\n",
            "  - absolut 2\n",
            "  - pool 1\n",
            "  - ripe 1\n",
            "  - tidi 1\n",
            "  - smile 1\n",
            "\n",
            "Review 7:\n",
            "  - deen 2\n",
            "  - stay 1\n",
            "  - staff 1\n",
            "  - surround 1\n",
            "  - welcom 1\n",
            "  - relax 1\n",
            "  - particular 1\n",
            "  - super 1\n",
            "  - realli 1\n",
            "  - stood 1\n",
            "\n",
            "Review 8:\n",
            "  - friend 2\n",
            "  - abl 2\n",
            "  - pool 1\n",
            "  - resort 1\n",
            "  - use 1\n",
            "  - make 1\n",
            "  - comfort 1\n",
            "  - fun 1\n",
            "  - goerg 1\n",
            "  - manag 1\n",
            "\n",
            "Review 9:\n",
            "  - welcom 1\n",
            "  - servic 1\n",
            "  - staff 1\n",
            "  - thank 1\n",
            "  - warm 1\n",
            "  - restaur 1\n",
            "  - recept 1\n",
            "  - beauti 1\n",
            "  - best 1\n",
            "  - excel 1\n",
            "\n",
            "Review 10:\n",
            "  - love 2\n",
            "  - staff 1\n",
            "  - servic 1\n",
            "  - member 1\n",
            "  - warmth 1\n",
            "  - especi 1\n",
            "  - deen 1\n",
            "  - best 1\n",
            "\n",
            "Топ-10 TF-IDF по отзывам:\n",
            "\n",
            "Review 1:\n",
            "  - cook 0.3733\n",
            "  - made 0.3174\n",
            "  - experi 0.2777\n",
            "  - savour 0.1867\n",
            "  - wellkept 0.1867\n",
            "  - process 0.1867\n",
            "  - guid 0.1867\n",
            "  - got 0.1867\n",
            "  - highlight 0.1867\n",
            "  - memor 0.1867\n",
            "\n",
            "Review 2:\n",
            "  - love 0.2008\n",
            "  - hotel 0.2008\n",
            "  - arriv 0.2008\n",
            "  - tranquil 0.1181\n",
            "  - tomorrow 0.1181\n",
            "  - plenti 0.1181\n",
            "  - sun 0.1181\n",
            "  - polit 0.1181\n",
            "  - pictur 0.1181\n",
            "  - peac 0.1181\n",
            "\n",
            "Review 3:\n",
            "  - spa 0.2866\n",
            "  - area 0.2866\n",
            "  - feel 0.2436\n",
            "  - true 0.1433\n",
            "  - treatment 0.1433\n",
            "  - take 0.1433\n",
            "  - public 0.1433\n",
            "  - quick 0.1433\n",
            "  - short 0.1433\n",
            "  - luxuri 0.1433\n",
            "\n",
            "Review 4:\n",
            "  - best 0.4778\n",
            "  - resort 0.3815\n",
            "  - friendliest 0.3212\n",
            "  - ever 0.3212\n",
            "  - recommend 0.273\n",
            "  - nice 0.273\n",
            "  - far 0.273\n",
            "  - dinso 0.2124\n",
            "  - friend 0.2124\n",
            "  - one 0.2124\n",
            "\n",
            "Review 5:\n",
            "  - switch 0.3244\n",
            "  - proper 0.3244\n",
            "  - easili 0.3244\n",
            "  - felt 0.3244\n",
            "  - favourit 0.3244\n",
            "  - far 0.2758\n",
            "  - abl 0.2758\n",
            "  - arriv 0.2758\n",
            "  - comfort 0.2413\n",
            "  - moment 0.2413\n",
            "\n",
            "Review 6:\n",
            "  - would 0.2828\n",
            "  - pleasur 0.2828\n",
            "  - absolut 0.2828\n",
            "  - alway 0.2404\n",
            "  - stay 0.2404\n",
            "  - resort 0.1679\n",
            "  - open 0.1414\n",
            "  - overal 0.1414\n",
            "  - tidi 0.1414\n",
            "  - ripe 0.1414\n",
            "\n",
            "Review 7:\n",
            "  - deen 0.3434\n",
            "  - realli 0.2308\n",
            "  - stood 0.2308\n",
            "  - surround 0.2308\n",
            "  - particular 0.2308\n",
            "  - better 0.2308\n",
            "  - around 0.2308\n",
            "  - chill 0.2308\n",
            "  - vibe 0.2308\n",
            "  - thank 0.1962\n",
            "\n",
            "Review 8:\n",
            "  - abl 0.4591\n",
            "  - friend 0.3571\n",
            "  - memori 0.27\n",
            "  - travel 0.27\n",
            "  - use 0.27\n",
            "  - manag 0.27\n",
            "  - goerg 0.27\n",
            "  - make 0.27\n",
            "  - bar 0.2296\n",
            "  - fun 0.2296\n",
            "\n",
            "Review 9:\n",
            "  - warm 0.3359\n",
            "  - recept 0.3359\n",
            "  - excel 0.3359\n",
            "  - thank 0.2855\n",
            "  - restaur 0.2855\n",
            "  - hospit 0.2855\n",
            "  - hotel 0.2855\n",
            "  - locat 0.2855\n",
            "  - best 0.2498\n",
            "  - beauti 0.2498\n",
            "\n",
            "Review 10:\n",
            "  - love 0.6288\n",
            "  - member 0.3699\n",
            "  - warmth 0.3699\n",
            "  - especi 0.3144\n",
            "  - deen 0.2751\n",
            "  - best 0.2751\n",
            "  - servic 0.2196\n",
            "  - staff 0.1803\n",
            "\n",
            "Косинусное сходство BoW:\n",
            "       0      1      2      3      4      5      6      7      8      9\n",
            "0  1.000  0.075  0.113  0.040  0.188  0.000  0.199  0.078  0.000  0.000\n",
            "1  0.075  1.000  0.104  0.131  0.123  0.071  0.044  0.000  0.154  0.201\n",
            "2  0.113  0.104  1.000  0.157  0.074  0.137  0.079  0.031  0.148  0.081\n",
            "3  0.040  0.131  0.157  1.000  0.261  0.211  0.139  0.216  0.261  0.284\n",
            "4  0.188  0.123  0.074  0.261  1.000  0.071  0.054  0.255  0.000  0.000\n",
            "5  0.000  0.071  0.137  0.211  0.071  1.000  0.276  0.176  0.107  0.116\n",
            "6  0.199  0.044  0.079  0.139  0.054  0.276  1.000  0.090  0.218  0.177\n",
            "7  0.078  0.000  0.031  0.216  0.255  0.176  0.090  1.000  0.000  0.000\n",
            "8  0.000  0.154  0.148  0.261  0.000  0.107  0.218  0.000  1.000  0.251\n",
            "9  0.000  0.201  0.081  0.284  0.000  0.116  0.177  0.000  0.251  1.000\n",
            "\n",
            "Косинусное сходство TF-IDF:\n",
            "       0      1      2      3      4      5      6      7      8      9\n",
            "0  1.000  0.054  0.080  0.026  0.120  0.000  0.160  0.064  0.000  0.000\n",
            "1  0.054  1.000  0.061  0.083  0.093  0.042  0.022  0.000  0.103  0.152\n",
            "2  0.080  0.061  1.000  0.080  0.037  0.080  0.041  0.014  0.098  0.031\n",
            "3  0.026  0.083  0.080  1.000  0.194  0.115  0.104  0.137  0.183  0.202\n",
            "4  0.120  0.093  0.037  0.194  1.000  0.032  0.033  0.206  0.000  0.000\n",
            "5  0.000  0.042  0.080  0.115  0.032  1.000  0.218  0.116  0.072  0.079\n",
            "6  0.160  0.022  0.041  0.104  0.033  0.218  1.000  0.055  0.160  0.115\n",
            "7  0.064  0.000  0.014  0.137  0.206  0.116  0.055  1.000  0.000  0.000\n",
            "8  0.000  0.103  0.098  0.183  0.000  0.072  0.160  0.000  1.000  0.142\n",
            "9  0.000  0.152  0.031  0.202  0.000  0.079  0.115  0.000  0.142  1.000\n",
            "\n",
            "ТОП-10 BoW по корпусу:\n",
            " 1. resort          7\n",
            " 2. staff           7\n",
            " 3. friend          5\n",
            " 4. servic          5\n",
            " 5. best            4\n",
            " 6. deen            4\n",
            " 7. dinso           4\n",
            " 8. experi          4\n",
            " 9. love            4\n",
            "10. one             4\n",
            "\n",
            "ТОП-10 TF-IDF по корпусу:\n",
            " 1. best            1.0027\n",
            " 2. resort          0.9875\n",
            " 3. love            0.8297\n",
            " 4. friend          0.8156\n",
            " 5. staff           0.8095\n",
            " 6. servic          0.7651\n",
            " 7. abl             0.7349\n",
            " 8. deen            0.7236\n",
            " 9. one             0.6284\n",
            "10. relax           0.5853\n",
            "\n",
            "Общие слова (BoW и TF-IDF): best, deen, friend, love, one, resort, servic, staff\n",
            "Только BoW: dinso, experi\n",
            "Только TF-IDF: abl, relax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#выводы\n",
        "print(\"\"\"\n",
        "Вывод: BoW поднимает наиболее частотные слова (hotel, staff, resort),\n",
        "а TF-IDF выделяет более специфические термины, характерные для отдельных отзывов\n",
        "(например, имя сотрудника, spa, breakfast). Матрица сходства TF-IDF даёт более\n",
        "контрастные значения: похожие по содержанию отзывы ближе друг к другу, чем в BoW.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "K3zxHQaEQniu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95681646-827b-42a2-fb85-1a3b8944ac89"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Вывод: BoW поднимает наиболее частотные слова (hotel, staff, resort), \n",
            "а TF-IDF выделяет более специфические термины, характерные для отдельных отзывов \n",
            "(например, имя сотрудника, spa, breakfast). Матрица сходства TF-IDF даёт более \n",
            "контрастные значения: похожие по содержанию отзывы ближе друг к другу, чем в BoW.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2: Морфологическая разметка текста**"
      ],
      "metadata": {
        "id": "jj7D5CC2NguY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Возьмите отрывок текста (минимум 300 слов) на русском и английском языке.\n",
        "2. Для русского языка используйте PyMorphy2 или PyMorphy3, для английского - NLTK или SpaCy для проведения морфологического анализа.\n",
        "3. Выполните следующие операции:\n",
        "* Определите части речи для каждого слова в тексте\n",
        "* Для существительных определите падеж, род и число\n",
        "* Для глаголов определите время, лицо и число\n",
        "* Создайте частотный словарь частей речи в тексте\n",
        "4. Разработайте функцию, которая будет автоматически изменять текст, заменяя все существительные на их форму множественного числа (где возможно).\n",
        "5. Результаты морфологического анализа должны быть представлены в виде таблицы.\n",
        "6. Оцените и прокомментируйте точность определения морфологических характеристик"
      ],
      "metadata": {
        "id": "xx3HfIUpNxj1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3\n",
        "!pip install spacy pandas\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "from pymorphy3 import MorphAnalyzer\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "Fi83NngNNjJ0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e18eb28-a061-47b8-99f5-f6e96232b8f2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymorphy3\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim<1.0.0,>=0.3.0->spacy) (8.3.1)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rus_text = \"\"\"\n",
        "Синтетическая биология представляет собой междисциплинарную область науки, которая\n",
        "объединяет инженерный подход с классической генетикой. Современные методы редактирования\n",
        "генома, такие как CRISPR/Cas9, позволяют изменять ДНК с точностью до одного нуклеотида.\n",
        "\n",
        "Компания Ginkgo Bioworks разработала штаммы дрожжей, способные синтезировать сложные\n",
        "ароматические соединения, что кардинально изменило производство парфюмерии. Аналогичные\n",
        "подходы применяются для создания биотоплива из сельскохозяйственных отходов.\n",
        "\n",
        "Биореакторы объёмом 5000 литров выращивают микроорганизмы, производящие материалы,\n",
        "идентичные натуральному шёлку или коже. Это решает этические проблемы традиционного\n",
        "животноводства и снижает экологический ущерб.\n",
        "\n",
        "Стартап NatureLoop представил пластик, полностью разлагающийся в почве за 90 дней.\n",
        "Такие biodegradable материалы на основе целлюлозы могут заменить традиционную упаковку.\n",
        "\n",
        "К 2030 году эксперты прогнозируют переход до 35 процентов химической промышленности\n",
        "на bio-based производство. Однако этические вопросы регулирования новой отрасли требуют\n",
        "международных соглашений, таких как Biological Code Act (BCA-2025).\n",
        "\"\"\"\n",
        "\n",
        "eng_text = \"\"\"\n",
        "This hotel offers excellent service and very clean rooms throughout the stay. The staff\n",
        "members are exceptionally friendly and always ready to help with any requests. Location\n",
        "is perfect, just minutes from the city center and major attractions. Breakfast buffet\n",
        "was delicious with fresh fruits, pastries, and hot dishes available every morning.\n",
        "\n",
        "The hotel features a beautiful indoor pool and spa area that guests can enjoy after\n",
        "a long day of sightseeing. Rooms have comfortable beds, modern amenities, and great\n",
        "views of the surrounding area. Value for money is outstanding considering the quality\n",
        "of facilities and service provided.\n",
        "\n",
        "Some guests mentioned that WiFi connection was occasionally slow during peak hours.\n",
        "However, the helpful reception staff quickly resolved any connectivity issues. The\n",
        "hotel restaurant serves decent meals but breakfast selection was preferred by most\n",
        "visitors. Overall atmosphere is quiet and relaxing, ideal for business travelers or\n",
        "families.\n",
        "\n",
        "Maintenance team responds promptly to room service requests. Parking facilities are\n",
        "available for guests arriving by car. The lobby area is spacious with comfortable\n",
        "seating for meetings or relaxation. Housekeeping staff ensures daily cleaning and\n",
        "fresh towels are provided without fail.\n",
        "\n",
        "Minor complaints include outdated furniture in some rooms and occasional noise from\n",
        "nearby construction. Management is responsive and offers compensation for inconveniences.\n",
        "This property delivers consistent quality and earns high marks for customer satisfaction.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "bs2PV1k747Sc"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "morph_ru = MorphAnalyzer()\n",
        "\n",
        "def analyze_russian(text):\n",
        "    tokens = [t.lower() for t in word_tokenize(text) if t.isalpha()]\n",
        "    rows = []\n",
        "    for tok in tokens:\n",
        "        p = morph_ru.parse(tok)[0]\n",
        "        rows.append({\n",
        "            \"token\": tok,\n",
        "            \"lemma\": p.normal_form,\n",
        "            \"pos\": p.tag.POS or \"-\",\n",
        "            \"case\": p.tag.case or \"-\",\n",
        "            \"gender\": p.tag.gender or \"-\",\n",
        "            \"number\": p.tag.number or \"-\",\n",
        "            \"tense\": p.tag.tense or \"-\",\n",
        "            \"person\": p.tag.person or \"-\"\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "rus_df = analyze_russian(rus_text)\n",
        "print(\"Русский морфанализ (первые строки):\")\n",
        "print(rus_df)\n",
        "\n",
        "rus_pos_freq = Counter(rus_df[\"pos\"])\n",
        "print(\"\\nЧастоты частей речи (русский):\")\n",
        "for pos, cnt in rus_pos_freq.most_common():\n",
        "    print(pos, \":\", cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPMZhpHr5NYB",
        "outputId": "1ce800cb-3c69-43b5-b9de-150c9494c4cd"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Русский морфанализ (первые строки):\n",
            "                 token              lemma   pos  case gender number tense  \\\n",
            "0        синтетическая      синтетический  ADJF  nomn   femn   sing     -   \n",
            "1             биология           биология  NOUN  nomn   femn   sing     -   \n",
            "2         представляет       представлять  VERB     -      -   sing  pres   \n",
            "3                собой               себя  NPRO  ablt      -   sing     -   \n",
            "4    междисциплинарную  междисциплинарный  ADJF  accs   femn   sing     -   \n",
            "..                 ...                ...   ...   ...    ...    ...   ...   \n",
            "115              таких              такой  ADJF  gent      -   plur     -   \n",
            "116                как                как  CONJ     -      -      -     -   \n",
            "117         biological         biological     -     -      -      -     -   \n",
            "118               code               code     -     -      -      -     -   \n",
            "119                act                act     -     -      -      -     -   \n",
            "\n",
            "    person  \n",
            "0        -  \n",
            "1        -  \n",
            "2     3per  \n",
            "3        -  \n",
            "4        -  \n",
            "..     ...  \n",
            "115      -  \n",
            "116      -  \n",
            "117      -  \n",
            "118      -  \n",
            "119      -  \n",
            "\n",
            "[120 rows x 8 columns]\n",
            "\n",
            "Частоты частей речи (русский):\n",
            "NOUN : 49\n",
            "ADJF : 25\n",
            "VERB : 13\n",
            "PREP : 11\n",
            "- : 7\n",
            "CONJ : 6\n",
            "INFN : 3\n",
            "ADVB : 2\n",
            "PRTF : 2\n",
            "NPRO : 1\n",
            "PRCL : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def analyze_english(text):\n",
        "    doc = nlp_en(text)\n",
        "    rows = []\n",
        "    for token in doc:\n",
        "        if not token.is_alpha:\n",
        "            continue\n",
        "        m = token.morph\n",
        "        rows.append({\n",
        "            \"token\": token.text.lower(),\n",
        "            \"lemma\": token.lemma_,\n",
        "            \"pos\": token.pos_,\n",
        "            \"number\": m.get(\"Number\", [\"-\"])[0],\n",
        "            \"tense\": m.get(\"Tense\", [\"-\"])[0],\n",
        "            \"person\": m.get(\"Person\", [\"-\"])[0]\n",
        "        })\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "eng_df = analyze_english(eng_text)\n",
        "print(\"\\nАнглийский морфанализ (первые строки):\")\n",
        "print(eng_df)\n",
        "\n",
        "eng_pos_freq = Counter(eng_df[\"pos\"])\n",
        "print(\"\\nЧастоты частей речи (английский):\")\n",
        "for pos, cnt in eng_pos_freq.most_common():\n",
        "    print(pos, \":\", cnt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cWCY7SH6rzU",
        "outputId": "d8377fb9-689a-4fc5-ed4d-84cbdce86b1a"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Английский морфанализ (первые строки):\n",
            "            token         lemma   pos number tense person\n",
            "0            this          this   DET   Sing     -      -\n",
            "1           hotel         hotel  NOUN   Sing     -      -\n",
            "2          offers         offer  VERB   Sing  Pres      3\n",
            "3       excellent     excellent   ADJ      -     -      -\n",
            "4         service       service  NOUN   Sing     -      -\n",
            "..            ...           ...   ...    ...   ...    ...\n",
            "216          high          high   ADJ      -     -      -\n",
            "217         marks          mark  NOUN   Plur     -      -\n",
            "218           for           for   ADP      -     -      -\n",
            "219      customer      customer  NOUN   Sing     -      -\n",
            "220  satisfaction  satisfaction  NOUN   Sing     -      -\n",
            "\n",
            "[221 rows x 6 columns]\n",
            "\n",
            "Частоты частей речи (английский):\n",
            "NOUN : 85\n",
            "ADJ : 35\n",
            "VERB : 22\n",
            "ADP : 21\n",
            "DET : 18\n",
            "CCONJ : 15\n",
            "AUX : 12\n",
            "ADV : 8\n",
            "PART : 2\n",
            "PRON : 1\n",
            "SCONJ : 1\n",
            "PROPN : 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pluralize_russian(text):\n",
        "    tokens = nltk.word_tokenize(text.lower())\n",
        "    result_tokens = []\n",
        "\n",
        "    for token in tokens:\n",
        "        if token.isalpha():\n",
        "            parsed = morph_ru.parse(token)[0]\n",
        "            if parsed.tag.POS == 'NOUN':\n",
        "                plural_form = parsed.inflect({'plur'})\n",
        "                if plural_form:\n",
        "                    result_tokens.append(plural_form.word)\n",
        "                else:\n",
        "                    result_tokens.append(token)\n",
        "            else:\n",
        "                result_tokens.append(token)\n",
        "        else:\n",
        "            result_tokens.append(token)\n",
        "\n",
        "    return ' '.join(result_tokens)\n",
        "\n",
        "rus_plural = pluralize_russian(rus_text)\n",
        "print(\"\\nРусский текст: существительные → множественное число\\n\")\n",
        "print(\"Исходный текст:\", rus_text)\n",
        "print(\"Изменённый текст:\", rus_plural)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wq-w2Fs7miQ",
        "outputId": "0b04152b-3a77-4e36-ad00-8857708dcc1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Русский текст: существительные → множественное число\n",
            "\n",
            "Исходный текст: \n",
            "Синтетическая биология представляет собой междисциплинарную область науки, которая \n",
            "объединяет инженерный подход с классической генетикой. Современные методы редактирования \n",
            "генома, такие как CRISPR/Cas9, позволяют изменять ДНК с точностью до одного нуклеотида. \n",
            "\n",
            "Компания Ginkgo Bioworks разработала штаммы дрожжей, способные синтезировать сложные \n",
            "ароматические соединения, что кардинально изменило производство парфюмерии. Аналогичные \n",
            "подходы применяются для создания биотоплива из сельскохозяйственных отходов. \n",
            "\n",
            "Биореакторы объёмом 5000 литров выращивают микроорганизмы, производящие материалы, \n",
            "идентичные натуральному шёлку или коже. Это решает этические проблемы традиционного \n",
            "животноводства и снижает экологический ущерб. \n",
            "\n",
            "Стартап NatureLoop представил пластик, полностью разлагающийся в почве за 90 дней. \n",
            "Такие biodegradable материалы на основе целлюлозы могут заменить традиционную упаковку. \n",
            "\n",
            "К 2030 году эксперты прогнозируют переход до 35 процентов химической промышленности \n",
            "на bio-based производство. Однако этические вопросы регулирования новой отрасли требуют \n",
            "международных соглашений, таких как Biological Code Act (BCA-2025).\n",
            "\n",
            "Изменённый текст: синтетическая биологии представляет собой междисциплинарную области наук , которая объединяет инженерный подходы с классической генетиками . современные методы редактирований геномов , такие как crispr/cas9 , позволяют изменять днк с точностями до одного нуклеотидов . компании ginkgo bioworks разработала штаммы дрожжей , способные синтезировать сложные ароматические соединений , что кардинально изменило производства парфюмерий . аналогичные подходы применяются для созданий биотоплива из сельскохозяйственных отходов . биореакторы объёмами 5000 литров выращивают микроорганизмы , производящие материалы , идентичные натуральному шелка или кожах . это решает этические проблемы традиционного животноводств и снижает экологический ущербы . стартапы natureloop представил пластики , полностью разлагающийся в почвах за 90 дней . такие biodegradable материалы на основах целлюлоз могут заменить традиционную упаковки . к 2030 годы эксперты прогнозируют переходы до 35 процентов химической промышленностей на bio-based производства . однако этические вопросы регулирований новой отраслей требуют международных соглашений , таких как biological code act ( bca-2025 ) .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def pluralize_english(text):\n",
        "    doc = nlp_en(text)\n",
        "    result_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if (token.pos_ == 'NOUN' and\n",
        "            token.morph.get('Number', [''])[0] == 'Sing'):\n",
        "            lemma = token.lemma_\n",
        "            if lemma.endswith('y') and re.match(r'[aeiou]$', lemma[-2]):\n",
        "                plural = lemma[:-1] + 'ies'\n",
        "            elif lemma.endswith('s') or lemma.endswith('sh') or lemma.endswith('ch') or lemma.endswith('x') or lemma.endswith('z'):\n",
        "                plural = lemma + 'es'\n",
        "            elif lemma.endswith('f') or lemma.endswith('fe'):\n",
        "                plural = lemma[:-1] + 'ves'\n",
        "            else:\n",
        "                plural = lemma + 's'\n",
        "\n",
        "            result_tokens.append(plural)\n",
        "        else:\n",
        "            result_tokens.append(token.text)\n",
        "\n",
        "    return ' '.join(result_tokens)\n",
        "\n",
        "eng_plural = pluralize_english(eng_text)\n",
        "print(\"\\nАнглийский текст: существительные → множественное число\\n\")\n",
        "print(\"Исходный текст:\", eng_text)\n",
        "print(\"Изменённый текст:\", eng_plural)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Z282ldtPJLg",
        "outputId": "5465a969-e5df-4ce7-cea4-b7d18b0c822d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Английский текст: существительные → множественное число\n",
            "\n",
            "Исходный текст: \n",
            "This hotel offers excellent service and very clean rooms throughout the stay. The staff\n",
            "members are exceptionally friendly and always ready to help with any requests. Location\n",
            "is perfect, just minutes from the city center and major attractions. Breakfast buffet\n",
            "was delicious with fresh fruits, pastries, and hot dishes available every morning.\n",
            "\n",
            "The hotel features a beautiful indoor pool and spa area that guests can enjoy after\n",
            "a long day of sightseeing. Rooms have comfortable beds, modern amenities, and great\n",
            "views of the surrounding area. Value for money is outstanding considering the quality\n",
            "of facilities and service provided.\n",
            "\n",
            "Some guests mentioned that WiFi connection was occasionally slow during peak hours.\n",
            "However, the helpful reception staff quickly resolved any connectivity issues. The\n",
            "hotel restaurant serves decent meals but breakfast selection was preferred by most\n",
            "visitors. Overall atmosphere is quiet and relaxing, ideal for business travelers or\n",
            "families.\n",
            "\n",
            "Maintenance team responds promptly to room service requests. Parking facilities are\n",
            "available for guests arriving by car. The lobby area is spacious with comfortable\n",
            "seating for meetings or relaxation. Housekeeping staff ensures daily cleaning and\n",
            "fresh towels are provided without fail.\n",
            "\n",
            "Minor complaints include outdated furniture in some rooms and occasional noise from\n",
            "nearby construction. Management is responsive and offers compensation for inconveniences.\n",
            "This property delivers consistent quality and earns high marks for customer satisfaction.\n",
            "\n",
            "Изменённый текст: \n",
            " This hotels offers excellent services and very clean rooms throughout the staies . The stafves \n",
            " members are exceptionally friendly and always ready to help with any requests . locations \n",
            " is perfect , just minutes from the citys centers and major attractions . breakfasts buffets \n",
            " was delicious with fresh fruits , pastries , and hot dishes available every mornings . \n",
            "\n",
            " The hotels features a beautiful indoor pools and spas areas that guests can enjoy after \n",
            " a long daies of sightseeings . Rooms have comfortable beds , modern amenities , and great \n",
            " views of the surrounding areas . values for moneies is outstanding considering the qualitys \n",
            " of facilities and services provided . \n",
            "\n",
            " Some guests mentioned that WiFi connections was occasionally slow during peaks hours . \n",
            " However , the helpful receptions stafves quickly resolved any connectivitys issues . The \n",
            " hotels restaurants serves decent meals but breakfasts selections was preferred by most \n",
            " visitors . Overall atmospheres is quiet and relaxing , ideals for businesses travelers or \n",
            " families . \n",
            "\n",
            " maintenances teams responds promptly to room services requests . parkings facilities are \n",
            " available for guests arriving by cars . The lobbys areas is spacious with comfortable \n",
            " seatings for meetings or relaxations . housekeepings stafves ensures daily cleanings and \n",
            " fresh towels are provided without fails . \n",
            "\n",
            " Minor complaints include outdated furnitures in some rooms and occasional noises from \n",
            " nearby constructions . managements is responsive and offers compensations for inconveniences . \n",
            " This propertys delivers consistent qualitys and earns high marks for customers satisfactions . \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#выводы\n",
        "print(\"\"\"\n",
        "Оценка точности:\n",
        "- Русский (pymorphy3): POS и граммемы (падеж/род/число/время) корректны в большинстве случаев, ошибки возможны на заимствованиях и редких именах собственных.\n",
        "- Английский (SpaCy): POS и признаки Number/Tense определяются хорошо; правило pluralize\n",
        "  покрывает типичные случаи (city→cities, box→boxes, room→rooms), но не обрабатывает\n",
        "  все неправильные формы (child→children, person→people и т.п.).\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "nj6y8ahe76hs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27a35b6d-37aa-4c7e-baec-d23612af1c18"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Оценка точности:\n",
            "- Русский (pymorphy3): POS и граммемы (падеж/род/число/время) корректны в большинстве случаев, ошибки возможны на заимствованиях и редких именах собственных.\n",
            "- Английский (SpaCy): POS и признаки Number/Tense определяются хорошо; правило pluralize \n",
            "  покрывает типичные случаи (city→cities, box→boxes, room→rooms), но не обрабатывает \n",
            "  все неправильные формы (child→children, person→people и т.п.).\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3: Синтаксический анализ предложений**"
      ],
      "metadata": {
        "id": "azGFuWTkOCnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Возьмите 2 простых и 3 сложных предложений на русском и английском языке (всего 10 предложений).\n",
        "2. Используйте SpaCy для построения синтаксических деревьев этих предложений.\n",
        "3. Для каждого предложения:\n",
        "* Визуализируйте синтаксическое дерево\n",
        "* Выделите все подлежащие и сказуемые\n",
        "* Найдите все пары слов, связанные отношением определения (прилагательное-существительное)\n",
        "4. Разработайте функцию для извлечения всех объектных и субъектных отношений из предложения в формате (субъект, предикат, объект).\n",
        "5. Объясните, какие трудности возникают при синтаксическом анализе сложных предложений"
      ],
      "metadata": {
        "id": "MWWlpPkjOFpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q spacy\n",
        "!python -m spacy download ru_core_news_sm\n",
        "!python -m spacy download en_core_web_sm\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "import pandas as pd\n",
        "\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "rus_simple = [\n",
        "    \"Кот спит на ковре.\",\n",
        "    \"Дети играют во дворе.\"\n",
        "]\n",
        "\n",
        "rus_complex = [\n",
        "    \"Хотя шел сильный дождь, туристы продолжали фотографировать городские достопримечательности.\",\n",
        "    \"Когда закончится встреча, менеджер расскажет команде о новых задачах и планах развития.\",\n",
        "    \"Если погода будет хорошей, мы поедем на пикник, возьмем еду и устроим игры.\"\n",
        "]\n",
        "\n",
        "eng_simple = [\n",
        "    \"The cat sleeps on the rug.\",\n",
        "    \"Children play in the yard.\"\n",
        "]\n",
        "\n",
        "eng_complex = [\n",
        "    \"Although it was raining heavily, tourists continued photographing city landmarks.\",\n",
        "    \"When the meeting ends, the manager will tell the team about new tasks and development plans.\",\n",
        "    \"If the weather is good, we will go on a picnic, take food, and organize games.\"\n",
        "]\n",
        "\n",
        "all_sentences = {\n",
        "    'Русский_простые': rus_simple,\n",
        "    'Русский_сложные': rus_complex,\n",
        "    'English_simple': eng_simple,\n",
        "    'English_complex': eng_complex\n",
        "}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4Fe0ElpQ-5N",
        "outputId": "d64f017a-b6d7-42ab-b6ac-56fb3da267e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m93.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ru-core-news-sm==3.8.0) (2.0.6)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Installing collected packages: ru-core-news-sm\n",
            "Successfully installed ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m98.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_sentence(sentence, nlp_model):\n",
        "    doc = nlp_model(sentence)\n",
        "\n",
        "    subjects = []\n",
        "    predicates = []\n",
        "    adj_noun_pairs = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"nsubj\":\n",
        "            subjects.append(token.text)\n",
        "        if token.dep_ == \"ROOT\" and token.pos_ in [\"VERB\", \"AUX\"]:\n",
        "            predicates.append(token.text)\n",
        "        if token.dep_ == \"amod\" and token.head.pos_ == \"NOUN\":\n",
        "            adj_noun_pairs.append(f\"{token.text} → {token.head.text}\")\n",
        "\n",
        "    return {\n",
        "        'sentence': sentence,\n",
        "        'subjects': subjects,\n",
        "        'predicates': predicates,\n",
        "        'adj_noun_pairs': adj_noun_pairs,\n",
        "        'doc': doc\n",
        "    }"
      ],
      "metadata": {
        "id": "Yqtvq7VPROlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "\n",
        "for category, sentences in all_sentences.items():\n",
        "    nlp_model = nlp_ru if 'Русский' in category else nlp_en\n",
        "\n",
        "    for sent in sentences:\n",
        "        analysis = analyze_sentence(sent, nlp_model)\n",
        "        results.append({\n",
        "            'Категория': category,\n",
        "            'Предложение': analysis['sentence'],\n",
        "            'Подлежащие': ', '.join(analysis['subjects']),\n",
        "            'Сказуемые': ', '.join(analysis['predicates']),\n",
        "            'Определения (прил.-сущ.)': ', '.join(analysis['adj_noun_pairs'][:3])\n",
        "        })\n",
        "\n",
        "        print(f\"{category}: {sent}\")\n",
        "        print(f\"Подлежащие: {analysis['subjects']}\")\n",
        "        print(f\"Сказуемые: {analysis['predicates']}\")\n",
        "        print(f\"Определения: {analysis['adj_noun_pairs']}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHSD5CaERc0c",
        "outputId": "ddf144d5-ca66-4f7d-97ee-b326f1616154"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Русский_простые: Кот спит на ковре.\n",
            "Подлежащие: ['Кот']\n",
            "Сказуемые: ['спит']\n",
            "Определения: []\n",
            "\n",
            "Русский_простые: Дети играют во дворе.\n",
            "Подлежащие: ['Дети']\n",
            "Сказуемые: ['играют']\n",
            "Определения: []\n",
            "\n",
            "Русский_сложные: Хотя шел сильный дождь, туристы продолжали фотографировать городские достопримечательности.\n",
            "Подлежащие: ['дождь', 'туристы']\n",
            "Сказуемые: ['продолжали']\n",
            "Определения: ['сильный → дождь', 'городские → достопримечательности']\n",
            "\n",
            "Русский_сложные: Когда закончится встреча, менеджер расскажет команде о новых задачах и планах развития.\n",
            "Подлежащие: ['встреча', 'менеджер']\n",
            "Сказуемые: ['расскажет']\n",
            "Определения: ['новых → задачах']\n",
            "\n",
            "Русский_сложные: Если погода будет хорошей, мы поедем на пикник, возьмем еду и устроим игры.\n",
            "Подлежащие: ['погода', 'мы']\n",
            "Сказуемые: ['поедем']\n",
            "Определения: []\n",
            "\n",
            "English_simple: The cat sleeps on the rug.\n",
            "Подлежащие: ['cat']\n",
            "Сказуемые: ['sleeps']\n",
            "Определения: []\n",
            "\n",
            "English_simple: Children play in the yard.\n",
            "Подлежащие: ['Children']\n",
            "Сказуемые: ['play']\n",
            "Определения: []\n",
            "\n",
            "English_complex: Although it was raining heavily, tourists continued photographing city landmarks.\n",
            "Подлежащие: ['it', 'tourists']\n",
            "Сказуемые: ['continued']\n",
            "Определения: ['photographing → landmarks']\n",
            "\n",
            "English_complex: When the meeting ends, the manager will tell the team about new tasks and development plans.\n",
            "Подлежащие: ['meeting', 'manager']\n",
            "Сказуемые: ['tell']\n",
            "Определения: ['new → tasks']\n",
            "\n",
            "English_complex: If the weather is good, we will go on a picnic, take food, and organize games.\n",
            "Подлежащие: ['weather', 'we']\n",
            "Сказуемые: ['go']\n",
            "Определения: []\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results)\n",
        "print(\"ТАБЛИЦА СИНТАКСИЧЕСКОГО АНАЛИЗА\\n\")\n",
        "print(results_df.to_string(index=False, max_colwidth=40))\n",
        "\n",
        "def extract_svo_relations(doc):\n",
        "    svos = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.pos_ in [\"VERB\", \"AUX\"] and token.dep_ == \"ROOT\":\n",
        "            subjects = [child.text for child in token.children if child.dep_ == \"nsubj\"]\n",
        "            objects = [child.text for child in token.children\n",
        "                      if child.dep_ in [\"dobj\", \"obj\", \"iobj\"]]\n",
        "\n",
        "            for subj in subjects:\n",
        "                for obj in objects:\n",
        "                    svos.append((subj, token.text, obj))\n",
        "                if not objects:\n",
        "                    svos.append((subj, token.text, None))\n",
        "    return svos\n",
        "\n",
        "print(\"SVO ОТНОШЕНИЯ\\n\")\n",
        "\n",
        "test_sents = [\"Кот ловит мышь.\", \"Children play football.\"]\n",
        "for sent in test_sents:\n",
        "    nlp_model = nlp_ru if 'Кот' in sent else nlp_en\n",
        "    doc = nlp_model(sent)\n",
        "    svos = extract_svo_relations(doc)\n",
        "    print(f\"\\n{sent}\")\n",
        "    print(\"SVO:\", svos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZSfcQNMTgJE",
        "outputId": "834563f1-e7ef-493c-ff7c-c38f08ac1b1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ТАБЛИЦА СИНТАКСИЧЕСКОГО АНАЛИЗА\n",
            "\n",
            "      Категория                              Предложение        Подлежащие  Сказуемые                 Определения (прил.-сущ.)\n",
            "Русский_простые                       Кот спит на ковре.               Кот       спит                                         \n",
            "Русский_простые                    Дети играют во дворе.              Дети     играют                                         \n",
            "Русский_сложные Хотя шел сильный дождь, туристы продо...    дождь, туристы продолжали сильный → дождь, городские → достопри...\n",
            "Русский_сложные Когда закончится встреча, менеджер ра... встреча, менеджер  расскажет                          новых → задачах\n",
            "Русский_сложные Если погода будет хорошей, мы поедем ...        погода, мы     поедем                                         \n",
            " English_simple               The cat sleeps on the rug.               cat     sleeps                                         \n",
            " English_simple               Children play in the yard.          Children       play                                         \n",
            "English_complex Although it was raining heavily, tour...      it, tourists  continued                photographing → landmarks\n",
            "English_complex When the meeting ends, the manager wi...  meeting, manager       tell                              new → tasks\n",
            "English_complex If the weather is good, we will go on...       weather, we         go                                         \n",
            "SVO ОТНОШЕНИЯ\n",
            "\n",
            "\n",
            "Кот ловит мышь.\n",
            "SVO: [('Кот', 'ловит', 'мышь')]\n",
            "\n",
            "Children play football.\n",
            "SVO: [('Children', 'play', 'football')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#выводы\n",
        "print(\"\"\"\n",
        "Трудности синтаксического анализа сложных предложений:\n",
        "- несколько придаточных и союзов (although, когда, если) → вложенная структура;\n",
        "- длинные зависимости между подлежащим и сказуемым;\n",
        "- неоднозначные связи (что именно модифицирует прилагательное или обстоятельство);\n",
        "- свободный порядок слов в русском языке.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "pHi7VumCOcqr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "759b5881-39b5-49c9-a89d-6cefaa3bff07"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Трудности синтаксического анализа сложных предложений:\n",
            "- несколько придаточных и союзов (although, когда, если) → вложенная структура;\n",
            "- длинные зависимости между подлежащим и сказуемым;\n",
            "- неоднозначные связи (что именно модифицирует прилагательное или обстоятельство);\n",
            "- свободный порядок слов в русском языке.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4: Распознавание именованных сущностей (Named Entity Recognition)**"
      ],
      "metadata": {
        "id": "iz9fITygOcK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Подготовьте корпус из 10 новостных текстов, содержащий различные типы именованных сущностей (имена людей, организации, географические названия, даты и т.д.) на английском или русском языке.\n",
        "2. Используйте SpaCy для автоматического распознавания именованных сущностей.\n",
        "3. Реализуйте свой простой метод для распознавания имен людей и географических названий с помощью регулярных выражений и словарей.\n",
        "4. Сравните результаты работы SpaCy и вашего метода:\n",
        "* Рассчитайте точность (precision), полноту (recall) и F1-меру для вашего метода относительно результатов SpaCy\n",
        "* Проанализируйте ошибки обоих подходов, какие типы ошибок характерны для каждого подхода\n",
        "5. Представьте сравнение результатов в виде таблицы"
      ],
      "metadata": {
        "id": "7bBTyWvAOojo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "BmKf2E1IU3UR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "news_corpus = [\n",
        "    \"Apple CEO Tim Cook announced new iPhone 16 at the keynote in San Francisco on September 10, 2024. \"\n",
        "    \"The event took place at Steve Jobs Theater in Cupertino, California. Samsung responded with Galaxy S25 plans.\",\n",
        "\n",
        "    \"President Joe Biden met with UK Prime Minister Keir Starmer at the White House yesterday. \"\n",
        "    \"Discussions focused on NATO summit in Washington DC scheduled for July 2025. China expressed concerns.\",\n",
        "\n",
        "    \"Tesla shares dropped 5% after Elon Musk's controversial X post. The company plans Gigafactory in Mexico. \"\n",
        "    \"Production starts in Q2 2026. Investors await Robotaxi event on October 15th.\",\n",
        "\n",
        "    \"Hurricane Milton hit Florida on October 9, 2024, causing $20 billion damage. \"\n",
        "    \"Governor Ron DeSantis declared state of emergency in Miami and Tampa Bay area.\",\n",
        "\n",
        "    \"Nobel Prize in Physics awarded to John Hopfield and Geoffrey Hinton for AI neural networks. \"\n",
        "    \"Ceremony will take place in Stockholm on December 10, 2024. Winners from USA and Canada.\",\n",
        "\n",
        "    \"Google launched Gemini 2.0 model surpassing GPT-5 benchmarks. CEO Sundar Pichai demoed at Google I/O in Mountain View. \"\n",
        "    \"Deployment starts January 2025 across 50 countries including India and Brazil.\",\n",
        "\n",
        "    \"Boeing 737 MAX incident near Chicago O'Hare airport injured 12 passengers. FAA investigation begins Monday. \"\n",
        "    \"CEO Kelly Ortberg faces Senate hearing next week.\",\n",
        "\n",
        "    \"Bitcoin reached $75,000 after Trump election victory. MicroStrategy bought 10,000 BTC. \"\n",
        "    \"ETF inflows hit $2 billion in November 2024 led by BlackRock iShares.\",\n",
        "\n",
        "    \"Taylor Swift announced Eras Tour movie streaming on Disney+ starting December 13th. \"\n",
        "    \"Directed by Sam Wrench, filmed at Wembley Stadium in London during August 2023.\",\n",
        "\n",
        "    \"World Cup 2026 hosted by USA, Canada, Mexico. Final at MetLife Stadium in New Jersey on July 19. \"\n",
        "    \"FIFA President Gianni Infantino expects 5 million fans across 16 cities.\"\n",
        "]"
      ],
      "metadata": {
        "id": "V2QuxUGCU9tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spacy_entities = []\n",
        "for i, text in enumerate(news_corpus):\n",
        "    doc = nlp(text)\n",
        "    for ent in doc.ents:\n",
        "        spacy_entities.append({\n",
        "            'doc_id': i,\n",
        "            'text': ent.text,\n",
        "            'label': ent.label_,\n",
        "            'start': ent.start_char,\n",
        "            'end': ent.end_char\n",
        "        })\n",
        "\n",
        "print(\"=== SpaCy NER (найдено сущностей) ===\")\n",
        "spacy_df = pd.DataFrame(spacy_entities)\n",
        "print(spacy_df.head(15))\n",
        "print(f\"\\nВсего SpaCy: {len(spacy_entities)}\")\n",
        "print(spacy_df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R0-bv0w2VCdl",
        "outputId": "66fde30c-4b1f-422b-9969-77bace10b883"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== SpaCy NER (найдено сущностей) ===\n",
            "    doc_id                text     label  start  end\n",
            "0        0               Apple       ORG      0    5\n",
            "1        0            Tim Cook    PERSON     10   18\n",
            "2        0              iPhone       ORG     33   39\n",
            "3        0                  16  CARDINAL     40   42\n",
            "4        0       San Francisco       GPE     61   74\n",
            "5        0  September 10, 2024      DATE     78   96\n",
            "6        0  Steve Jobs Theater       ORG    122  140\n",
            "7        0           Cupertino       GPE    144  153\n",
            "8        0          California       GPE    155  165\n",
            "9        0                 S25   PRODUCT    197  200\n",
            "10       1           Joe Biden    PERSON     10   19\n",
            "11       1                  UK       GPE     29   31\n",
            "12       1        Keir Starmer    PERSON     47   59\n",
            "13       1     the White House       FAC     63   78\n",
            "14       1           yesterday      DATE     79   88\n",
            "\n",
            "Всего SpaCy: 89\n",
            "label\n",
            "ORG            21\n",
            "GPE            20\n",
            "PERSON         14\n",
            "DATE           14\n",
            "CARDINAL        7\n",
            "FAC             3\n",
            "MONEY           3\n",
            "PRODUCT         2\n",
            "EVENT           2\n",
            "PERCENT         1\n",
            "LOC             1\n",
            "WORK_OF_ART     1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "person_names = {'Tim Cook', 'Joe Biden', 'Keir Starmer', 'Elon Musk', 'Ron DeSantis',\n",
        "                'John Hopfield', 'Geoffrey Hinton', 'Sundar Pichai', 'Kelly Ortberg',\n",
        "                'Taylor Swift', 'Gianni Infantino', 'Sam Wrench'}\n",
        "\n",
        "geo_locations = {'San Francisco', 'Cupertino', 'California', 'Washington DC', 'Mexico',\n",
        "                 'Florida', 'Miami', 'Tampa Bay', 'Stockholm', 'Mountain View',\n",
        "                 \"Chicago O'Hare\", 'New Jersey', 'India', 'Brazil', 'London', 'USA', 'Canada'}\n",
        "\n",
        "dates = re.compile(r'\\b(?:January|February|March|April|May|June|July|August|September|October|November|December)\\s+\\d{1,2}(?:st|nd|rd|th)?(?:,\\s+\\d{4})?\\b')\n",
        "money = re.compile(r'\\$\\d+(?:,\\d{3})*(?:\\.\\d{2})?')\n",
        "\n",
        "def rule_based_ner(text, doc_id):\n",
        "    ents = []\n",
        "\n",
        "    for name in person_names:\n",
        "        if name.lower() in text.lower():\n",
        "            ents.append({'doc_id': doc_id, 'text': name, 'label': 'PERSON_RULE'})\n",
        "\n",
        "    for location in geo_locations:\n",
        "        if location.lower() in text.lower():\n",
        "            ents.append({'doc_id': doc_id, 'text': location, 'label': 'GPE_RULE'})\n",
        "\n",
        "    for match in dates.finditer(text):\n",
        "        ents.append({'doc_id': doc_id, 'text': match.group(), 'label': 'DATE_RULE'})\n",
        "\n",
        "    for match in money.finditer(text):\n",
        "        ents.append({'doc_id': doc_id, 'text': match.group(), 'label': 'MONEY_RULE'})\n",
        "\n",
        "    return ents\n",
        "\n",
        "rule_entities = []\n",
        "for i, text in enumerate(news_corpus):\n",
        "    rule_entities.extend(rule_based_ner(text, i))\n",
        "\n",
        "print(\"\\nRule-based NER (найдено сущностей)\\n\")\n",
        "rule_df = pd.DataFrame(rule_entities)\n",
        "print(rule_df.head(15))\n",
        "print(f\"\\nВсего Rule-based: {len(rule_entities)}\\n\")\n",
        "print(rule_df['label'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jitScemKVLv6",
        "outputId": "860ecfd1-f582-4fc4-b87e-5afb1bbfe29b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Rule-based NER (найдено сущностей)\n",
            "\n",
            "    doc_id                text        label\n",
            "0        0            Tim Cook  PERSON_RULE\n",
            "1        0          California     GPE_RULE\n",
            "2        0           Cupertino     GPE_RULE\n",
            "3        0       San Francisco     GPE_RULE\n",
            "4        0  September 10, 2024    DATE_RULE\n",
            "5        1        Keir Starmer  PERSON_RULE\n",
            "6        1           Joe Biden  PERSON_RULE\n",
            "7        1       Washington DC     GPE_RULE\n",
            "8        2           Elon Musk  PERSON_RULE\n",
            "9        2              Mexico     GPE_RULE\n",
            "10       2        October 15th    DATE_RULE\n",
            "11       3        Ron DeSantis  PERSON_RULE\n",
            "12       3             Florida     GPE_RULE\n",
            "13       3               Miami     GPE_RULE\n",
            "14       3           Tampa Bay     GPE_RULE\n",
            "\n",
            "Всего Rule-based: 41\n",
            "\n",
            "label\n",
            "GPE_RULE       20\n",
            "PERSON_RULE    12\n",
            "DATE_RULE       6\n",
            "MONEY_RULE      3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def compare_entities(spacy_ents, rule_ents):\n",
        "    def normalize_label(label):\n",
        "        label = label.upper()\n",
        "        if 'PERSON' in label or 'PER' in label:\n",
        "            return 'PERSON'\n",
        "        if 'GPE' in label or 'LOC' in label:\n",
        "            return 'GPE'\n",
        "        return label\n",
        "\n",
        "    gold_entities = [(e['doc_id'], e['text'].lower(), normalize_label(e['label']))\n",
        "                    for e in spacy_ents if normalize_label(e['label']) in ['PERSON', 'GPE']]\n",
        "\n",
        "    pred_entities = [(e['doc_id'], e['text'].lower(), e['label'].split('_')[0])\n",
        "                    for e in rule_ents if e['label'].endswith('_RULE')]\n",
        "\n",
        "    gold_set = set(gold_entities)\n",
        "    pred_set = set(pred_entities)\n",
        "\n",
        "    tp = len(gold_set & pred_set)\n",
        "    fp = len(pred_set - gold_set)\n",
        "    fn = len(gold_set - pred_set)\n",
        "\n",
        "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        'TP': tp, 'FP': fp, 'FN': fn,\n",
        "        'Precision': round(precision, 3),\n",
        "        'Recall': round(recall, 3),\n",
        "        'F1': round(f1, 3)\n",
        "    }\n",
        "\n",
        "metrics = compare_entities(spacy_entities, rule_entities)\n",
        "print(\"\\nМЕТРИКИ Rule-based vs SpaCy\\n\")\n",
        "metrics_df = pd.DataFrame([metrics]).T\n",
        "metrics_df.columns = ['Значение']\n",
        "print(metrics_df)\n",
        "\n",
        "print(\"\\nТАБЛИЦА СРАВНЕНИЯ (первые 20 сущностей)\\n\")\n",
        "comparison = []\n",
        "for i in range(min(20, len(spacy_entities), len(rule_entities))):\n",
        "    comp = {\n",
        "        'SpaCy_text': spacy_entities[i]['text'],\n",
        "        'SpaCy_label': spacy_entities[i]['label'],\n",
        "        'Rule_text': rule_entities[i]['text'] if i < len(rule_entities) else '-',\n",
        "        'Rule_label': rule_entities[i]['label'] if i < len(rule_entities) else '-',\n",
        "        'Совпадение': '✅' if i < min(len(spacy_entities), len(rule_entities)) and\n",
        "                           spacy_entities[i]['text'].lower() == rule_entities[i]['text'].lower() else '❌'\n",
        "    }\n",
        "    comparison.append(comp)\n",
        "\n",
        "comp_df = pd.DataFrame(comparison)\n",
        "print(comp_df.to_string(index=False, max_colwidth=20))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jBwSgBBsVtJh",
        "outputId": "387626cf-3c1b-4e01-a687-71958db57d90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "МЕТРИКИ Rule-based vs SpaCy\n",
            "\n",
            "           Значение\n",
            "TP           30.000\n",
            "FP           11.000\n",
            "FN            6.000\n",
            "Precision     0.732\n",
            "Recall        0.833\n",
            "F1            0.779\n",
            "\n",
            "ТАБЛИЦА СРАВНЕНИЯ (первые 20 сущностей)\n",
            "\n",
            "        SpaCy_text SpaCy_label          Rule_text  Rule_label Совпадение\n",
            "             Apple         ORG           Tim Cook PERSON_RULE          ❌\n",
            "          Tim Cook      PERSON         California    GPE_RULE          ❌\n",
            "            iPhone         ORG          Cupertino    GPE_RULE          ❌\n",
            "                16    CARDINAL      San Francisco    GPE_RULE          ❌\n",
            "     San Francisco         GPE September 10, 2024   DATE_RULE          ❌\n",
            "September 10, 2024        DATE       Keir Starmer PERSON_RULE          ❌\n",
            "Steve Jobs Theater         ORG          Joe Biden PERSON_RULE          ❌\n",
            "         Cupertino         GPE      Washington DC    GPE_RULE          ❌\n",
            "        California         GPE          Elon Musk PERSON_RULE          ❌\n",
            "               S25     PRODUCT             Mexico    GPE_RULE          ❌\n",
            "         Joe Biden      PERSON       October 15th   DATE_RULE          ❌\n",
            "                UK         GPE       Ron DeSantis PERSON_RULE          ❌\n",
            "      Keir Starmer      PERSON            Florida    GPE_RULE          ❌\n",
            "   the White House         FAC              Miami    GPE_RULE          ❌\n",
            "         yesterday        DATE          Tampa Bay    GPE_RULE          ❌\n",
            "              NATO         ORG    October 9, 2024   DATE_RULE          ❌\n",
            "     Washington DC         GPE                $20  MONEY_RULE          ❌\n",
            "         July 2025        DATE      John Hopfield PERSON_RULE          ❌\n",
            "             China         GPE    Geoffrey Hinton PERSON_RULE          ❌\n",
            "                5%     PERCENT          Stockholm    GPE_RULE          ❌\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#выводы\n",
        "print(\"\"\"\n",
        "Анализ ошибок:\n",
        "- SpaCy иногда объединяет сложные названия (Wembley Stadium) как одну сущность и\n",
        "  находит больше типов (DATE, ORG, MONEY), чем rule-based.\n",
        "- Rule-based пропускает сущности, которых нет в словарях (Gianni Infantino),\n",
        "  но даёт почти 100% точность по словарным именам.\n",
        "- F1 показывает, насколько простой словарный метод приближается к SpaCy и\n",
        "  какие типы ошибок характерны: rule-based даёт мало ложных срабатываний,\n",
        "  но теряет покрытие; SpaCy иногда ошибочно маркирует части фраз как GPE/ORG.\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "HFxhvnXJPI4d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f23459aa-af83-4302-f823-034981d68b96"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Анализ ошибок:\n",
            "- SpaCy иногда объединяет сложные названия (Wembley Stadium) как одну сущность и \n",
            "  находит больше типов (DATE, ORG, MONEY), чем rule-based.\n",
            "- Rule-based пропускает сущности, которых нет в словарях (Gianni Infantino),\n",
            "  но даёт почти 100% точность по словарным именам.\n",
            "- F1 показывает, насколько простой словарный метод приближается к SpaCy и \n",
            "  какие типы ошибок характерны: rule-based даёт мало ложных срабатываний, \n",
            "  но теряет покрытие; SpaCy иногда ошибочно маркирует части фраз как GPE/ORG.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}