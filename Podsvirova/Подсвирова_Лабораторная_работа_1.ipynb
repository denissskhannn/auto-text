{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 0. Загрузите необходимые библиотеки**"
      ],
      "metadata": {
        "id": "VL_YISip2qbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy\n",
        "!pip install pymystem3\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ru_core_news_sm\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pymystem3 import Mystem\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ueCqHoV1yFg0",
        "outputId": "d5cbf831-4351-4e0f-e84f-a7cc6fd50ceb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pymystem3) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2025.11.12)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Using cached https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "Requirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ru-core-news-sm==3.8.0) (2.0.6)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1. Загрузка данных**"
      ],
      "metadata": {
        "id": "0YUM4z_92XzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вставьте текст для обработки согласно вашему варианту"
      ],
      "metadata": {
        "id": "hwqI1lLF2j3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"SpaceX планирует отправить первую пилотируемую миссию на Марс к 2029 году, несмотря на множество технических проблем.\n",
        "Марсианский грунт содержит до 60% перхлоратов — токсичных соединений, представляющих опасность для человека.\n",
        "Radiation на поверхности Красной планеты в 2,5 раза превышает допустимые нормы для астронавтов NASA!\n",
        "\"Мы разрабатываем специальные Martian habitats, защищающие от космической радиации и перепадов температур от +20°C до -153°C,\" — сообщает главный инженер проекта.\n",
        "Одна из ключевых технологий — ISRU (In-Situ Resource Utilization), позволяющая производить кислород из CO₂ марсианской атмосферы.\n",
        "Марсоход Perseverance успешно протестировал этот процесс в 2022 году, получив 12 г O₂ за час работы.\n",
        "Российско-европейский проект ExoMars-2028 предполагает бурение на глубину до 2 м для поиска следов древней жизни.\n",
        "По оценкам экспертов, первая самодостаточная колония на Марсе численностью 1000+ человек может появиться к 2050 году.\"\"\""
      ],
      "metadata": {
        "id": "B2Tsy8T62ZRo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2. Нормализация текста.**"
      ],
      "metadata": {
        "id": "NYmTHvNm2ph6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведите текст к нижнему регистру, удалите лишние пробоелы, переносы строк, спецсимволы, пунктуацию, обработайте цифры."
      ],
      "metadata": {
        "id": "XgLHFLVp3Apg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[©®™℠]+', ' ', text)\n",
        "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
        "    text = text.replace('ё', 'е')\n",
        "    text = re.sub(r'(\\w)-(\\w)', r'\\1HYPHEN\\2', text)\n",
        "    text = re.sub(r'[^\\w\\sHYPHEN]', ' ', text)\n",
        "    text = re.sub(r'HYPHEN', '-', text)\n",
        "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "normalized_text = normalize_text(text)\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "id": "KiyJLZrm3Z8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74332678-fb53-4866-f692-e4341ef7018d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "spacex планирует отправить первую пилотируемую миссию на марс к году несмотря на множество технических проблем марсианский грунт содержит до перхлоратов токсичных соединений представляющих опасность для человека radiation на поверхности красной планеты в раза превышает допустимые нормы для астронавтов nasa мы разрабатываем специальные martian habitats защищающие от космической радиации и перепадов температур от c до c сообщает главный инженер проекта одна из ключевых технологий isru позволяющая производить кислород из co₂ марсианской атмосферы марсоход perseverance успешно протестировал этот процесс в году получив г o₂ за час работы российско-европейский проект exomars- предполагает бурение на глубину до м для поиска следов древней жизни по оценкам экспертов первая самодостаточная колония на марсе численностью человек может появиться к году\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3. Токенизация**"
      ],
      "metadata": {
        "id": "I6evm-5d3dPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизируйте текст."
      ],
      "metadata": {
        "id": "O90JD4L83gbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(normalized_text):\n",
        "    tokens = re.findall(r'\\b[\\w\\-]+\\b', normalized_text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "tokenized_text = tokenize(normalized_text)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "QjGIQM3F3udR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e123291a-b3b4-4eed-f3b7-53b8080bcd12"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spacex', 'планирует', 'отправить', 'первую', 'пилотируемую', 'миссию', 'на', 'марс', 'к', 'году', 'несмотря', 'на', 'множество', 'технических', 'проблем', 'марсианский', 'грунт', 'содержит', 'до', 'перхлоратов', 'токсичных', 'соединений', 'представляющих', 'опасность', 'для', 'человека', 'radiation', 'на', 'поверхности', 'красной', 'планеты', 'в', 'раза', 'превышает', 'допустимые', 'нормы', 'для', 'астронавтов', 'nasa', 'мы', 'разрабатываем', 'специальные', 'martian', 'habitats', 'защищающие', 'от', 'космической', 'радиации', 'и', 'перепадов', 'температур', 'от', 'c', 'до', 'c', 'сообщает', 'главный', 'инженер', 'проекта', 'одна', 'из', 'ключевых', 'технологий', 'isru', 'позволяющая', 'производить', 'кислород', 'из', 'co₂', 'марсианской', 'атмосферы', 'марсоход', 'perseverance', 'успешно', 'протестировал', 'этот', 'процесс', 'в', 'году', 'получив', 'г', 'o₂', 'за', 'час', 'работы', 'российско-европейский', 'проект', 'exomars', 'предполагает', 'бурение', 'на', 'глубину', 'до', 'м', 'для', 'поиска', 'следов', 'древней', 'жизни', 'по', 'оценкам', 'экспертов', 'первая', 'самодостаточная', 'колония', 'на', 'марсе', 'численностью', 'человек', 'может', 'появиться', 'к', 'году']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4. Удаление стоп-слов**"
      ],
      "metadata": {
        "id": "x0ED6dCL3vrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведите 2 списка - 1. Очищенных токенов, 2. Список удаленных стоп-слов"
      ],
      "metadata": {
        "id": "awJNjV8y30zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "\n",
        "filtered_tokens = []\n",
        "removed_stopwords = []\n",
        "\n",
        "for token in tokenized_text:\n",
        "    if token in russian_stopwords:\n",
        "        removed_stopwords.append(token)\n",
        "    else:\n",
        "        filtered_tokens.append(token)\n",
        "\n",
        "print(filtered_tokens)\n",
        "print(removed_stopwords)"
      ],
      "metadata": {
        "id": "c9uFV9dG39L2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a6b7f2b2-b2da-48ec-f10f-ec830ed398b8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['spacex', 'планирует', 'отправить', 'первую', 'пилотируемую', 'миссию', 'марс', 'году', 'несмотря', 'множество', 'технических', 'проблем', 'марсианский', 'грунт', 'содержит', 'перхлоратов', 'токсичных', 'соединений', 'представляющих', 'опасность', 'человека', 'radiation', 'поверхности', 'красной', 'планеты', 'раза', 'превышает', 'допустимые', 'нормы', 'астронавтов', 'nasa', 'разрабатываем', 'специальные', 'martian', 'habitats', 'защищающие', 'космической', 'радиации', 'перепадов', 'температур', 'c', 'c', 'сообщает', 'главный', 'инженер', 'проекта', 'одна', 'ключевых', 'технологий', 'isru', 'позволяющая', 'производить', 'кислород', 'co₂', 'марсианской', 'атмосферы', 'марсоход', 'perseverance', 'успешно', 'протестировал', 'процесс', 'году', 'получив', 'г', 'o₂', 'час', 'работы', 'российско-европейский', 'проект', 'exomars', 'предполагает', 'бурение', 'глубину', 'м', 'поиска', 'следов', 'древней', 'жизни', 'оценкам', 'экспертов', 'первая', 'самодостаточная', 'колония', 'марсе', 'численностью', 'человек', 'появиться', 'году']\n",
            "['на', 'к', 'на', 'до', 'для', 'на', 'в', 'для', 'мы', 'от', 'и', 'от', 'до', 'из', 'из', 'этот', 'в', 'за', 'на', 'до', 'для', 'по', 'на', 'может', 'к']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 5. Лемматизация и стемминг**"
      ],
      "metadata": {
        "id": "sbV8P56O3-ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примените к токенам алгоритмы лемматизации и стемминга. Выведите 2 списка - 1. Лемматизированные токены 2. Стемматизированные токены"
      ],
      "metadata": {
        "id": "OZ1A8VhK4HTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_stemmer = SnowballStemmer(\"english\")\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "mystem = Mystem()\n",
        "lemmatizer_en = WordNetLemmatizer()\n",
        "\n",
        "def is_cyrillic(token):\n",
        "    return any('а' <= char <= 'я' or char == 'ё' for char in token)\n",
        "\n",
        "def is_latin(token):\n",
        "    return any('a' <= char <= 'z' for char in token)\n",
        "\n",
        "def stem_words(filtered_tokens):\n",
        "\n",
        "    stemmed_tokens = []\n",
        "\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                stem = russian_stemmer.stem(token)\n",
        "                stemmed_tokens.append(stem)\n",
        "\n",
        "            elif is_latin(token):\n",
        "                stem = english_stemmer.stem(token)\n",
        "                stemmed_tokens.append(stem)\n",
        "    return stemmed_tokens\n",
        "\n",
        "def lemm_words(filtered_tokens):\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                lemma = mystem.lemmatize(token)[0]\n",
        "                lemmatized_tokens.append(lemma)\n",
        "\n",
        "            elif is_latin(token):\n",
        "                lemma = lemmatizer_en.lemmatize(token)\n",
        "                lemmatized_tokens.append(lemma)\n",
        "    return lemmatized_tokens\n",
        "\n",
        "stemmed_tokens = stem_words(filtered_tokens)\n",
        "lemmatized_tokens = lemm_words(filtered_tokens)\n",
        "\n",
        "print('Стемматизированные токены: ', stemmed_tokens)\n",
        "print('Лемматизированные токены: ', lemmatized_tokens)"
      ],
      "metadata": {
        "id": "OrCmA4tt4WH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a3c386e1-09bd-4b8c-cf00-8a3ad29bd4b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стемматизированные токены:  ['spacex', 'планир', 'отправ', 'перв', 'пилотируем', 'мисс', 'марс', 'год', 'несмотр', 'множеств', 'техническ', 'пробл', 'марсианск', 'грунт', 'содерж', 'перхлорат', 'токсичн', 'соединен', 'представля', 'опасн', 'человек', 'radiat', 'поверхн', 'красн', 'планет', 'раз', 'превыша', 'допустим', 'норм', 'астронавт', 'nasa', 'разрабатыва', 'специальн', 'martian', 'habitat', 'защища', 'космическ', 'радиац', 'перепад', 'температур', 'c', 'c', 'сообща', 'главн', 'инженер', 'проект', 'одн', 'ключев', 'технолог', 'isru', 'позволя', 'производ', 'кислород', 'марсианск', 'атмосфер', 'марсоход', 'persever', 'успешн', 'протестирова', 'процесс', 'год', 'получ', 'г', 'час', 'работ', 'проект', 'exomar', 'предполага', 'бурен', 'глубин', 'м', 'поиск', 'след', 'древн', 'жизн', 'оценк', 'эксперт', 'перв', 'самодостаточн', 'колон', 'марс', 'числен', 'человек', 'появ', 'год']\n",
            "Лемматизированные токены:  ['spacex', 'планировать', 'отправлять', 'первый', 'пилотировать', 'миссия', 'марс', 'год', 'несмотря', 'множество', 'технический', 'проблема', 'марсианский', 'грунт', 'содержать', 'перхлорат', 'токсичный', 'соединение', 'представлять', 'опасность', 'человек', 'radiation', 'поверхность', 'красный', 'планета', 'раз', 'превышать', 'допустимый', 'норма', 'астронавт', 'nasa', 'разрабатывать', 'специальный', 'martian', 'habitat', 'защищать', 'космический', 'радиация', 'перепад', 'температура', 'c', 'c', 'сообщать', 'главный', 'инженер', 'проект', 'один', 'ключевой', 'технология', 'isru', 'позволять', 'производить', 'кислород', 'марсианский', 'атмосфера', 'марсоход', 'perseverance', 'успешно', 'протестировать', 'процесс', 'год', 'получать', 'г', 'час', 'работа', 'проект', 'exomars', 'предполагать', 'бурение', 'глубина', 'м', 'поиск', 'след', 'древний', 'жизнь', 'оценка', 'эксперт', 'первый', 'самодостаточный', 'колония', 'марс', 'численность', 'человек', 'появляться', 'год']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 6. Напишите функцию для препроцессинга текста**"
      ],
      "metadata": {
        "id": "R_R2xPrh4bW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объедините все шаги в одну функцию. Выведите результат с лемматизированным списком"
      ],
      "metadata": {
        "id": "drJR_Rff4j_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_cyrillic(token):\n",
        "    return any('а' <= ch <= 'я' or ch == 'ё' for ch in token.lower())\n",
        "\n",
        "def is_latin(token):\n",
        "    return any('a' <= ch <= 'z' for ch in token.lower())\n",
        "\n",
        "def big_function(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[©®™℠]+', ' ', text)\n",
        "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
        "    text = text.replace('ё', 'е')\n",
        "    text = re.sub(r'(\\w)-(\\w)', r'\\1HYPHEN\\2', text)\n",
        "    text = re.sub(r'[^\\w\\sHYPHEN]', ' ', text)\n",
        "    text = re.sub(r'HYPHEN', '-', text)\n",
        "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    tokens = re.findall(r'\\b[\\w\\-]+\\b', text)\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "\n",
        "    stemmed_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                stemmed_tokens.append(russian_stemmer.stem(token))\n",
        "            elif is_latin(token):\n",
        "                stemmed_tokens.append(english_stemmer.stem(token))\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                lemmatized_tokens.append(mystem.lemmatize(token)[0])\n",
        "            elif is_latin(token):\n",
        "                lemmatized_tokens.append(lemmatizer_en.lemmatize(token))\n",
        "\n",
        "    return {\n",
        "        'tokens': tokens,\n",
        "        'filtered_tokens': filtered_tokens,\n",
        "        'stemmed_tokens': stemmed_tokens,\n",
        "        'lemmatized_tokens': lemmatized_tokens\n",
        "    }\n",
        "\n",
        "result = big_function(text)\n",
        "\n",
        "print('Лемматизированные токены:', result['lemmatized_tokens'])"
      ],
      "metadata": {
        "id": "RUE3jJp940iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "560fa23c-8b29-47ef-b9d4-4a1d2d32d3c9"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные токены: ['spacex', 'планировать', 'отправлять', 'первый', 'пилотировать', 'миссия', 'марс', 'год', 'несмотря', 'множество', 'технический', 'проблема', 'марсианский', 'грунт', 'содержать', 'перхлорат', 'токсичный', 'соединение', 'представлять', 'опасность', 'человек', 'radiation', 'поверхность', 'красный', 'планета', 'раз', 'превышать', 'допустимый', 'норма', 'астронавт', 'nasa', 'разрабатывать', 'специальный', 'martian', 'habitat', 'защищать', 'космический', 'радиация', 'перепад', 'температура', 'c', 'c', 'сообщать', 'главный', 'инженер', 'проект', 'один', 'ключевой', 'технология', 'isru', 'позволять', 'производить', 'кислород', 'марсианский', 'атмосфера', 'марсоход', 'perseverance', 'успешно', 'протестировать', 'процесс', 'год', 'получать', 'г', 'час', 'работа', 'проект', 'exomars', 'предполагать', 'бурение', 'глубина', 'м', 'поиск', 'след', 'древний', 'жизнь', 'оценка', 'эксперт', 'первый', 'самодостаточный', 'колония', 'марс', 'численность', 'человек', 'появляться', 'год']\n"
          ]
        }
      ]
    }
  ]
}