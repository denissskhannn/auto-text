{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 0. Загрузите необходимые библиотеки**"
      ],
      "metadata": {
        "id": "VL_YISip2qbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy\n",
        "!pip install pymystem3\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ru_core_news_sm\n",
        "import spacy\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import re\n",
        "from nltk.stem import SnowballStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from pymystem3 import Mystem\n",
        "import pandas as pd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ueCqHoV1yFg0",
        "outputId": "e23f0905-fef2-47e4-f230-333e5e8728d3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Requirement already satisfied: pymystem3 in /usr/local/lib/python3.12/dist-packages (0.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from pymystem3) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->pymystem3) (2025.11.12)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ru-core-news-sm==3.8.0) (2.0.6)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1. Загрузка данных**"
      ],
      "metadata": {
        "id": "0YUM4z_92XzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вставьте текст для обработки согласно вашему варианту"
      ],
      "metadata": {
        "id": "hwqI1lLF2j3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"В 2025 году команда ResearchLab_42 запустила пилотный проект Digital Culture Mapping,\n",
        "цель которого — анализ user-generated content (UGC) из соцсетей и мессенджеров (Telegram, X/Twitter, VK).\n",
        "По словам project manager’а, KPI выросли на 37% по сравнению с Q4 2024, несмотря на budget cut в −15%.\n",
        "Основные данные собирались через API, затем обрабатывались в Python (NLTK, spaCy, pymystem3).\n",
        "В логах часто появлялись строки вида: [WARN][12:45:07] Token \"update2025_version2\" skipped. Внутренние отчёты\n",
        "сохранялись как PDF (size ≈ 2.4 MB) и DOCX-файлы. Интересно, что в текстах часто встречались гибридные конструкции:\n",
        "fake news, инфодемия, post-truth эпоха, fact-checking, soft power, hard skills vs. soft skills.\n",
        "Некоторые респонденты писали: “Это было totally unexpected”, “Я апдейтнул файл вчера в 23:59”,\n",
        "“Meeting перенесли на Monday”. В итоговом summary (≈ 1 250 words) отмечалось, что частотность аббревиатур\n",
        "типа AI, NLP, LLM, GPT-4, EU, USA превышает норму для академического дискурса, что подтверждает гипотезу о гибридизации научного и медийного регистров.\"\"\""
      ],
      "metadata": {
        "id": "B2Tsy8T62ZRo"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2. Нормализация текста.**"
      ],
      "metadata": {
        "id": "NYmTHvNm2ph6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведите текст к нижнему регистру, удалите лишние пробоелы, переносы строк, спецсимволы, пунктуацию, обработайте цифры."
      ],
      "metadata": {
        "id": "XgLHFLVp3Apg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[©®™℠]+', ' ', text)\n",
        "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
        "    text = text.replace('ё', 'е')\n",
        "    text = re.sub(r'(\\w)-(\\w)', r'\\1HYPHEN\\2', text)\n",
        "    text = re.sub(r'[^\\w\\sHYPHEN]', ' ', text)\n",
        "    text = re.sub(r'HYPHEN', '-', text)\n",
        "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "normalized_text = normalize_text(text)\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "id": "KiyJLZrm3Z8s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1cce818-8557-4ab9-fc1e-f1c6d818fb24"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "я апдейтнул файл вчера в meeting перенесли на monday в итоговом summary отмечалось что частотность аббревиатур типа ai nlp llm gpt- eu usa превышает норму для академического дискурса что подтверждает гипотезу о гибридизации научного и медийного регистров\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3. Токенизация**"
      ],
      "metadata": {
        "id": "I6evm-5d3dPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизируйте текст."
      ],
      "metadata": {
        "id": "O90JD4L83gbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(normalized_text):\n",
        "    tokens = re.findall(r'\\b[\\w\\-]+\\b', normalized_text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "tokenized_text = tokenize(normalized_text)\n",
        "print(tokenized_text)"
      ],
      "metadata": {
        "id": "QjGIQM3F3udR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1c41fa6-c558-452e-bdce-ab1f1690958e"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['в', 'году', 'команда', 'researchlab_42', 'запустила', 'пилотный', 'проект', 'digital', 'culture', 'mapping', 'цель', 'которого', 'анализ', 'user-generated', 'content', 'из', 'соцсетей', 'и', 'мессенджеров', 'по', 'словам', 'project', 'manager', 'а', 'kpi', 'выросли', 'на', 'по', 'сравнению', 'с', 'q4', 'несмотря', 'на', 'budget', 'cut', 'в', 'основные', 'данные', 'собирались', 'через', 'api', 'затем', 'обрабатывались', 'в', 'python', 'в', 'логах', 'часто', 'появлялись', 'строки', 'вида', 'warn', 'token', 'update2025_version2', 'skipped', 'внутренние', 'отчеты', 'сохранялись', 'как', 'pdf', 'и', 'docx-файлы', 'интересно', 'что', 'в', 'текстах', 'часто', 'встречались', 'гибридные', 'конструкции', 'fake', 'news', 'инфодемия', 'post-truth', 'эпоха', 'fact-checking', 'soft', 'power', 'hard', 'skills', 'vs', 'soft', 'skills', 'некоторые', 'респонденты', 'писали', 'это', 'было', 'totally', 'unexpected', 'я', 'апдейтнул', 'файл', 'вчера', 'в', 'meeting', 'перенесли', 'на', 'monday', 'в', 'итоговом', 'summary', 'отмечалось', 'что', 'частотность', 'аббревиатур', 'типа', 'ai', 'nlp', 'llm', 'gpt', 'eu', 'usa', 'превышает', 'норму', 'для', 'академического', 'дискурса', 'что', 'подтверждает', 'гипотезу', 'о', 'гибридизации', 'научного', 'и', 'медийного', 'регистров']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4. Удаление стоп-слов**"
      ],
      "metadata": {
        "id": "x0ED6dCL3vrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведите 2 списка - 1. Очищенных токенов, 2. Список удаленных стоп-слов"
      ],
      "metadata": {
        "id": "awJNjV8y30zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "\n",
        "filtered_tokens = []\n",
        "removed_stopwords = []\n",
        "\n",
        "for token in tokenized_text:\n",
        "    if token in russian_stopwords:\n",
        "        removed_stopwords.append(token)\n",
        "    else:\n",
        "        filtered_tokens.append(token)\n",
        "\n",
        "print(filtered_tokens)\n",
        "print(removed_stopwords)"
      ],
      "metadata": {
        "id": "c9uFV9dG39L2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08cd706-782a-4a74-ca84-563c28af7ff9"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['году', 'команда', 'researchlab_42', 'запустила', 'пилотный', 'проект', 'digital', 'culture', 'mapping', 'цель', 'которого', 'анализ', 'user-generated', 'content', 'соцсетей', 'мессенджеров', 'словам', 'project', 'manager', 'kpi', 'выросли', 'сравнению', 'q4', 'несмотря', 'budget', 'cut', 'основные', 'данные', 'собирались', 'api', 'затем', 'обрабатывались', 'python', 'логах', 'часто', 'появлялись', 'строки', 'вида', 'warn', 'token', 'update2025_version2', 'skipped', 'внутренние', 'отчеты', 'сохранялись', 'pdf', 'docx-файлы', 'интересно', 'текстах', 'часто', 'встречались', 'гибридные', 'конструкции', 'fake', 'news', 'инфодемия', 'post-truth', 'эпоха', 'fact-checking', 'soft', 'power', 'hard', 'skills', 'vs', 'soft', 'skills', 'некоторые', 'респонденты', 'писали', 'это', 'totally', 'unexpected', 'апдейтнул', 'файл', 'вчера', 'meeting', 'перенесли', 'monday', 'итоговом', 'summary', 'отмечалось', 'частотность', 'аббревиатур', 'типа', 'ai', 'nlp', 'llm', 'gpt', 'eu', 'usa', 'превышает', 'норму', 'академического', 'дискурса', 'подтверждает', 'гипотезу', 'гибридизации', 'научного', 'медийного', 'регистров']\n",
            "['в', 'из', 'и', 'по', 'а', 'на', 'по', 'с', 'на', 'в', 'через', 'в', 'в', 'как', 'и', 'что', 'в', 'было', 'я', 'в', 'на', 'в', 'что', 'для', 'что', 'о', 'и']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 5. Лемматизация и стемминг**"
      ],
      "metadata": {
        "id": "sbV8P56O3-ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примените к токенам алгоритмы лемматизации и стемминга. Выведите 2 списка - 1. Лемматизированные токены 2. Стемматизированные токены"
      ],
      "metadata": {
        "id": "OZ1A8VhK4HTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "english_stemmer = SnowballStemmer(\"english\")\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "nltk.download('wordnet')\n",
        "\n",
        "mystem = Mystem()\n",
        "lemmatizer_en = WordNetLemmatizer()\n",
        "\n",
        "def is_cyrillic(token):\n",
        "    return any('а' <= char <= 'я' or char == 'ё' for char in token)\n",
        "\n",
        "def is_latin(token):\n",
        "    return any('a' <= char <= 'z' for char in token)\n",
        "\n",
        "def stem_words(filtered_tokens):\n",
        "\n",
        "    stemmed_tokens = []\n",
        "\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                stem = russian_stemmer.stem(token)\n",
        "                stemmed_tokens.append(stem)\n",
        "\n",
        "            elif is_latin(token):\n",
        "                stem = english_stemmer.stem(token)\n",
        "                stemmed_tokens.append(stem)\n",
        "    return stemmed_tokens\n",
        "\n",
        "def lemm_words(filtered_tokens):\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                lemma = mystem.lemmatize(token)[0]\n",
        "                lemmatized_tokens.append(lemma)\n",
        "\n",
        "            elif is_latin(token):\n",
        "                lemma = lemmatizer_en.lemmatize(token)\n",
        "                lemmatized_tokens.append(lemma)\n",
        "    return lemmatized_tokens\n",
        "\n",
        "stemmed_tokens = stem_words(filtered_tokens)\n",
        "lemmatized_tokens = lemm_words(filtered_tokens)\n",
        "\n",
        "print('Стемматизированные токены: ', stemmed_tokens)\n",
        "print('Лемматизированные токены: ', lemmatized_tokens)"
      ],
      "metadata": {
        "id": "OrCmA4tt4WH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "713fb293-e6b0-4ea9-813a-4c526c01a433"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стемматизированные токены:  ['год', 'команд', 'запуст', 'пилотн', 'проект', 'digit', 'cultur', 'map', 'цел', 'котор', 'анализ', 'content', 'соцсет', 'мессенджер', 'слов', 'project', 'manag', 'kpi', 'выросл', 'сравнен', 'несмотр', 'budget', 'cut', 'основн', 'дан', 'собира', 'api', 'зат', 'обрабатыва', 'python', 'лог', 'част', 'появля', 'строк', 'вид', 'warn', 'token', 'skip', 'внутрен', 'отчет', 'сохраня', 'pdf', 'интересн', 'текст', 'част', 'встреча', 'гибридн', 'конструкц', 'fake', 'news', 'инфодем', 'эпох', 'soft', 'power', 'hard', 'skill', 'vs', 'soft', 'skill', 'некотор', 'респондент', 'писа', 'эт', 'total', 'unexpect', 'апдейтнул', 'файл', 'вчер', 'meet', 'перенесл', 'monday', 'итогов', 'summari', 'отмеча', 'частотн', 'аббревиатур', 'тип', 'ai', 'nlp', 'llm', 'gpt', 'eu', 'usa', 'превыша', 'норм', 'академическ', 'дискурс', 'подтвержда', 'гипотез', 'гибридизац', 'научн', 'медийн', 'регистр']\n",
            "Лемматизированные токены:  ['год', 'команда', 'запускать', 'пилотный', 'проект', 'digital', 'culture', 'mapping', 'цель', 'который', 'анализ', 'content', 'соцсеть', 'мессенджер', 'слово', 'project', 'manager', 'kpi', 'вырастать', 'сравнение', 'несмотря', 'budget', 'cut', 'основной', 'данные', 'собираться', 'api', 'затем', 'обрабатываться', 'python', 'лог', 'часто', 'появляться', 'строка', 'вид', 'warn', 'token', 'skipped', 'внутренний', 'отчет', 'сохраняться', 'pdf', 'интересно', 'текст', 'часто', 'встречаться', 'гибридный', 'конструкция', 'fake', 'news', 'инфодемия', 'эпоха', 'soft', 'power', 'hard', 'skill', 'v', 'soft', 'skill', 'некоторые', 'респондент', 'писать', 'это', 'totally', 'unexpected', 'апдейтнуть', 'файл', 'вчера', 'meeting', 'перенести', 'monday', 'итоговый', 'summary', 'отмечаться', 'частотность', 'аббревиатура', 'тип', 'ai', 'nlp', 'llm', 'gpt', 'eu', 'usa', 'превышать', 'норма', 'академический', 'дискурс', 'подтверждать', 'гипотеза', 'гибридизация', 'научный', 'медийный', 'регистр']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 6. Напишите функцию для препроцессинга текста**"
      ],
      "metadata": {
        "id": "R_R2xPrh4bW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объедините все шаги в одну функцию. Выведите результат с лемматизированным списком"
      ],
      "metadata": {
        "id": "drJR_Rff4j_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_cyrillic(token):\n",
        "    return any('а' <= ch <= 'я' or ch == 'ё' for ch in token.lower())\n",
        "\n",
        "def is_latin(token):\n",
        "    return any('a' <= ch <= 'z' for ch in token.lower())\n",
        "\n",
        "def big_function(text):\n",
        "\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[©®™℠]+', ' ', text)\n",
        "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
        "    text = text.replace('ё', 'е')\n",
        "    text = re.sub(r'(\\w)-(\\w)', r'\\1HYPHEN\\2', text)\n",
        "    text = re.sub(r'[^\\w\\sHYPHEN]', ' ', text)\n",
        "    text = re.sub(r'HYPHEN', '-', text)\n",
        "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    tokens = re.findall(r'\\b[\\w\\-]+\\b', text)\n",
        "\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "\n",
        "    stemmed_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                stemmed_tokens.append(russian_stemmer.stem(token))\n",
        "            elif is_latin(token):\n",
        "                stemmed_tokens.append(english_stemmer.stem(token))\n",
        "\n",
        "    lemmatized_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        if token.isalpha():\n",
        "            if is_cyrillic(token):\n",
        "                lemmatized_tokens.append(mystem.lemmatize(token)[0])\n",
        "            elif is_latin(token):\n",
        "                lemmatized_tokens.append(lemmatizer_en.lemmatize(token))\n",
        "\n",
        "    return {\n",
        "        'tokens': tokens,\n",
        "        'filtered_tokens': filtered_tokens,\n",
        "        'stemmed_tokens': stemmed_tokens,\n",
        "        'lemmatized_tokens': lemmatized_tokens\n",
        "    }\n",
        "\n",
        "result = big_function(text)\n",
        "\n",
        "print('Лемматизированные токены:', result['lemmatized_tokens'])"
      ],
      "metadata": {
        "id": "RUE3jJp940iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "794983d0-e574-492e-9b02-feecdc960944"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные токены: ['год', 'команда', 'запускать', 'пилотный', 'проект', 'digital', 'culture', 'mapping', 'цель', 'который', 'анализ', 'content', 'соцсеть', 'мессенджер', 'слово', 'project', 'manager', 'kpi', 'вырастать', 'сравнение', 'несмотря', 'budget', 'cut', 'основной', 'данные', 'собираться', 'api', 'затем', 'обрабатываться', 'python', 'лог', 'часто', 'появляться', 'строка', 'вид', 'warn', 'token', 'skipped', 'внутренний', 'отчет', 'сохраняться', 'pdf', 'интересно', 'текст', 'часто', 'встречаться', 'гибридный', 'конструкция', 'fake', 'news', 'инфодемия', 'эпоха', 'soft', 'power', 'hard', 'skill', 'v', 'soft', 'skill', 'некоторые', 'респондент', 'писать', 'это', 'totally', 'unexpected', 'апдейтнуть', 'файл', 'вчера', 'meeting', 'перенести', 'monday', 'итоговый', 'summary', 'отмечаться', 'частотность', 'аббревиатура', 'тип', 'ai', 'nlp', 'llm', 'gpt', 'eu', 'usa', 'превышать', 'норма', 'академический', 'дискурс', 'подтверждать', 'гипотеза', 'гибридизация', 'научный', 'медийный', 'регистр']\n"
          ]
        }
      ]
    }
  ]
}