{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 0. Загрузите необходимые библиотеки**"
      ],
      "metadata": {
        "id": "VL_YISip2qbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy pymorphy2 natasha\n",
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slYHHNxbw2_i",
        "outputId": "c1a2c503-2113-48a3-8360-c1d5099221c9"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.21.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.2.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2026.1.4)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Collecting sortedcontainers (from intervaltree>=3->ipymarkup>=0.8.0->natasha)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Downloading intervaltree-3.2.1-py2.py3-none-any.whl (25 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Building wheels for collected packages: docopt\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=457120c9cfee566c44c1f4f89527306e8ffa6be7d7020386b6d782a791df7e3a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "Successfully built docopt\n",
            "Installing collected packages: sortedcontainers, razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.2.1 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 sortedcontainers-2.4.0 yargy-0.16.0\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m104.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m56.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1. Загрузка данных**"
      ],
      "metadata": {
        "id": "0YUM4z_92XzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вставьте текст для обработки согласно вашему варианту"
      ],
      "metadata": {
        "id": "hwqI1lLF2j3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Синтетическая биология — междисциплинарная область, объединяющая инженерный подход с генетикой.\n",
        "\n",
        "В 2024 году методы CRISPR/Cas9 позволяют редактировать геном с точностью до 1 нуклеотида!\n",
        "\n",
        "Компания Ginkgo Bioworks создала дрожжи, производящие 16 различных ароматических соединений, что удешевило парфюмерное производство на 68%.\n",
        "\n",
        "\"Bio-engineering становится новой промышленной революцией XXI века,\" — считает профессор Ли из MIT.\n",
        "\n",
        "Микроорганизмы, выращенные в 5000-литровых биореакторах, синтезируют материалы, идентичные шёлку и кожи, без участия животных.\n",
        "\n",
        "В 2023 году стартап NatureLoop представил biodegradable пластик на основе целлюлозы, который разлагается в почве за 90 дней.\n",
        "\n",
        "Искусственные бактерии способны превращать отходы агропромышленности в биотопливо с эффективностью до 72%!\n",
        "\n",
        "Согласно прогнозам, к 2030 году до 35% химической промышленности перейдет на bio-based производство.\n",
        "\n",
        "Этические вопросы регулирования новой отрасли обсуждаются в рамках международного соглашения Biological Code Act (BCA-2025).\"\"\""
      ],
      "metadata": {
        "id": "B2Tsy8T62ZRo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2. Нормализация текста.**"
      ],
      "metadata": {
        "id": "NYmTHvNm2ph6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведите текст к нижнему регистру, удалите лишние пробелы, переносы строк, спецсимволы, пунктуацию, обработайте цифры."
      ],
      "metadata": {
        "id": "XgLHFLVp3Apg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "import re\n",
        "text = text.lower()\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "text = re.sub(r'\\d+', ' ', text)\n",
        "text = re.sub(r'\\n', ' ', text)\n",
        "text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "text = \" \".join(text.split())"
      ],
      "metadata": {
        "id": "KiyJLZrm3Z8s"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3. Токенизация**"
      ],
      "metadata": {
        "id": "I6evm-5d3dPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизируйте текст."
      ],
      "metadata": {
        "id": "O90JD4L83gbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "tokens = text.split()"
      ],
      "metadata": {
        "id": "QjGIQM3F3udR"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4. Удаление стоп-слов**"
      ],
      "metadata": {
        "id": "x0ED6dCL3vrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведите 2 списка - 1. Очищенных токенов, 2. Список удаленных стоп-слов"
      ],
      "metadata": {
        "id": "awJNjV8y30zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "stop_words = [token for token in tokens if token in russian_stopwords]\n",
        "print(f'Очищенные токены:\\n{filtered_tokens}')\n",
        "print(f'\\nУдаленные стоп-слова:\\n{stop_words}')"
      ],
      "metadata": {
        "id": "c9uFV9dG39L2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d787263-8037-4d6e-e9e0-991ebbc9d938"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Очищенные токены:\n",
            "['синтетическая', 'биология', 'междисциплинарная', 'область', 'объединяющая', 'инженерный', 'подход', 'генетикой', 'году', 'методы', 'crispr', 'cas', 'позволяют', 'редактировать', 'геном', 'точностью', 'нуклеотида', 'компания', 'ginkgo', 'bioworks', 'создала', 'дрожжи', 'производящие', 'различных', 'ароматических', 'соединений', 'удешевило', 'парфюмерное', 'производство', 'bio', 'engineering', 'становится', 'новой', 'промышленной', 'революцией', 'xxi', 'века', 'считает', 'профессор', 'mit', 'микроорганизмы', 'выращенные', 'литровых', 'биореакторах', 'синтезируют', 'материалы', 'идентичные', 'шёлку', 'кожи', 'участия', 'животных', 'году', 'стартап', 'natureloop', 'представил', 'biodegradable', 'пластик', 'основе', 'целлюлозы', 'который', 'разлагается', 'почве', 'дней', 'искусственные', 'бактерии', 'способны', 'превращать', 'отходы', 'агропромышленности', 'биотопливо', 'эффективностью', 'согласно', 'прогнозам', 'году', 'химической', 'промышленности', 'перейдет', 'bio', 'based', 'производство', 'этические', 'вопросы', 'регулирования', 'новой', 'отрасли', 'обсуждаются', 'рамках', 'международного', 'соглашения', 'biological', 'code', 'act', 'bca']\n",
            "\n",
            "Удаленные стоп-слова:\n",
            "['с', 'в', 'с', 'до', 'что', 'на', 'ли', 'из', 'в', 'и', 'без', 'в', 'на', 'в', 'за', 'в', 'с', 'до', 'к', 'до', 'на', 'в']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 5. Лемматизация и стемминг**"
      ],
      "metadata": {
        "id": "sbV8P56O3-ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примените к токенам алгоритмы лемматизации и стемминга. Выведите 2 списка - 1. Лемматизированные токены 2. Стемматизированные токены"
      ],
      "metadata": {
        "id": "OZ1A8VhK4HTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "new_tokens = ' '.join(filtered_tokens)\n",
        "doc = nlp_ru(new_tokens)\n",
        "lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "words = re.findall(r'\\b\\w+\\b', new_tokens)\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "print(f'Лемматизированные токены:\\n{lemmas}')\n",
        "print(f'\\nСтемматизированнные токены:\\n{stems}')"
      ],
      "metadata": {
        "id": "OrCmA4tt4WH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f15991b5-a019-48a5-e332-3143a0d017e0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные токены:\n",
            "['синтетический', 'биология', 'междисциплинарный', 'область', 'объединять', 'инженерный', 'подход', 'генетикой', 'год', 'метод', 'crispr', 'cas', 'позволять', 'редактировать', 'геном', 'точность', 'нуклеотид', 'компания', 'ginkgo', 'bioworks', 'создать', 'дрожжи', 'производить', 'различный', 'ароматический', 'соединение', 'удешевить', 'парфюмерный', 'производство', 'bio', 'engineering', 'становиться', 'новый', 'промышленный', 'революция', 'xxi', 'век', 'считать', 'профессор', 'mit', 'микроорганизм', 'выращенные', 'литровый', 'биореактор', 'синтезировать', 'материал', 'идентичный', 'шёлку', 'кожа', 'участие', 'животное', 'год', 'стартап', 'natureloop', 'представить', 'biodegradable', 'пластик', 'основа', 'целлюлоза', 'который', 'разлагаться', 'почве', 'день', 'искусственный', 'бактерия', 'способный', 'превращать', 'отход', 'агропромышленность', 'биотопливо', 'эффективность', 'согласно', 'прогноз', 'год', 'химический', 'промышленность', 'перейти', 'bio', 'based', 'производство', 'этический', 'вопрос', 'регулирование', 'новый', 'отрасль', 'обсуждаться', 'рамка', 'международный', 'соглашение', 'biological', 'code', 'act', 'bca']\n",
            "\n",
            "Стемматизированнные токены:\n",
            "['синтетическ', 'биолог', 'междисциплинарн', 'област', 'объединя', 'инженерн', 'подход', 'генетик', 'год', 'метод', 'crispr', 'cas', 'позволя', 'редактирова', 'ген', 'точност', 'нуклеотид', 'компан', 'ginkgo', 'bioworks', 'созда', 'дрожж', 'производя', 'различн', 'ароматическ', 'соединен', 'удешев', 'парфюмерн', 'производств', 'bio', 'engineering', 'станов', 'нов', 'промышлен', 'революц', 'xxi', 'век', 'счита', 'профессор', 'mit', 'микроорганизм', 'выращен', 'литров', 'биореактор', 'синтезир', 'материал', 'идентичн', 'шелк', 'кож', 'участ', 'животн', 'год', 'стартап', 'natureloop', 'представ', 'biodegradable', 'пластик', 'основ', 'целлюлоз', 'котор', 'разлага', 'почв', 'дне', 'искусствен', 'бактер', 'способн', 'превраща', 'отход', 'агропромышлен', 'биотоплив', 'эффективн', 'согласн', 'прогноз', 'год', 'химическ', 'промышлен', 'перейдет', 'bio', 'based', 'производств', 'этическ', 'вопрос', 'регулирован', 'нов', 'отрасл', 'обсужда', 'рамк', 'международн', 'соглашен', 'biological', 'code', 'act', 'bca']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 6. Напишите функцию для препроцессинга текста**"
      ],
      "metadata": {
        "id": "R_R2xPrh4bW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объедините все шаги в одну функцию. Выведите результат с лемматизированным списком"
      ],
      "metadata": {
        "id": "drJR_Rff4j_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "def preprocessing(text):\n",
        "    # Нормализация текста.\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Токенизация\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "\n",
        "    # Лемматизация\n",
        "    new_tokens = ' '.join(filtered_tokens)\n",
        "    doc = nlp_ru(new_tokens)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    print(f'Лемматизированный список токенов:\\n{lemmas}')\n",
        "\n",
        "preprocessing(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUE3jJp940iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47d63fa7-875f-40e8-b6f0-a715ad016fac"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный список токенов:\n",
            "['синтетический', 'биология', 'междисциплинарный', 'область', 'объединять', 'инженерный', 'подход', 'генетикой', 'год', 'метод', 'crispr', 'cas', 'позволять', 'редактировать', 'геном', 'точность', 'нуклеотид', 'компания', 'ginkgo', 'bioworks', 'создать', 'дрожжи', 'производить', 'различный', 'ароматический', 'соединение', 'удешевить', 'парфюмерный', 'производство', 'bio', 'engineering', 'становиться', 'новый', 'промышленный', 'революция', 'xxi', 'век', 'считать', 'профессор', 'mit', 'микроорганизм', 'выращенные', 'литровый', 'биореактор', 'синтезировать', 'материал', 'идентичный', 'шёлку', 'кожа', 'участие', 'животное', 'год', 'стартап', 'natureloop', 'представить', 'biodegradable', 'пластик', 'основа', 'целлюлоза', 'который', 'разлагаться', 'почве', 'день', 'искусственный', 'бактерия', 'способный', 'превращать', 'отход', 'агропромышленность', 'биотопливо', 'эффективность', 'согласно', 'прогноз', 'год', 'химический', 'промышленность', 'перейти', 'bio', 'based', 'производство', 'этический', 'вопрос', 'регулирование', 'новый', 'отрасль', 'обсуждаться', 'рамка', 'международный', 'соглашение', 'biological', 'code', 'act', 'bca']\n"
          ]
        }
      ]
    }
  ]
}