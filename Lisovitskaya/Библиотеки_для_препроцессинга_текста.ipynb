{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgpxdmVlmRLW"
      },
      "source": [
        "В этой практической тетрадке мы изучим различные методы нормализации текста – стемматизацию и лемматизацию – и сравним, как работают основные библиотеки обработки естественного языка для русского и английского языков."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XAy7q_PxmWFd"
      },
      "source": [
        "Мы рассмотрим:\n",
        "\n",
        "* Разницу между стемматизацией и лемматизацией\n",
        "* Сравнение работы библиотек NLTK, spaCy, PyMorphy2 и Natasha\n",
        "* Создание комплексных пайплайнов препроцессинга для разных языков\n",
        "* Практические упражнения для закрепления материала"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true,
        "id": "XpmGCPeXmcbl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "zsh:1: command not found: pip\n",
            "zsh:1: command not found: python\n",
            "zsh:1: command not found: python\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'nltk'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      3\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpython -m spacy download en_core_web_sm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      4\u001b[39m get_ipython().system(\u001b[33m'\u001b[39m\u001b[33mpython -m spacy download ru_core_news_sm\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\n\u001b[32m      7\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mpunkt\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      8\u001b[39m nltk.download(\u001b[33m'\u001b[39m\u001b[33mstopwords\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'nltk'"
          ]
        }
      ],
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install nltk spacy pymorphy2 natasha\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import spacy\n",
        "import pymorphy2\n",
        "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORdenrTamgWN"
      },
      "source": [
        "**1. Сравнение стемматизации и лемматизации**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qqJqaLM6mprF"
      },
      "outputs": [],
      "source": [
        "# Тестовые тексты\n",
        "russian_text = \"\"\"\n",
        "В России проживают народы с разными традициями и верованиями.\n",
        "Русские люди всегда с интересом относились к культуре соседних народов.\n",
        "Петр Первый ввел многие европейские обычаи.\n",
        "Екатерина Великая переписывалась с французскими просветителями.\n",
        "Современные россияне любят путешествовать по разным странам мира.\n",
        "\"\"\"\n",
        "\n",
        "english_text = \"\"\"\n",
        "The United States is home to people with diverse traditions and beliefs.\n",
        "Americans have always been interested in the cultures of neighboring countries.\n",
        "George Washington established many governmental customs.\n",
        "Thomas Jefferson corresponded with French intellectuals.\n",
        "Modern Americans enjoy traveling to different countries around the world.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x07ZPxQImzj9"
      },
      "source": [
        "1.1 Стемматизация с помощью NLTK. Стемматизация - это процесс нахождения основы слова путем отбрасывания аффиксов (окончаний, суффиксов). Стемматизаторы используют набор правил без словарей.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D9_rxdQAm46d"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Создаем стемматизаторы для английского и русского языков\n",
        "english_stemmer = SnowballStemmer(\"english\")\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "# Функция для токенизации и стемматизации\n",
        "def stem_text(text, stemmer):\n",
        "    # Приводим к нижнему регистру и токенизируем\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    # Применяем стемматизацию\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "# Стемматизация русского текста\n",
        "russian_stemmed = stem_text(russian_text, russian_stemmer)\n",
        "print(\"Стемматизированные русские слова:\")\n",
        "for i, word in enumerate(russian_stemmed[:15]):  # Выводим первые 15 слов\n",
        "    print(f\"{i+1}. {word}\")\n",
        "\n",
        "# Стемматизация английского текста\n",
        "english_stemmed = stem_text(english_text, english_stemmer)\n",
        "print(\"\\nСтемматизированные английские слова:\")\n",
        "for i, word in enumerate(english_stemmed[:15]):  # Выводим первые 15 слов\n",
        "    print(f\"{i+1}. {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5RQGjFJ-nBEk"
      },
      "source": [
        "1.2 Лемматизация с помощью разных библиотек. Лемматизация - это процесс приведения слова к его словарной (нормальной) форме. Лемматизаторы используют словари и учитывают грамматические особенности."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sUhOl_A9nFIE"
      },
      "source": [
        "1.2.1 NLTK (только для английского)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMz6k24ynhut"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cplx5w3znCe1"
      },
      "outputs": [],
      "source": [
        "# Инициализация лемматизатора\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Функция для конвертации POS-тегов NLTK в формат WordNet\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # По умолчанию существительное\n",
        "\n",
        "# Функция для лемматизации с учетом части речи\n",
        "def lemmatize_with_pos(text):\n",
        "    # Токенизация и определение части речи\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Лемматизация с учетом части речи\n",
        "    lemmas = []\n",
        "    for word, tag in tagged:\n",
        "        if word.isalpha():  # Исключаем числа и знаки препинания\n",
        "            wordnet_pos = get_wordnet_pos(tag)\n",
        "            lemmas.append(lemmatizer.lemmatize(word, wordnet_pos))\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация английского текста\n",
        "english_lemmas_nltk = lemmatize_with_pos(english_text)\n",
        "print(\"Лемматизированные английские слова (NLTK):\")\n",
        "for i, word in enumerate(english_lemmas_nltk[:15]):\n",
        "    print(f\"{i+1}. {word}\")\n",
        "\n",
        "# Примечание: NLTK не имеет встроенного лемматизатора для русского языка\n",
        "print(\"\\nNLTK не поддерживает лемматизацию русского языка напрямую.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EC1ibf9HoP0E"
      },
      "source": [
        "1.2.2 spaCy (для обоих языков)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NEAcyouloRVU"
      },
      "outputs": [],
      "source": [
        "# Загрузка моделей spaCy\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Функция для лемматизации с помощью spaCy\n",
        "def lemmatize_with_spacy(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация английского текста\n",
        "english_lemmas_spacy = lemmatize_with_spacy(english_text, nlp_en)\n",
        "print(\"Лемматизированные английские слова (spaCy):\")\n",
        "for i, word in enumerate(english_lemmas_spacy[:15]):\n",
        "    print(f\"{i+1}. {word}\")\n",
        "\n",
        "# Лемматизация русского текста\n",
        "russian_lemmas_spacy = lemmatize_with_spacy(russian_text, nlp_ru)\n",
        "print(\"\\nЛемматизированные русские слова (spaCy):\")\n",
        "for i, word in enumerate(russian_lemmas_spacy[:15]):\n",
        "    print(f\"{i+1}. {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n-0rEI35pz4j"
      },
      "source": [
        "1.2.3 PyMorphy2 (для русского языка)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JskPV0C3p0UT"
      },
      "outputs": [],
      "source": [
        "# Инициализация анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для лемматизации с помощью PyMorphy2\n",
        "def lemmatize_with_pymorphy(text):\n",
        "    # Токенизация\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    # Лемматизация\n",
        "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация русского текста\n",
        "russian_lemmas_pymorphy = lemmatize_with_pymorphy(russian_text)\n",
        "print(\"Лемматизированные русские слова (PyMorphy2):\")\n",
        "for i, word in enumerate(russian_lemmas_pymorphy[:15]):\n",
        "    print(f\"{i+1}. {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAdupAM1qYi7"
      },
      "source": [
        "1.2.4 Natasha (для русского языка)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFPv5xZFqaAz"
      },
      "outputs": [],
      "source": [
        "# Инициализация компонентов Natasha\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "\n",
        "# Функция для лемматизации с помощью Natasha\n",
        "def lemmatize_with_natasha(text):\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "\n",
        "    lemmas = []\n",
        "    for token in doc.tokens:\n",
        "        if token.text.isalpha():  # Исключаем числа и знаки препинания\n",
        "            token.lemmatize(morph_vocab)\n",
        "            lemmas.append(token.lemma)\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация русского текста\n",
        "russian_lemmas_natasha = lemmatize_with_natasha(russian_text)\n",
        "print(\"Лемматизированные русские слова (Natasha):\")\n",
        "for i, word in enumerate(russian_lemmas_natasha[:15]):\n",
        "    print(f\"{i+1}. {word}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LjqZiOKqi4r"
      },
      "source": [
        "1.3 Сравнительная таблица результатов"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9vTvNd9qjbT"
      },
      "source": [
        "Для наглядности сравним результаты обработки одних и тех же слов разными методами."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MBJxuEUlqlkr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Выбираем несколько интересных слов из русского текста для сравнения\n",
        "russian_words = [\"России\", \"проживают\", \"народы\", \"традициями\", \"верованиями\",\n",
        "                \"русские\", \"людей\", \"относились\", \"культуре\", \"Петр\", \"ввел\", \"обычаи\"]\n",
        "\n",
        "# Создаем DataFrame для сравнения\n",
        "comparison_ru = []\n",
        "for word in russian_words:\n",
        "    stem = russian_stemmer.stem(word.lower())\n",
        "    lemma_spacy = None\n",
        "\n",
        "    # Находим лемму в spaCy\n",
        "    doc = nlp_ru(word)\n",
        "    for token in doc:\n",
        "        lemma_spacy = token.lemma_\n",
        "\n",
        "    lemma_pymorphy = morph.parse(word.lower())[0].normal_form\n",
        "\n",
        "    # Находим лемму в Natasha\n",
        "    doc = Doc(word)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "        lemma_natasha = token.lemma\n",
        "\n",
        "    comparison_ru.append({\n",
        "        \"Слово\": word,\n",
        "        \"Стем (NLTK)\": stem,\n",
        "        \"Лемма (spaCy)\": lemma_spacy,\n",
        "        \"Лемма (PyMorphy2)\": lemma_pymorphy,\n",
        "        \"Лемма (Natasha)\": lemma_natasha\n",
        "    })\n",
        "\n",
        "# Создаем таблицу для русских слов\n",
        "comparison_ru_df = pd.DataFrame(comparison_ru)\n",
        "print(\"Сравнение методов для русского языка:\")\n",
        "comparison_ru_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dHPkDhnlqyAz"
      },
      "outputs": [],
      "source": [
        "# Выбираем несколько интересных слов из английского текста для сравнения\n",
        "english_words = [\"States\", \"people\", \"diverse\", \"traditions\", \"beliefs\",\n",
        "                \"Americans\", \"interested\", \"cultures\", \"neighboring\", \"established\", \"corresponded\", \"traveling\"]\n",
        "\n",
        "# Создаем DataFrame для сравнения\n",
        "comparison_en = []\n",
        "for word in english_words:\n",
        "    stem = english_stemmer.stem(word.lower())\n",
        "\n",
        "    # Находим лемму в NLTK\n",
        "    pos = pos_tag([word.lower()])[0][1]\n",
        "    wordnet_pos = get_wordnet_pos(pos)\n",
        "    lemma_nltk = lemmatizer.lemmatize(word.lower(), wordnet_pos)\n",
        "\n",
        "    # Находим лемму в spaCy\n",
        "    doc = nlp_en(word)\n",
        "    for token in doc:\n",
        "        lemma_spacy = token.lemma_\n",
        "\n",
        "    comparison_en.append({\n",
        "        \"Слово\": word,\n",
        "        \"Стем (NLTK)\": stem,\n",
        "        \"Лемма (NLTK)\": lemma_nltk,\n",
        "        \"Лемма (spaCy)\": lemma_spacy\n",
        "    })\n",
        "\n",
        "# Создаем таблицу для английских слов\n",
        "comparison_en_df = pd.DataFrame(comparison_en)\n",
        "print(\"Сравнение методов для английского языка:\")\n",
        "comparison_en_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6ti34MVq4xS"
      },
      "source": [
        "**2. Комплексные пайплайны препроцессинга текста**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jqmpnF2Tq7jy"
      },
      "source": [
        "2.1 Пайплайн для русского языка с PyMorphy2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EiCoa-CmrFa7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pymorphy2\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uOLOXZbLq-Xz"
      },
      "outputs": [],
      "source": [
        "# Загрузка стоп-слов\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "\n",
        "# Инициализация анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess_russian_text(text):\n",
        "    \"\"\"\n",
        "    Полный пайплайн предобработки русского текста с использованием PyMorphy2\n",
        "    \"\"\"\n",
        "    # Шаг 1: Приведение текста к нижнему регистру\n",
        "    text = text.lower()\n",
        "    print(f\"1. Приведение к нижнему регистру: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 2: Замена 'ё' на 'е'\n",
        "    text = text.replace('ё', 'е')\n",
        "    print(f\"2. Замена 'ё' на 'е': {text[:50]}...\")\n",
        "\n",
        "    # Шаг 3: Удаление цифр, знаков препинания и лишних пробелов\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Удаление знаков препинания\n",
        "    text = re.sub(r'\\d+', ' ', text)      # Удаление цифр\n",
        "    text = re.sub(r'\\s+', ' ', text)      # Удаление лишних пробелов\n",
        "    text = text.strip()                    # Удаление пробелов в начале и конце\n",
        "    print(f\"3. Очистка текста: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 4: Токенизация\n",
        "    tokens = text.split()\n",
        "    print(f\"4. Токенизация: получено {len(tokens)} токенов\")\n",
        "\n",
        "    # Шаг 5: Удаление стоп-слов\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "    print(f\"5. Удаление стоп-слов: осталось {len(filtered_tokens)} токенов\")\n",
        "\n",
        "    # Шаг 6: Лемматизация с PyMorphy2\n",
        "    lemmas = []\n",
        "    for token in filtered_tokens:\n",
        "        parsed = morph.parse(token)[0]\n",
        "        lemmas.append(parsed.normal_form)\n",
        "\n",
        "    print(f\"6. Лемматизация: получено {len(lemmas)} лемм\")\n",
        "\n",
        "    # Шаг 7: Удаление слишком коротких слов\n",
        "    final_lemmas = [lemma for lemma in lemmas if len(lemma) > 2]\n",
        "    print(f\"7. Фильтрация коротких слов: итоговое количество {len(final_lemmas)} лемм\")\n",
        "\n",
        "    return final_lemmas\n",
        "\n",
        "# Применение пайплайна к русскому тексту\n",
        "print(\"Комплексный пайплайн для русского текста:\")\n",
        "preprocessed_russian = preprocess_russian_text(russian_text)\n",
        "print(\"\\nРезультат:\")\n",
        "print(preprocessed_russian)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiC4PNPlrQAy"
      },
      "source": [
        "2.2 Пайплайн для английского языка с spaCy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "avOTgnkUrRBj"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7cbkaplkrVCq"
      },
      "outputs": [],
      "source": [
        "# Загрузка стоп-слов и модели spaCy\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_english_text(text):\n",
        "    \"\"\"\n",
        "    Полный пайплайн предобработки английского текста с использованием spaCy\n",
        "    \"\"\"\n",
        "    # Шаг 1: Приведение текста к нижнему регистру\n",
        "    text = text.lower()\n",
        "    print(f\"1. Приведение к нижнему регистру: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 2: Расширение сокращений (контракций)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    print(f\"2. Расширение контракций: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 3: Обработка текста с помощью spaCy\n",
        "    doc = nlp_en(text)\n",
        "    print(f\"3. Обработка с spaCy: получено {len(doc)} токенов\")\n",
        "\n",
        "    # Шаг 4: Фильтрация и лемматизация\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "        # Проверяем, что токен не является стоп-словом, пунктуацией или числом\n",
        "        if (not token.is_stop and\n",
        "            not token.is_punct and\n",
        "            not token.is_digit and\n",
        "            token.is_alpha):  # Только буквенные токены\n",
        "\n",
        "            # Получаем лемму и проверяем, что она не является специальным токеном spaCy\n",
        "            lemma = token.lemma_\n",
        "            if lemma != '-PRON-':  # spaCy иногда использует -PRON- для местоимений\n",
        "                lemmas.append(lemma)\n",
        "\n",
        "    print(f\"4. Фильтрация и лемматизация: получено {len(lemmas)} лемм\")\n",
        "\n",
        "    # Шаг 5: Удаление слишком коротких слов\n",
        "    final_lemmas = [lemma for lemma in lemmas if len(lemma) > 2]\n",
        "    print(f\"5. Фильтрация коротких слов: итоговое количество {len(final_lemmas)} лемм\")\n",
        "\n",
        "    return final_lemmas\n",
        "\n",
        "# Применение пайплайна к английскому тексту\n",
        "print(\"Комплексный пайплайн для английского текста:\")\n",
        "preprocessed_english = preprocess_english_text(english_text)\n",
        "print(\"\\nРезультат:\")\n",
        "print(preprocessed_english)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EViy6ISyraCK"
      },
      "source": [
        "2.3 Пайплайн для смешанного текста (русский + английский)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIbjAyRlCFmY"
      },
      "source": [
        "Блок 1: Импорт необходимых библиотек и инициализация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS3MzLG0BNSz"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import pymorphy2\n",
        "\n",
        "# Загрузим необходимые ресурсы\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Инициализация анализаторов\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "english_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Списки стоп-слов\n",
        "russian_stop_words = set(stopwords.words('russian'))\n",
        "english_stop_words = set(stopwords.words('english'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "M2gIZ8yTCr9-"
      },
      "outputs": [],
      "source": [
        "# Исходный текст\n",
        "mixed_text = \"\"\"\n",
        "<p>В современном мире digital-технологии стали неотъемлемой частью жизни.</p>\n",
        "<strong>Information technologies</strong> изменили способы коммуникации между людьми в 2023 году.\n",
        "CEO крупных IT-компаний (более 500 сотрудников) регулярно проводят online-конференции.\n",
        "<a href=\"https://example.com\">Developers</a> из разных стран сотрудничают в open-source проектах.\n",
        "Искусственный интеллект и <em>machine learning</em> находят применение в разных сферах.\n",
        "Для связи: contact@tech-info.com или посетите https://tech-conference.org.\n",
        "<div class=\"footer\">\n",
        "  © 2023 AI-News. Телефон: +7 (123) 456-78-90. #искусственныйинтеллект #bigdata\n",
        "</div>\n",
        "Data-scientists и ML-инженеры работают с web-приложениями и AI-моделями.\n",
        "Кибер-безопасность и e-commerce — важные направления IT-индустрии.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWkQF_PhCMk3"
      },
      "source": [
        "Блок 2: Нормализация текста"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfC2GfQ2Br8k"
      },
      "outputs": [],
      "source": [
        "def normalize_mixed_text(text):\n",
        "    \"\"\"\n",
        "    Нормализует смешанный текст, удаляя HTML-теги, URL, email, специальные символы\n",
        "    и выполняя базовые преобразования для стандартизации текста.\n",
        "\n",
        "    Args:\n",
        "        text (str): Исходный смешанный текст с \"шумом\"\n",
        "\n",
        "    Returns:\n",
        "        str: Нормализованный текст\n",
        "    \"\"\"\n",
        "    # Шаг 1: Удаление HTML-тегов\n",
        "    # Находит и удаляет всё между < и >, включая скрипты и стили\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "    # Шаг 2: Удаление URL-адресов\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "    # Шаг 3: Удаление email-адресов\n",
        "    text = re.sub(r'\\S+@\\S+\\.\\S+', ' ', text)\n",
        "\n",
        "    # Шаг 4: Удаление хештегов\n",
        "    text = re.sub(r'#\\S+', ' ', text)\n",
        "\n",
        "    # Шаг 5: Удаление номеров телефонов (различные форматы)\n",
        "    text = re.sub(r'\\+\\d+\\s*[\\(\\)]*\\s*\\d+[\\s\\-\\(\\)]*\\d+[\\s\\-\\(\\)]*\\d+', ' ', text)\n",
        "\n",
        "    # Шаг 6: Удаление символов копирайта, специальных символов\n",
        "    text = re.sub(r'[©®™℠]+', ' ', text)\n",
        "\n",
        "    # Шаг 7: Замена скобок и их содержимого на пробел\n",
        "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
        "\n",
        "    # Шаг 8: Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Шаг 9: Замена буквы \"ё\" на \"е\" для унификации русского текста\n",
        "    text = text.replace('ё', 'е')\n",
        "\n",
        "    # Шаг 10: Обработка пунктуации\n",
        "    # Сохраняем дефисы внутри слов, но удаляем остальную пунктуацию\n",
        "    # Сначала заменим дефисы временным маркером\n",
        "    text = re.sub(r'(\\w)-(\\w)', r'\\1HYPHEN\\2', text)\n",
        "\n",
        "    # Удаляем пунктуацию кроме временных маркеров дефисов\n",
        "    text = re.sub(r'[^\\w\\sHYPHEN]', ' ', text)\n",
        "\n",
        "    # Возвращаем дефисы\n",
        "    text = re.sub(r'HYPHEN', '-', text)\n",
        "\n",
        "    # Шаг 11: Удаление цифр, но сохранение слов с цифрами (например, ML-инженеры5)\n",
        "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
        "\n",
        "    # Шаг 12: Удаление избыточных пробелов (в том числе в начале и конце строк)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Тест функции\n",
        "normalized_text = normalize_mixed_text(mixed_text)\n",
        "print(\"Нормализованный текст:\")\n",
        "print(normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMtLsyCMDZ9G"
      },
      "source": [
        "Блок 3: Определение алфавита символа и токенизация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0fKwh812DcQO"
      },
      "outputs": [],
      "source": [
        "def is_cyrillic(char):\n",
        "    \"\"\"Проверяет, является ли символ кириллическим.\"\"\"\n",
        "    return 'а' <= char <= 'я' or char == 'ё'\n",
        "\n",
        "def is_latin(char):\n",
        "    \"\"\"Проверяет, является ли символ латинским.\"\"\"\n",
        "    return 'a' <= char <= 'z'\n",
        "\n",
        "def classify_token(token):\n",
        "    \"\"\"Определяет язык токена: 'russian', 'english' или 'mixed'.\"\"\"\n",
        "    if not token:\n",
        "        return None\n",
        "\n",
        "    # Удаляем все не-буквенные символы для определения языка\n",
        "    letters = [c for c in token.lower() if c.isalpha()]\n",
        "    if not letters:\n",
        "        return None\n",
        "\n",
        "    cyrillic_count = sum(1 for c in letters if is_cyrillic(c))\n",
        "    latin_count = sum(1 for c in letters if is_latin(c))\n",
        "\n",
        "    if cyrillic_count > 0 and latin_count == 0:\n",
        "        return 'russian'\n",
        "    elif latin_count > 0 and cyrillic_count == 0:\n",
        "        return 'english'\n",
        "    elif cyrillic_count > 0 and latin_count > 0:\n",
        "        return 'mixed'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def split_hyphenated_words(token):\n",
        "    \"\"\"Разбивает слова с дефисом, если они состоят из смешанных символов.\"\"\"\n",
        "    if '-' not in token:\n",
        "        return [token]\n",
        "\n",
        "    # Проверяем, есть ли смешанные символы в токене\n",
        "    if classify_token(token) != 'mixed':\n",
        "        return [token]\n",
        "\n",
        "    # Разбиваем по дефису\n",
        "    parts = token.split('-')\n",
        "    result = []\n",
        "\n",
        "    for part in parts:\n",
        "        if part:  # Игнорируем пустые части\n",
        "            result.append(part)\n",
        "\n",
        "    return result\n",
        "\n",
        "def tokenize_and_classify(text):\n",
        "    \"\"\"Токенизирует текст и классифицирует токены по языку.\"\"\"\n",
        "    # Сначала разделим текст на слова, сохраняя дефисы внутри слов\n",
        "    raw_tokens = re.findall(r'\\b[\\w\\-]+\\b', text)\n",
        "\n",
        "    russian_tokens = []\n",
        "    english_tokens = []\n",
        "\n",
        "    for token in raw_tokens:\n",
        "        # Проверяем, нужно ли разделить слово с дефисом\n",
        "        split_tokens = split_hyphenated_words(token)\n",
        "\n",
        "        for t in split_tokens:\n",
        "            lang = classify_token(t)\n",
        "            if lang == 'russian':\n",
        "                russian_tokens.append(t)\n",
        "            elif lang == 'english':\n",
        "                english_tokens.append(t)\n",
        "\n",
        "    return russian_tokens, english_tokens\n",
        "\n",
        "# Применяем токенизацию и классификацию\n",
        "russian_tokens, english_tokens = tokenize_and_classify(normalized_text)\n",
        "\n",
        "print(\"\\nРусские токены:\")\n",
        "print(russian_tokens)\n",
        "print(\"\\nАнглийские токены:\")\n",
        "print(english_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GaZMugJFDtHQ"
      },
      "source": [
        "Блок 4: Обработка токенов по языковым правилам"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Fon7yaBDtuL"
      },
      "outputs": [],
      "source": [
        "def process_russian_tokens(tokens):\n",
        "    \"\"\"Обрабатывает русские токены по пайплайну для русского языка.\"\"\"\n",
        "    # Фильтрация стоп-слов\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stop_words]\n",
        "\n",
        "    # Лемматизация с помощью pymorphy2\n",
        "    lemmatized_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        # Получаем нормальную форму\n",
        "        lemma = morph.parse(token)[0].normal_form\n",
        "        lemmatized_tokens.append(lemma)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def process_english_tokens(tokens):\n",
        "    \"\"\"Обрабатывает английские токены по пайплайну для английского языка.\"\"\"\n",
        "    # Обработка сокращений и апострофов\n",
        "    expanded_tokens = []\n",
        "    for token in tokens:\n",
        "        # Обработка притяжательных форм и сокращений\n",
        "        token = re.sub(r'\\'s$|\\'$', '', token)  # Удаляем 's и '\n",
        "        expanded_tokens.append(token)\n",
        "\n",
        "    # Фильтрация стоп-слов\n",
        "    filtered_tokens = [token for token in expanded_tokens if token not in english_stop_words]\n",
        "\n",
        "    # Стемматизация\n",
        "    stemmed_tokens = [english_stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Обработка токенов по языковым правилам\n",
        "processed_russian = process_russian_tokens(russian_tokens)\n",
        "processed_english = process_english_tokens(english_tokens)\n",
        "\n",
        "print(\"\\nОбработанные русские токены:\")\n",
        "print(processed_russian)\n",
        "print(\"\\nОбработанные английские токены:\")\n",
        "print(processed_english)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrZFA-rID2XX"
      },
      "source": [
        "Блок 5: Финальная обработка и объединение результатов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ulvjX8hDD4LJ"
      },
      "outputs": [],
      "source": [
        "def final_processing(russian_tokens, english_tokens):\n",
        "    \"\"\"Выполняет постобработку и объединяет результаты.\"\"\"\n",
        "    # Фильтрация коротких слов (менее 3 символов)\n",
        "    filtered_russian = [token for token in russian_tokens if len(token) >= 3]\n",
        "    filtered_english = [token for token in english_tokens if len(token) >= 3]\n",
        "\n",
        "    # Удаление дубликатов\n",
        "    unique_russian = list(dict.fromkeys(filtered_russian))\n",
        "    unique_english = list(dict.fromkeys(filtered_english))\n",
        "\n",
        "    return {\n",
        "        'russian_tokens': unique_russian,\n",
        "        'english_tokens': unique_english,\n",
        "        'stats': {\n",
        "            'russian_count': len(unique_russian),\n",
        "            'english_count': len(unique_english),\n",
        "            'total_unique': len(unique_russian) + len(unique_english)\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Финальная обработка\n",
        "result = final_processing(processed_russian, processed_english)\n",
        "\n",
        "print(\"\\nФинальный результат:\")\n",
        "print(\"Русские слова:\", result['russian_tokens'])\n",
        "print(\"Английские слова:\", result['english_tokens'])\n",
        "print(\"\\nСтатистика:\")\n",
        "print(f\"Количество русских слов: {result['stats']['russian_count']}\")\n",
        "print(f\"Количество английских слов: {result['stats']['english_count']}\")\n",
        "print(f\"Общее количество уникальных слов: {result['stats']['total_unique']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwJWCho_Eebo"
      },
      "source": [
        "**Практические упражнения:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xThTr52IEvhr"
      },
      "source": [
        "Упражнение 1: Базовая стемматизация русского текста с помощью NLTK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b-U9dWlEyUB"
      },
      "source": [
        "Задание:\n",
        "Напишите функцию, которая принимает русский текст, выполняет токенизацию и применяет стемматизацию с использованием SnowballStemmer из библиотеки NLTK. Функция должна возвращать список стемматизированных слов.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Используйте SnowballStemmer(\"russian\") из NLTK\n",
        "* Удалите все знаки препинания перед стемматизацией\n",
        "* Приведите слова к нижнему регистру\n",
        "* Выведите исходные слова и их стеммы в виде таблицы для сравнения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kG7vf-0jE6US"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Collecting spacy\n",
            "  Downloading spacy-3.8.7-cp313-cp313-macosx_11_0_arm64.whl.metadata (27 kB)\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting click (from nltk)\n",
            "  Downloading click-8.3.0-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting joblib (from nltk)\n",
            "  Downloading joblib-1.5.2-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting regex>=2021.8.3 (from nltk)\n",
            "  Downloading regex-2025.10.23-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
            "Collecting tqdm (from nltk)\n",
            "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
            "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
            "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
            "  Downloading spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
            "  Downloading murmurhash-1.0.13-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
            "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
            "  Downloading cymem-2.0.11-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.5 kB)\n",
            "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
            "  Downloading preshed-3.0.10-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.4 kB)\n",
            "Collecting thinc<8.4.0,>=8.3.4 (from spacy)\n",
            "  Downloading thinc-8.3.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (15 kB)\n",
            "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
            "  Downloading wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
            "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
            "  Downloading srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (19 kB)\n",
            "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
            "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
            "  Downloading weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
            "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting numpy>=1.19.0 (from spacy)\n",
            "  Downloading numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl.metadata (62 kB)\n",
            "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
            "  Downloading pydantic-2.12.3-py3-none-any.whl.metadata (87 kB)\n",
            "Collecting jinja2 (from spacy)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting setuptools (from spacy)\n",
            "  Downloading setuptools-80.9.0-py3-none-any.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from spacy) (25.0)\n",
            "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
            "  Downloading langcodes-3.5.0-py3-none-any.whl.metadata (29 kB)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Installing build dependencies ... \u001b[?25ldone\n",
            "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
            "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading language_data-1.3.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting pydantic-core==2.41.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading pydantic_core-2.41.4-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.3 kB)\n",
            "Collecting typing-extensions>=4.14.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting typing-inspection>=0.4.2 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
            "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
            "  Downloading certifi-2025.10.5-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting blis<1.4.0,>=1.3.0 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading blis-1.3.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (7.4 kB)\n",
            "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.4->spacy)\n",
            "  Downloading confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading cloudpathlib-0.23.0-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading smart_open-7.4.1-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
            "  Downloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
            "Collecting sortedcontainers<3.0,>=2.0 (from intervaltree>=3->ipymarkup>=0.8.0->natasha)\n",
            "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl.metadata (10 kB)\n",
            "Collecting marisa-trie>=1.1.0 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
            "  Downloading marisa_trie-1.3.1-cp313-cp313-macosx_11_0_arm64.whl.metadata (10 kB)\n",
            "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
            "  Downloading wrapt-2.0.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (8.8 kB)\n",
            "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
            "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m493.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading spacy-3.8.7-cp313-cp313-macosx_11_0_arm64.whl (5.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.9/5.9 MB\u001b[0m \u001b[31m225.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
            "\u001b[?25hDownloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "Downloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m348.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:04\u001b[0m\n",
            "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
            "Downloading cymem-2.0.11-cp313-cp313-macosx_11_0_arm64.whl (41 kB)\n",
            "Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Downloading langcodes-3.5.0-py3-none-any.whl (182 kB)\n",
            "Downloading murmurhash-1.0.13-cp313-cp313-macosx_11_0_arm64.whl (26 kB)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Downloading numpy-2.3.4-cp313-cp313-macosx_14_0_arm64.whl (5.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m307.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading preshed-3.0.10-cp313-cp313-macosx_11_0_arm64.whl (124 kB)\n",
            "Downloading pydantic-2.12.3-py3-none-any.whl (462 kB)\n",
            "Downloading pydantic_core-2.41.4-cp313-cp313-macosx_11_0_arm64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m617.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m870.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hDownloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading regex-2025.10.23-cp313-cp313-macosx_11_0_arm64.whl (288 kB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
            "Downloading spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
            "Downloading srsly-2.5.1-cp313-cp313-macosx_11_0_arm64.whl (632 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m632.8/632.8 kB\u001b[0m \u001b[31m537.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading thinc-8.3.6-cp313-cp313-macosx_11_0_arm64.whl (832 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m832.7/832.7 kB\u001b[0m \u001b[31m529.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
            "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
            "Downloading click-8.3.0-py3-none-any.whl (107 kB)\n",
            "Downloading wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
            "Downloading weasel-0.4.1-py3-none-any.whl (50 kB)\n",
            "Downloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Downloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "Downloading joblib-1.5.2-py3-none-any.whl (308 kB)\n",
            "Using cached setuptools-80.9.0-py3-none-any.whl (1.2 MB)\n",
            "Downloading annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
            "Downloading blis-1.3.0-cp313-cp313-macosx_11_0_arm64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m407.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2025.10.5-py3-none-any.whl (163 kB)\n",
            "Downloading charset_normalizer-3.4.4-cp313-cp313-macosx_10_13_universal2.whl (208 kB)\n",
            "Downloading cloudpathlib-0.23.0-py3-none-any.whl (62 kB)\n",
            "Downloading confection-0.1.5-py3-none-any.whl (35 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "Downloading language_data-1.3.0-py3-none-any.whl (5.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m700.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hDownloading markupsafe-3.0.3-cp313-cp313-macosx_11_0_arm64.whl (12 kB)\n",
            "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
            "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Downloading smart_open-7.4.1-py3-none-any.whl (63 kB)\n",
            "Downloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
            "Downloading urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
            "Downloading marisa_trie-1.3.1-cp313-cp313-macosx_11_0_arm64.whl (154 kB)\n",
            "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
            "Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
            "Downloading wrapt-2.0.0-cp313-cp313-macosx_11_0_arm64.whl (61 kB)\n",
            "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13783 sha256=63dc9d94d3f91ab79e3ab71c51752204677cb4a6771a2ec5cd49321d15700f0d\n",
            "  Stored in directory: /Users/morkooovka/Library/Caches/pip/wheels/0b/1d/03/175286677fb5a1341cc3e4755bf8ec0ed08f3329afd67446b0\n",
            "  Building wheel for intervaltree (pyproject.toml) ... \u001b[?25ldone\n",
            "\u001b[?25h  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26190 sha256=c478b768c34ed09b5c674b47ce659ec52db104bf747ad3715655a5a40beb296c\n",
            "  Stored in directory: /Users/morkooovka/Library/Caches/pip/wheels/a7/d2/99/50f53015b573c9b65ff643d7f213fc7784dad87976e79cf02c\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: sortedcontainers, razdel, pymorphy2-dicts-ru, docopt, dawg-python, cymem, wrapt, wasabi, urllib3, typing-extensions, tqdm, spacy-loggers, spacy-legacy, shellingham, setuptools, regex, pymorphy2, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, joblib, intervaltree, idna, cloudpathlib, click, charset_normalizer, certifi, catalogue, annotated-types, yargy, typing-inspection, srsly, smart-open, requests, pydantic-core, preshed, nltk, navec, markdown-it-py, language-data, jinja2, ipymarkup, blis, slovnet, rich, pydantic, langcodes, typer, natasha, confection, weasel, thinc, spacy\n",
            "Successfully installed MarkupSafe-3.0.3 annotated-types-0.7.0 blis-1.3.0 catalogue-2.0.10 certifi-2025.10.5 charset_normalizer-3.4.4 click-8.3.0 cloudpathlib-0.23.0 confection-0.1.5 cymem-2.0.11 dawg-python-0.7.2 docopt-0.6.2 idna-3.11 intervaltree-3.1.0 ipymarkup-0.9.0 jinja2-3.1.6 joblib-1.5.2 langcodes-3.5.0 language-data-1.3.0 marisa-trie-1.3.1 markdown-it-py-4.0.0 mdurl-0.1.2 murmurhash-1.0.13 natasha-1.6.0 navec-0.10.0 nltk-3.9.2 numpy-2.3.4 preshed-3.0.10 pydantic-2.12.3 pydantic-core-2.41.4 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 regex-2025.10.23 requests-2.32.5 rich-14.2.0 setuptools-80.9.0 shellingham-1.5.4 slovnet-0.6.0 smart-open-7.4.1 sortedcontainers-2.4.0 spacy-3.8.7 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.5.1 thinc-8.3.6 tqdm-4.67.1 typer-0.20.0 typing-extensions-4.15.0 typing-inspection-0.4.2 urllib3-2.5.0 wasabi-1.1.3 weasel-0.4.1 wrapt-2.0.0 yargy-0.16.0\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
          ]
        }
      ],
      "source": [
        "#ваш код\n",
        "%pip install nltk spacy pymorphy2 natasha\n",
        "%python -m spacy download en_core_web_sm\n",
        "python -m spacy download ru_core_news_sm\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "def stem_russian_text(text):\n",
        "    \"\"\"\n",
        "    Функция для стемматизации русского текста с использованием SnowballStemmer из NLTK\n",
        "    \n",
        "    Args:\n",
        "        text (str): Исходный русский текст\n",
        "        \n",
        "    Returns:\n",
        "        list: Список стемматизированных слов\n",
        "    \"\"\"\n",
        "    russian_stemmer = SnowballStemmer(\"russian\")\n",
        "    \n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    \n",
        "    stemmed_words = [russian_stemmer.stem(word) for word in words]\n",
        "    \n",
        "    print(\"Сравнение исходных слов и их стемм:\")\n",
        "    print(\"-\" * 40)\n",
        "    print(f\"{'Исходное слово':<20} {'Стемма':<20}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    for i, (original, stemmed) in enumerate(zip(words, stemmed_words)):\n",
        "        print(f\"{original:<20} {stemmed:<20}\")\n",
        "    \n",
        "    return stemmed_words\n",
        "\n",
        "russian_text = \"\"\"\n",
        "Программирование - это искусство создания программ для компьютеров.\n",
        "Программисты разрабатывают различные приложения, которые используются миллионами людей.\n",
        "Хорошие разработчики всегда учатся новому и совершенствуют свои навыки программирования.\n",
        "\"\"\"\n",
        "result = stem_russian_text(russian_text)\n",
        "print(f\"\\nСтемматизированные слова: {result}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHBKG099FGa5"
      },
      "source": [
        "Упражнение 2: Сравнение стемматизации и лемматизации английского текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bRCilNwJFJKu"
      },
      "source": [
        "Задание: Реализуйте две функции, которые обрабатывают один и тот же английский текст с использованием PorterStemmer и WordNetLemmatizer из NLTK соответственно. Сравните результаты и определите, какой метод лучше сохраняет семантику слов.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Для стемматизации используйте PorterStemmer из NLTK\n",
        "* Для лемматизации используйте WordNetLemmatizer с определением части речи\n",
        "* Создайте таблицу сравнения из трех колонок: исходное слово, стемма, лемма\n",
        "* Выделите случаи, где результаты стемматизации и лемматизации существенно различаются"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuAXLHprFaqW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.2)\n",
            "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.8.7)\n",
            "Requirement already satisfied: pymorphy2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.1)\n",
            "Requirement already satisfied: natasha in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.6.0)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2025.10.23)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.3.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: slovnet>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.6.0)\n",
            "Requirement already satisfied: yargy>=0.16.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.16.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk spacy pymorphy2 natasha\n",
        "%python -m spacy download en_core_web_sm\n",
        "%python -m spacy download ru_core_news_sm\n",
        "\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "import pandas as pd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    \"\"\"Преобразование POS тегов из nltk.pos_tag в формат wordnet\"\"\"\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "def compare_stemming_lemmatization(text):\n",
        "    \"\"\"Сравнение стемматизации и лемматизации английского текста\"\"\"\n",
        "\n",
        "    words = word_tokenize(text.lower())\n",
        "    pos_tags = pos_tag(words)\n",
        "\n",
        "    stemmer = PorterStemmer()\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    comparison_data = []\n",
        "    different_results = []\n",
        "    \n",
        "    for word, pos in pos_tags:\n",
        "        if not word.isalpha():\n",
        "            continue\n",
        "            \n",
        "        stem = stemmer.stem(word)\n",
        "        lemma = lemmatizer.lemmatize(word, get_wordnet_pos(pos))\n",
        "        \n",
        "        comparison_data.append({\n",
        "            'Исходное слово': word,\n",
        "            'Стемма (Porter)': stem,\n",
        "            'Лемма (WordNet)': lemma\n",
        "        })\n",
        "\n",
        "        if stem != lemma and abs(len(stem) - len(lemma)) > 1:\n",
        "            different_results.append({\n",
        "                'word': word,\n",
        "                'stem': stem,\n",
        "                'lemma': lemma,\n",
        "                'pos': pos\n",
        "            })\n",
        "    \n",
        "\n",
        "    df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    print(\"Сравнение стемматизации и лемматизации:\")\n",
        "    print(\"=\" * 60)\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    print(\"\\n\\nСлучаи с существенными различиями:\")\n",
        "    print(\"=\" * 50)\n",
        "    for diff in different_results:\n",
        "        print(f\"Слово: '{diff['word']}' (POS: {diff['pos']})\")\n",
        "        print(f\"  Стемма: '{diff['stem']}'\")\n",
        "        print(f\"  Лемма:  '{diff['lemma']}'\")\n",
        "        print(\"-\" * 30)\n",
        "    \n",
        "    return df, different_results\n",
        "\n",
        "\n",
        "english_text = \"\"\"\n",
        "Natural language processing helps computers understand and generate human language.\n",
        "Researchers are developing better models that can recognize patterns in texts.\n",
        "The running applications are processing huge amounts of data daily.\n",
        "Many universities are teaching students how to build better language models.\n",
        "Better algorithms make systems work more efficiently and effectively.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "df, differences = compare_stemming_lemmatization(english_text)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JS5vnJXZFhPl"
      },
      "source": [
        "Упражнение 3: Лемматизация русского текста с использованием pymorphy2 и определение частей речи"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW0VZZMaFiBQ"
      },
      "source": [
        "Задание: Напишите функцию, которая выполняет морфологический анализ русского текста с помощью библиотеки pymorphy2. Функция должна возвращать для каждого слова его лемму (нормальную форму) и часть речи.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Используйте pymorphy2.MorphAnalyzer()\n",
        "* Обработайте токенизированный текст, игнорируя пунктуацию и числа\n",
        "* Для каждого слова определите нормальную форму и часть речи\n",
        "* Посчитайте частоту встречаемости каждой части речи в тексте\n",
        "* Найдите слова, имеющие омонимичные разборы, и предложите способ разрешения неоднозначности"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e9TfNGJrFtDl"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.2)\n",
            "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.8.7)\n",
            "Requirement already satisfied: pymorphy2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.1)\n",
            "Requirement already satisfied: natasha in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.6.0)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2025.10.23)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.3.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: slovnet>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.6.0)\n",
            "Requirement already satisfied: yargy>=0.16.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.16.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk spacy pymorphy2 natasha\n",
        "%python -m spacy download en_core_web_sm\n",
        "%python -m spacy download ru_core_news_sm\n",
        "import pymorphy2\n",
        "import re\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "\n",
        "def analyze_russian_text(text):\n",
        "    \"\"\"\n",
        "    Морфологический анализ русского текста с помощью pymorphy2\n",
        "    \"\"\"\n",
        "\n",
        "    morph = pymorphy2.MorphAnalyzer()\n",
        "    \n",
        "\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    \n",
        "    analysis_results = []\n",
        "    pos_counter = Counter()\n",
        "    ambiguous_words = []\n",
        "    \n",
        "    for word in words:\n",
        "\n",
        "        parses = morph.parse(word)\n",
        "        \n",
        "\n",
        "        best_parse = parses[0]\n",
        "        \n",
        "\n",
        "        lemma = best_parse.normal_form\n",
        "        pos = best_parse.tag.POS if best_parse.tag.POS else 'UNKN'\n",
        "        \n",
        "\n",
        "        analysis_results.append({\n",
        "            'Слово': word,\n",
        "            'Лемма': lemma,\n",
        "            'Часть речи': pos\n",
        "        })\n",
        "\n",
        "        pos_counter[pos] += 1\n",
        "        \n",
        "        if len(parses) > 1:\n",
        "            different_pos = set(parse.tag.POS for parse in parses if parse.tag.POS)\n",
        "            if len(different_pos) > 1:\n",
        "                ambiguous_words.append({\n",
        "                    'word': word,\n",
        "                    'parses': [(p.normal_form, p.tag.POS, p.score) for p in parses[:3]]  \n",
        "                })\n",
        "    \n",
        "    df = pd.DataFrame(analysis_results)\n",
        "    \n",
        "    print(\"Морфологический анализ текста:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(df.to_string(index=False))\n",
        "    \n",
        "    print(f\"\\nСтатистика по частям речи:\")\n",
        "    print(\"-\" * 30)\n",
        "    for pos, count in pos_counter.most_common():\n",
        "        print(f\"{pos or 'UNKN'}: {count}\")\n",
        "    \n",
        "    print(f\"\\nСлова с омонимичными разборами ({len(ambiguous_words)} слов):\")\n",
        "    print(\"-\" * 50)\n",
        "    for ambig in ambiguous_words[:5]:  \n",
        "        print(f\"Слово: '{ambig['word']}'\")\n",
        "        for i, (lemma, pos, score) in enumerate(ambig['parses']):\n",
        "            print(f\"  Вариант {i+1}: лемма='{lemma}', POS={pos}, score={score:.3f}\")\n",
        "        print()\n",
        "    \n",
        "    if ambiguous_words:\n",
        "        print(\"Способы разрешения неоднозначности:\")\n",
        "        print(\"1. Использование контекста (синтаксический анализ)\")\n",
        "        print(\"2. Выбор варианта с наибольшим score\")\n",
        "        print(\"3. Использование статистических моделей\")\n",
        "        print(\"4. Ручная проверка для критически важных случаев\")\n",
        "    \n",
        "    return df, pos_counter, ambiguous_words\n",
        "\n",
        "russian_text = \"\"\"\n",
        "Машинное обучение становится всё более популярным в современном мире.\n",
        "Компании используют алгоритмы машинного обучения для анализа данных и принятия решений.\n",
        "Инженеры по данным обучают модели на больших наборах данных, чтобы повысить их точность.\n",
        "Эти технологии меняют нашу жизнь, делая её более удобной и эффективной.\n",
        "\"\"\"\n",
        "\n",
        "df, pos_stats, ambiguous = analyze_russian_text(russian_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Gh_zSekF1EL"
      },
      "source": [
        "Упражнение 4: Обработка смешанного русско-английского текста с использованием spaCy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHzPZi7bF1sB"
      },
      "source": [
        "Задание: Разработайте систему для обработки смешанного русско-английского текста с использованием библиотеки spaCy. Ваша программа должна определять язык каждого слова, применять соответствующую модель для лемматизации и выполнять частеречную разметку.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Загрузите модели en_core_web_sm и ru_core_news_sm из spaCy\n",
        "* Реализуйте алгоритм определения языка для каждого токена (можно использовать множества символов кириллицы и латиницы)\n",
        "* Обработайте каждый токен соответствующей языковой моделью\n",
        "* Сформируйте два отдельных набора данных: лемматизированный русский и английский текст\n",
        "* Определите наиболее часто встречающиеся леммы для каждого языка"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0g_p-Y8KF-fh"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.9.2)\n",
            "Requirement already satisfied: spacy in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (3.8.7)\n",
            "Requirement already satisfied: pymorphy2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (0.9.1)\n",
            "Requirement already satisfied: natasha in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (1.6.0)\n",
            "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (2025.10.23)\n",
            "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.3.4)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.32.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (80.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: slovnet>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.6.0)\n",
            "Requirement already satisfied: yargy>=0.16.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.16.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from typer<1.0.0,>=0.3.0->spacy) (14.2.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/morkooovka/Library/Python/3.13/lib/python/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.2\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip3 install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "UsageError: Line magic function `%python` not found (But cell magic `%%python` exists, did you mean that instead?).\n"
          ]
        }
      ],
      "source": [
        "%pip install nltk spacy pymorphy2 natasha\n",
        "%python -m spacy download en_core_web_sm\n",
        "%python -m spacy download ru_core_news_sm\n",
        "import spacy\n",
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "def process_mixed_text(text):\n",
        "    \"\"\"\n",
        "    Обработка смешанного русско-английского текста с использованием spaCy\n",
        "    \"\"\"\n",
        "    nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "    nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "    \n",
        "    def is_cyrillic(char):\n",
        "        return 'а' <= char <= 'я' or char == 'ё'\n",
        "    \n",
        "    def is_latin(char):\n",
        "        return 'a' <= char <= 'z'\n",
        "    \n",
        "    def detect_language(token):\n",
        "        cyrillic_count = sum(1 for c in token if is_cyrillic(c))\n",
        "        latin_count = sum(1 for c in token if is_latin(c))\n",
        "        \n",
        "        if cyrillic_count > latin_count:\n",
        "            return 'russian'\n",
        "        elif latin_count > cyrillic_count:\n",
        "            return 'english'\n",
        "        else:\n",
        "            return 'unknown'\n",
        "    \n",
        "    tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    \n",
        "    russian_lemmas = []\n",
        "    english_lemmas = []\n",
        "    russian_pos = []\n",
        "    english_pos = []\n",
        "    \n",
        "    for token in tokens:\n",
        "        lang = detect_language(token)\n",
        "        \n",
        "        if lang == 'russian':\n",
        "            doc = nlp_ru(token)\n",
        "            for tok in doc:\n",
        "                if tok.is_alpha:\n",
        "                    russian_lemmas.append(tok.lemma_)\n",
        "                    russian_pos.append(tok.pos_)\n",
        "        \n",
        "        elif lang == 'english':\n",
        "            doc = nlp_en(token)\n",
        "            for tok in doc:\n",
        "                if tok.is_alpha:\n",
        "                    english_lemmas.append(tok.lemma_)\n",
        "                    english_pos.append(tok.pos_)\n",
        "    \n",
        "    russian_lemma_freq = Counter(russian_lemmas)\n",
        "    english_lemma_freq = Counter(english_lemmas)\n",
        "    \n",
        "    print(\"Обработка смешанного русско-английского текста\")\n",
        "    print(\"=\" * 60)\n",
        "    \n",
        "    print(f\"\\nРусская часть:\")\n",
        "    print(f\"Количество лемм: {len(russian_lemmas)}\")\n",
        "    print(f\"Уникальных лемм: {len(russian_lemma_freq)}\")\n",
        "    print(f\"Части речи: {Counter(russian_pos)}\")\n",
        "    \n",
        "    print(f\"\\nАнглийская часть:\")\n",
        "    print(f\"Количество лемм: {len(english_lemmas)}\")\n",
        "    print(f\"Уникальных лемм: {len(english_lemma_freq)}\")\n",
        "    print(f\"Части речи: {Counter(english_pos)}\")\n",
        "    \n",
        "    print(f\"\\nСамые частые русские леммы:\")\n",
        "    for lemma, count in russian_lemma_freq.most_common(10):\n",
        "        print(f\"  {lemma}: {count}\")\n",
        "    \n",
        "    print(f\"\\nСамые частые английские леммы:\")\n",
        "    for lemma, count in english_lemma_freq.most_common(10):\n",
        "        print(f\"  {lemma}: {count}\")\n",
        "    \n",
        "    return {\n",
        "        'russian': {\n",
        "            'lemmas': russian_lemmas,\n",
        "            'pos': russian_pos,\n",
        "            'lemma_freq': russian_lemma_freq\n",
        "        },\n",
        "        'english': {\n",
        "            'lemmas': english_lemmas,\n",
        "            'pos': english_pos,\n",
        "            'lemma_freq': english_lemma_freq\n",
        "        }\n",
        "    }\n",
        "\n",
        "mixed_text = \"\"\"\n",
        "Data science становится ключевой компетенцией в современном IT-мире.\n",
        "Machine learning инженеры и аналитики данных востребованы на рынке труда.\n",
        "Компании активно внедряют artificial intelligence решения для оптимизации бизнес-процессов.\n",
        "Работодатели ищут специалистов, владеющих языком программирования Python и имеющих опыт работы с big data.\n",
        "Deep learning модели показывают excellent результаты в computer vision задачах.\n",
        "\"\"\"\n",
        "\n",
        "results = process_mixed_text(mixed_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLVbeZHcGGK7"
      },
      "source": [
        "Упражнение 5*: Применение библиотеки Natasha для анализа именованных сущностей и лемматизации русского текста"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fs2q0cZOGLR7"
      },
      "source": [
        "Задание: Используйте библиотеку Natasha для выделения именованных сущностей (NER) и лемматизации русского текста. Создайте функцию, которая принимает текст, выделяет и классифицирует в нём именованные сущности (имена людей, организации, локации и т.д.), а также выполняет лемматизацию всех слов.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Используйте Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger из библиотеки Natasha\n",
        "* Выделите все именованные сущности в тексте и определите их тип\n",
        "* Выполните лемматизацию всех слов, учитывая контекст (используя синтаксический анализ)\n",
        "* Сформируйте структурированный результат, включающий: список всех именованных сущностей с указанием их типа, список всех слов с их леммами, статистику по типам найденных сущностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vkD1Mfj-GdAX"
      },
      "outputs": [],
      "source": [
        "# ваш код\n",
        "\n",
        "# Текст для проверки\n",
        "\n",
        "russian_ner_text = \"\"\"\n",
        "Президент России Владимир Путин провел совещание с правительством в Москве.\n",
        "Компания Яндекс представила новую технологию для анализа данных.\n",
        "Сотрудники Московского государственного университета и Института проблем искусственного интеллекта РАН\n",
        "разработали алгоритм для обработки естественного языка.\n",
        "Исследователи из Санкт-Петербурга и их коллеги из Казани опубликовали результаты исследований\n",
        "в журнале \"Компьютерная лингвистика и интеллектуальные технологии\".\n",
        "\"\"\""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
