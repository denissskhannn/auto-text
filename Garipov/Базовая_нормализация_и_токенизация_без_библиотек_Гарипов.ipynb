{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В этом ноутбуке вы узнаете:\n",
        "\n",
        "1.   Как использовать встроенные методы Python для работы со строками\n",
        "2.   Как выполнять базовую нормализацию текста\n",
        "3. Как токенизировать текст с помощью простых методов\n",
        "4. Как применять эти навыки на практике\n"
      ],
      "metadata": {
        "id": "xaCyWYJ6uGfk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Основные понятия и функции**"
      ],
      "metadata": {
        "id": "T4-yFhGcy934"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В Python текст представлен в виде строк (тип данных str). Строки можно создавать, используя одинарные или двойные кавычки."
      ],
      "metadata": {
        "id": "mMYBk9PluRfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание строк\n",
        "text1 = 'Это текст в \"одинарных\" кавычках'\n",
        "text2 = \"Это текст в 'двойных' кавычках\"\n",
        "text3 = '''Это текст в \"тройных\" кавычках'''\n",
        "text4 = '''Это\n",
        "текст\n",
        "в 'тройных'\n",
        "кавычках'''\n",
        "\n",
        "# Вывод строк\n",
        "print(text1)\n",
        "print(text2)\n",
        "print(text3)\n",
        "print(text4)"
      ],
      "metadata": {
        "id": "tXalOImZuMdk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1. Индексация и срезы строк.\n",
        "Строки в Python — это последовательности символов, к которым можно обращаться по индексу. Индексация начинается с 0."
      ],
      "metadata": {
        "id": "4dVUdNKvuYvU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Строка для примеров\n",
        "sample_text = \"Компьютерная лингвистика\"\n",
        "\n",
        "# Получение отдельного символа\n",
        "print(\"Первый символ:\", sample_text[0])\n",
        "print(\"Пятый символ:\", sample_text[4])\n",
        "\n",
        "# Срезы строк\n",
        "print(\"Первые 12 символов:\", sample_text[:12])  # от начала до 12-го символа (не включая)\n",
        "print(\"С 13-го символа до конца:\", sample_text[13:])  # с 13-го символа до конца\n",
        "print(\"С 5-го по 10-й символ:\", sample_text[5:11])  # с 5-го по 10-й символ (11-й не включается)"
      ],
      "metadata": {
        "id": "8RlZDm97uVrC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2. Основные методы строк\n",
        "Python предоставляет множество встроенных методов для работы со строками. Рассмотрим наиболее полезные для обработки текста:"
      ],
      "metadata": {
        "id": "-oi1N22TvbQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Строка для примеров\n",
        "text = \"   Пример текста для анализа.   \"\n",
        "\n",
        "# Удаление пробелов в начале и конце строки\n",
        "print(\"После strip():\", text.strip())\n",
        "\n",
        "# Перевод в нижний регистр\n",
        "print(\"После lower():\", text.lower())\n",
        "\n",
        "# Перевод в верхний регистр\n",
        "print(\"После upper():\", text.upper())\n",
        "\n",
        "# Проверка, начинается ли строка с определенной подстроки\n",
        "print(\"Начинается с 'Пример'?\", text.strip().startswith(\"Пример\"))\n",
        "\n",
        "# Проверка, заканчивается ли строка определенной подстрокой\n",
        "print(\"Заканчивается на '.'?\", text.strip().endswith(\".\"))\n",
        "\n",
        "# Замена подстроки\n",
        "print(\"После replace():\", text.replace(\"анализа\", \"обработки\"))\n",
        "\n",
        "# Подсчет вхождений подстроки\n",
        "print(\"Количество пробелов:\", text.count(\" \"))"
      ],
      "metadata": {
        "id": "vkcKQqqgvcpk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3. Конкатенация строк и форматирование.\n",
        "Существует несколько способов объединения строк в Python:"
      ],
      "metadata": {
        "id": "-oEf7vLCvvzL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Простая конкатенация с помощью оператора +\n",
        "first_name = \"Иван\"\n",
        "last_name = \"Петров\"\n",
        "full_name = first_name + \" \" + last_name\n",
        "print(\"Полное имя:\", full_name)\n",
        "\n",
        "# Форматирование строк с помощью метода format()\n",
        "age = 25\n",
        "message = \"Меня зовут {}, мне {} лет.\".format(full_name, age)\n",
        "print(message)\n",
        "\n",
        "# f-строки (начиная с Python 3.6)\n",
        "message_f = f\"Меня зовут {full_name}, мне {age} лет.\"\n",
        "print(message_f)\n",
        "\n",
        "# Использование %\n",
        "message_old = \"Меня зовут %s, мне %d лет.\" % (full_name, age)\n",
        "print(message_old)"
      ],
      "metadata": {
        "id": "CS50TkzKvwfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Базовая нормализация текста**"
      ],
      "metadata": {
        "id": "e0K6mt6Vv6gV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Нормализация текста — это процесс преобразования текста к стандартному виду для облегчения его дальнейшей обработки. Рассмотрим базовые операции нормализации."
      ],
      "metadata": {
        "id": "hL2BXNo8wEVY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1. Приведение к нижнему регистру помогает унифицировать текст. Обычно используется нижний регистр."
      ],
      "metadata": {
        "id": "J5Gk1AnRwHK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста с разным регистром\n",
        "mixed_case_text = \"КомпьЮтерная ЛИНГвистика изучает Методы Автоматической Обработки Текста.\"\n",
        "\n",
        "# Приведение к нижнему регистру\n",
        "normalized_text = mixed_case_text.lower()\n",
        "print(\"Исходный текст:\", mixed_case_text)\n",
        "print(\"Нормализованный текст:\", normalized_text)\n",
        "\n",
        "# Пример на английском\n",
        "english_text = \"Natural Language Processing (NLP) is a FIELD of AI.\"\n",
        "print(\"Английский текст после нормализации:\", english_text.lower())"
      ],
      "metadata": {
        "id": "vNNMpCqOv2Dz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2. Удаление лишних пробелов. Часто тексты содержат лишние пробелы, которые нужно удалить.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Vlwzl4-bw7u9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Текст с лишними пробелами\n",
        "text_with_spaces = \"   В  этом   тексте  есть  лишние    пробелы.   \"\n",
        "\n",
        "# Удаление пробелов в начале и конце\n",
        "trimmed_text = text_with_spaces.strip()\n",
        "print(\"После strip():\", trimmed_text)\n",
        "\n",
        "words = trimmed_text.split()  # Разбиваем текст на слова\n",
        "normalized_manually = ' '.join(words)  # Объединяем слова с одним пробелом\n",
        "print(\"Нормализация:\", normalized_manually)\n",
        "\n",
        "# Пример на английском\n",
        "english_spaces = \"  This   text   has   extra   spaces. \"\n",
        "english_normalized = ' '.join(english_spaces.strip().split())\n",
        "print(\"Английский текст после нормализации пробелов:\", english_normalized)"
      ],
      "metadata": {
        "id": "AioDUYPrw-Hl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3. Удаление пунктуации. Для многих задач обработки текста необходимо удалить знаки пунктуации."
      ],
      "metadata": {
        "id": "M7bFiY8syfRf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Текст с пунктуацией\n",
        "text_with_punctuation = \"Привет, мир! Как дела? Это - пример текста; с разными знаками пунктуации.\"\n",
        "\n",
        "# Определение знаков пунктуации для удаления\n",
        "import string\n",
        "punctuation = string.punctuation  # !\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
        "print(\"Знаки пунктуации:\", punctuation)\n",
        "\n",
        "# Удаление пунктуации (метод 1)\n",
        "no_punctuation = ''.join(char for char in text_with_punctuation if char not in punctuation)\n",
        "print(\"Текст без пунктуации (метод 1):\", no_punctuation)\n",
        "\n",
        "# Удаление пунктуации (метод 2)\n",
        "translator = str.maketrans('', '', punctuation)\n",
        "no_punctuation2 = text_with_punctuation.translate(translator)\n",
        "print(\"Текст без пунктуации (метод 2):\", no_punctuation2)\n",
        "\n",
        "# Пример на английском\n",
        "english_punctuation = \"Hello, world! How are you? This is a sample text; with various punctuation marks.\"\n",
        "english_no_punctuation = english_punctuation.translate(translator)\n",
        "print(\"Английский текст без пунктуации:\", english_no_punctuation)"
      ],
      "metadata": {
        "id": "g7M5Bna1yjB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.4. Комплексная нормализация текста. Объединим все изученные методы для комплексной нормализации текста."
      ],
      "metadata": {
        "id": "cCzDj3GayrSf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_text(text):\n",
        "    \"\"\"\n",
        "    Функция для комплексной нормализации текста:\n",
        "    1. Приведение к нижнему регистру\n",
        "    2. Удаление лишних пробелов\n",
        "    3. Удаление пунктуации\n",
        "    \"\"\"\n",
        "    # Шаг 1: Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Шаг 2: Удаление пунктуации\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    # Шаг 3: Удаление лишних пробелов\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "# Проверка функции на русском тексте\n",
        "russian_text = \"   Компьютерная    ЛИНГВИСТИКА - это ОБЛАСТЬ науки, изучающая   методы автоматической    обработки текста!   \"\n",
        "normalized_russian = normalize_text(russian_text)\n",
        "print(\"Исходный русский текст:\", russian_text)\n",
        "print(\"Нормализованный русский текст:\", normalized_russian)\n",
        "\n",
        "# Проверка функции на английском тексте\n",
        "english_text = \"   Natural    LANGUAGE Processing (NLP) - is a FIELD of AI, focusing   on text analysis!   \"\n",
        "normalized_english = normalize_text(english_text)\n",
        "print(\"Исходный английский текст:\", english_text)\n",
        "print(\"Нормализованный английский текст:\", normalized_english)"
      ],
      "metadata": {
        "id": "-EQfaUxNyuOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3. Простая токенизация текста**"
      ],
      "metadata": {
        "id": "H2IvHG_yy1uJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизация — это процесс разделения текста на отдельные токены (обычно слова или предложения). Рассмотрим простые методы токенизации с использованием встроенных возможностей Python."
      ],
      "metadata": {
        "id": "ISndfr9tzMRC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1. Токенизация по пробелам. Самый простой способ токенизации — разделение текста по пробелам с помощью метода .split()."
      ],
      "metadata": {
        "id": "sjvNUNImzP5i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример текста\n",
        "text = \"Компьютерная лингвистика изучает методы обработки текста\"\n",
        "\n",
        "# Токенизация по пробелам\n",
        "tokens = text.split()\n",
        "print(\"Токены:\", tokens)\n",
        "print(\"Количество токенов:\", len(tokens))\n",
        "\n",
        "# Токенизация английского текста\n",
        "english_text = \"Natural language processing studies methods of text analysis\"\n",
        "english_tokens = english_text.split()\n",
        "print(\"Английские токены:\", english_tokens)\n",
        "print(\"Количество английских токенов:\", len(english_tokens))"
      ],
      "metadata": {
        "id": "pRMRYdZfzTk2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.2. Токенизация с учетом пунктуации. Токенизация по пробелам не учитывает пунктуацию. Для более точной токенизации можно сначала отделить пунктуацию от слов."
      ],
      "metadata": {
        "id": "Ey-4jl42zahC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_tokenize(text):\n",
        "    \"\"\"\n",
        "    Простая токенизация с учетом пунктуации:\n",
        "    1. Добавляем пробелы вокруг знаков пунктуации\n",
        "    2. Разделяем по пробелам\n",
        "    3. Удаляем пустые токены\n",
        "    \"\"\"\n",
        "    # Добавляем пробелы вокруг знаков пунктуации\n",
        "    for punct in string.punctuation:\n",
        "        text = text.replace(punct, f' {punct} ')\n",
        "\n",
        "    # Разделяем по пробелам и удаляем пустые токены\n",
        "    tokens = [token for token in text.split() if token]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# Проверка на русском тексте\n",
        "russian_text = \"Привет, мир! Как дела? Это - пример текста; с разными знаками пунктуации.\"\n",
        "russian_tokens = simple_tokenize(russian_text)\n",
        "print(\"Русский текст:\", russian_text)\n",
        "print(\"Токены русского текста:\", russian_tokens)\n",
        "print(\"Количество токенов:\", len(russian_tokens))\n",
        "\n",
        "# Проверка на английском тексте\n",
        "english_text = \"Hello, world! How are you? This is a sample text; with various punctuation marks.\"\n",
        "english_tokens = simple_tokenize(english_text)\n",
        "print(\"Английский текст:\", english_text)\n",
        "print(\"Токены английского текста:\", english_tokens)\n",
        "print(\"Количество токенов:\", len(english_tokens))"
      ],
      "metadata": {
        "id": "8rYkdzlnzdkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.3. Токенизация предложений. Простой способ — разделение по знакам конца предложения."
      ],
      "metadata": {
        "id": "qfbRKk1S0wq_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def simple_sentence_tokenize(text):\n",
        "    \"\"\"\n",
        "    Простая токенизация на предложения:\n",
        "    1. Заменяем знаки конца предложения на специальный маркер\n",
        "    2. Разделяем текст по маркеру\n",
        "    3. Очищаем полученные предложения\n",
        "    \"\"\"\n",
        "    # Заменяем знаки конца предложения\n",
        "    for end_mark in ['.', '!', '?']:\n",
        "        text = text.replace(end_mark, f'{end_mark}SENTENCE_END')\n",
        "\n",
        "    # Разделяем по маркеру\n",
        "    sentences = text.split('SENTENCE_END')\n",
        "\n",
        "    # Очищаем предложения и удаляем пустые\n",
        "    cleaned_sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    return cleaned_sentences\n",
        "\n",
        "# Проверка на русском тексте\n",
        "russian_text = \"Привет, мир! Как дела? Это пример текста. Он состоит из нескольких предложений.\"\n",
        "russian_sentences = simple_sentence_tokenize(russian_text)\n",
        "print(\"Русский текст:\", russian_text)\n",
        "print(\"Предложения русского текста:\")\n",
        "for i, sentence in enumerate(russian_sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")\n",
        "\n",
        "# Проверка на английском тексте\n",
        "english_text = \"Hello, world! How are you? This is a sample text. It consists of several sentences.\"\n",
        "english_sentences = simple_sentence_tokenize(english_text)\n",
        "print(\"\\nАнглийский текст:\", english_text)\n",
        "print(\"Предложения английского текста:\")\n",
        "for i, sentence in enumerate(english_sentences, 1):\n",
        "    print(f\"{i}. {sentence}\")"
      ],
      "metadata": {
        "id": "bw3dAKeG0-fA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.4. Объединение токенов обратно в текст. Метод .join() позволяет объединить токены обратно в текст, указав разделитель."
      ],
      "metadata": {
        "id": "_a-rRcLp1NnW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Пример токенов\n",
        "tokens = [\"Компьютерная\", \"лингвистика\", \"изучает\", \"методы\", \"обработки\", \"текста\"]\n",
        "\n",
        "# Объединение токенов с пробелом\n",
        "text = ' '.join(tokens)\n",
        "print(\"Объединенный текст:\", text)\n",
        "\n",
        "# Объединение с другими разделителями\n",
        "comma_separated = ', '.join(tokens)\n",
        "print(\"Токены через запятую:\", comma_separated)\n",
        "\n",
        "# Объединение английских токенов\n",
        "english_tokens = [\"Natural\", \"language\", \"processing\", \"studies\", \"methods\", \"of\", \"text\", \"analysis\"]\n",
        "english_text = ' '.join(english_tokens)\n",
        "print(\"Объединенный английский текст:\", english_text)"
      ],
      "metadata": {
        "id": "jCOkR50y1PYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4. Практические упражнения**"
      ],
      "metadata": {
        "id": "zTAF-Lj21evo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 1: Нормализация текста. Напишите функцию, которая будет нормализовать текст следующим образом:\n",
        "\n",
        "1. Привести к нижнему регистру\n",
        "2. Удалить все цифры\n",
        "3. Заменить все символы пунктуации на пробелы\n",
        "4. Удалить лишние пробелы\n",
        "\n"
      ],
      "metadata": {
        "id": "2ffDLEVL1oiZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Пример текста123, с РАЗНЫМИ символами! И 456 цифрами?!\"\n",
        "# ваш код\n",
        "\n",
        "import string\n",
        "\n",
        "def normalize_text(text):\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    digits = \"1234567890\"\n",
        "    text = ''.join(char for char in text if char not in digits)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "\n",
        "    return text\n",
        "\n",
        "normalized_text = normalize_text(sample_text)\n",
        "print(\"Нормализованный текст:\", normalized_text)"
      ],
      "metadata": {
        "id": "rU4ZTUrd1jFX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f73a88f3-c047-4a70-abe0-611222f68a77"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Нормализованный текст: пример текста с разными символами и цифрами\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 2: Подсчет частотности слов. Напишите функцию, которая будет принимать текст, нормализовать его, токенизировать и возвращать словарь с частотностью каждого слова."
      ],
      "metadata": {
        "id": "waC9ZHWB2Kph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Компьютерная лингвистика изучает методы обработки текста. Компьютерная лингвистика является областью искусственного интеллекта.\"\n",
        "english_text = \"Natural language processing is a field of artificial intelligence. Natural language processing focuses on the interaction between computers and humans.\"\n",
        "# ваш код\n",
        "\n",
        "def simple_tokenize(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    digits = \"1234567890\"\n",
        "    text = ''.join(char for char in text if char not in digits)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    text = text.split()\n",
        "\n",
        "    dictionary = {}\n",
        "    for word in text:\n",
        "      dictionary[word] = text.count(word)\n",
        "\n",
        "    return dictionary\n",
        "\n",
        "russian_tokens = simple_tokenize(sample_text)\n",
        "english_tokens = simple_tokenize(english_text)\n",
        "print(\"Частотный словарь русского текста:\", russian_tokens)\n",
        "print(\"Частотный словарь английского текста:\", english_tokens)"
      ],
      "metadata": {
        "id": "HlWO9Bpn2SQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13444c75-3ab2-4253-dc1c-96116dd03b77"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частотный словарь русского текста: {'компьютерная': 2, 'лингвистика': 2, 'изучает': 1, 'методы': 1, 'обработки': 1, 'текста': 1, 'является': 1, 'областью': 1, 'искусственного': 1, 'интеллекта': 1}\n",
            "Частотный словарь английского текста: {'natural': 2, 'language': 2, 'processing': 2, 'is': 1, 'a': 1, 'field': 1, 'of': 1, 'artificial': 1, 'intelligence': 1, 'focuses': 1, 'on': 1, 'the': 1, 'interaction': 1, 'between': 1, 'computers': 1, 'and': 1, 'humans': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 3: Поиск самых длинных и коротких слов. Напишите функцию, которая находит самые длинные и самые короткие слова в тексте."
      ],
      "metadata": {
        "id": "RPTWlfsD2ehu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Компьютерная лингвистика является междисциплинарной областью науки, которая изучает математические и компьютерные модели естественного языка, а также его применение в системах искусственного интеллекта.\"\n",
        "english_text = \"Computational linguistics is an interdisciplinary field of science that studies mathematical and computational models of natural language and its application in artificial intelligence systems.\"\n",
        "# ваш код\n",
        "\n",
        "def words(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    digits = \"1234567890\"\n",
        "    text = ''.join(char for char in text if char not in digits)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "    for punct in string.punctuation:\n",
        "        text = text.replace(punct, f' {punct} ')\n",
        "\n",
        "    tokens = [token for token in text.split() if token]\n",
        "\n",
        "    longest_word = max(tokens, key=len)\n",
        "    shortest_word = min(tokens, key=len)\n",
        "\n",
        "    return longest_word, shortest_word\n",
        "\n",
        "english_words = words(english_text)\n",
        "russian_words = words(sample_text)\n",
        "\n",
        "print(\"В английском тексте cамое длинное слово это '{}', самое короткое - '{}'\".format(english_words[0], english_words[1]))\n",
        "print(\"В русском тексте cамое длинное слово это '{}', самое короткое - '{}'\".format(russian_words[0], russian_words[1]))"
      ],
      "metadata": {
        "id": "hizov4xW2YhX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb64f2c7-151e-43af-dae9-cbe3f31f2748"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "В английском тексте cамое длинное слово это 'interdisciplinary', самое короткое - 'is'\n",
            "В русском тексте cамое длинное слово это 'междисциплинарной', самое короткое - 'и'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 4: Поиск и подсчет определенных паттернов. Напишите функцию, которая находит и подсчитывает слова, начинающиеся с определенной буквы или содержащие определенную последовательность букв."
      ],
      "metadata": {
        "id": "wtQ6vmk922D6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"Компьютерная лингвистика изучает методы обработки текста. Математические модели и компьютерные алгоритмы используются для анализа естественных языков.\"\n",
        "\n",
        "# Поиск слов, начинающихся с 'к'\n",
        "# ваш код\n",
        "\n",
        "def finder(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    digits = \"1234567890\"\n",
        "    text = ''.join(char for char in text if char not in digits)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    text = text.split()\n",
        "    k_tokens = []\n",
        "\n",
        "    for word in text:\n",
        "        if word.startswith(\"к\"):\n",
        "            k_tokens.append(word)\n",
        "\n",
        "    return k_tokens\n",
        "\n",
        "k_words = finder(sample_text)\n",
        "print(k_words)\n"
      ],
      "metadata": {
        "id": "Pv0AYXVp23vc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7617b8bd-df5d-4bd7-def9-2933065318e9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['компьютерная', 'компьютерные']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = \"Computational linguistics studies methods of text processing. Mathematical models and computer algorithms are used to analyze natural languages.\"\n",
        "\n",
        "# Поиск слов, содержащих 'ing'\n",
        "# ваш код\n",
        "\n",
        "def finder(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    digits = \"1234567890\"\n",
        "    text = ''.join(char for char in text if char not in digits)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    text = text.split()\n",
        "    ing_tokens = []\n",
        "\n",
        "    for word in text:\n",
        "        if word.endswith(\"ing\"):\n",
        "            ing_tokens.append(word)\n",
        "\n",
        "    return ing_tokens\n",
        "\n",
        "words = finder(english_text)\n",
        "print(\"Слова, заканчивающиеся на 'ing':\", words)\n",
        "print(words)"
      ],
      "metadata": {
        "id": "1i4Yjoum26nz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5190682-84f6-4eb0-9fa2-14db9167bb24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слова, заканчивающиеся на 'ing': ['processing']\n",
            "['processing']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 5: Анализ предложений в тексте. Напишите функцию, которая анализирует текст на уровне предложений, подсчитывая:\n",
        "\n",
        "1. Количество предложений\n",
        "2. Среднюю длину предложения (в словах)\n",
        "3. Самое длинное и самое короткое предложение\n"
      ],
      "metadata": {
        "id": "anDhdjLG3M1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"Компьютерная лингвистика — это междисциплинарная область науки.\n",
        "Она изучает математические и компьютерные модели естественного языка.\n",
        "Методы компьютерной лингвистики применяются для решения различных задач, таких как машинный перевод, автоматическое реферирование и информационный поиск.\n",
        "Современные алгоритмы позволяют анализировать большие объемы текстов.\"\"\"\n",
        "\n",
        "# ваш код\n",
        "def analyze_text(text):\n",
        "    sentences = text.split('.')\n",
        "\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    lengths = [len(sentence.split()) for sentence in sentences]\n",
        "\n",
        "    average_length = sum(lengths) / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    longest_sentence = max(sentences, key=len) if sentences else \"\"\n",
        "    shortest_sentence = min(sentences, key=len) if sentences else \"\"\n",
        "\n",
        "    return {\n",
        "        \"number_of_sentences\": num_sentences,\n",
        "        \"average_length\": average_length,\n",
        "        \"longest_sentence\": longest_sentence,\n",
        "        \"shortest_sentence\": shortest_sentence\n",
        "    }\n",
        "result = analyze_text(sample_text)\n",
        "\n",
        "# Анализ предложений\n",
        "print(\"Результаты анализа предложений:\")\n",
        "print(\"Количество предложений:\", result[\"number_of_sentences\"])\n",
        "print(\"Средняя длина предложения (в словах):\", result[\"average_length\"])\n",
        "print(\"Самое длинное предложение:\", result[\"longest_sentence\"])\n",
        "print(\"Самое короткое предложение:\", result[\"shortest_sentence\"])"
      ],
      "metadata": {
        "id": "QQd2ZgIj3YlJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54e1d34-6b74-49fe-8c7b-dd29cf0ad2d1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Результаты анализа предложений:\n",
            "Количество предложений: 4\n",
            "Средняя длина предложения (в словах): 9.75\n",
            "Самое длинное предложение: Методы компьютерной лингвистики применяются для решения различных задач, таких как машинный перевод, автоматическое реферирование и информационный поиск\n",
            "Самое короткое предложение: Компьютерная лингвистика — это междисциплинарная область науки\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "english_text = \"\"\"Computational linguistics is an interdisciplinary field of science.\n",
        "It studies mathematical and computational models of natural language.\n",
        "Methods of computational linguistics are applied to solve various tasks such as machine translation, automatic summarization, and information retrieval.\n",
        "Modern algorithms allow analyzing large volumes of texts.\"\"\"\n",
        "\n",
        "# ваш код\n",
        "def analyze_text(text):\n",
        "    sentences = text.split('.')\n",
        "\n",
        "    sentences = [sentence.strip() for sentence in sentences if sentence.strip()]\n",
        "\n",
        "    num_sentences = len(sentences)\n",
        "\n",
        "    lengths = [len(sentence.split()) for sentence in sentences]\n",
        "\n",
        "    average_length = sum(lengths) / num_sentences if num_sentences > 0 else 0\n",
        "\n",
        "    longest_sentence = max(sentences, key=len) if sentences else \"\"\n",
        "    shortest_sentence = min(sentences, key=len) if sentences else \"\"\n",
        "\n",
        "    return {\n",
        "        \"number_of_sentences\": num_sentences,\n",
        "        \"average_length\": average_length,\n",
        "        \"longest_sentence\": longest_sentence,\n",
        "        \"shortest_sentence\": shortest_sentence\n",
        "    }\n",
        "result = analyze_text(english_text)\n",
        "\n",
        "# Анализ предложений на английском\n",
        "print(\"\\nРезультаты анализа предложений на английском:\")\n",
        "print(\"Количество предложений:\", result[\"number_of_sentences\"])\n",
        "print(\"Средняя длина предложения (в словах):\", result[\"average_length\"])\n",
        "print(\"Самое длинное предложение:\", result[\"longest_sentence\"])\n",
        "print(\"Самое короткое предложение:\", result[\"shortest_sentence\"])"
      ],
      "metadata": {
        "id": "N7pW-9_c3rmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2208a3ab-9f75-43c4-c130-75c0c74fc4e2"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Результаты анализа предложений на английском:\n",
            "Количество предложений: 4\n",
            "Средняя длина предложения (в словах): 11.0\n",
            "Самое длинное предложение: Methods of computational linguistics are applied to solve various tasks such as machine translation, automatic summarization, and information retrieval\n",
            "Самое короткое предложение: Modern algorithms allow analyzing large volumes of texts\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**5. Домашнее задание**"
      ],
      "metadata": {
        "id": "ZE7TEjrM37te"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Напишите функцию для нормализации текста, которая:\n",
        "\n",
        "1. Приводит текст к нижнему регистру\n",
        "2. Удаляет все знаки пунктуации\n",
        "3. Заменяет все цифры на символ '#'\n",
        "4. Удаляет лишние пробелы\n",
        "\n",
        "Напишите функцию, которая находит и выводит все слова из текста, содержащие гласные в определенной последовательности (например, 'о' и затем 'а').\n",
        "\n",
        "Создайте функцию, которая:\n",
        "\n",
        "1.   Токенизирует текст\n",
        "2.   Отбирает только слова длиной более 4 символов\n",
        "3. Сортирует их по алфавиту\n",
        "4. Возвращает первые 10 слов\n",
        "\n",
        "Напишите функцию для анализа частотности символов в тексте, которая возвращает 5 самых часто встречающихся символов и их количество.\n",
        "\n",
        "Напишите программу, которая разделяет текст на параграфы (по двойному переносу строки), а затем подсчитывает для каждого параграфа: количество предложений, количество слов, среднюю длину слова"
      ],
      "metadata": {
        "id": "qJtWAqaL3-od"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Первое ДЗ"
      ],
      "metadata": {
        "id": "NPwhCBhIPkTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "sample_text = \"Первая экспедиция (1492—1493 годы) Христофора Колумба в составе 91 человека на судах «Санта-Мария», «Пинта», «Нинья» вышла из Па́лоса-де-ла-Фронтера 3 августа 1492 года, от Канарских островов повернула на Запад (9 сентября), пересекла Атлантический океан в субтропическом поясе и достигла острова Сан-Сальвадор в Багамском архипелаге, где Христофор Колумб высадился 12 октября 1492 года (официальная дата открытия Америки).\"\n",
        "def normalize_text(text):\n",
        "    text = text.lower()\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    text = re.sub(r'\\d', '#', text)\n",
        "    text = text.strip()\n",
        "    text = text.split()\n",
        "    text = ' '.join(text)\n",
        "    return text\n",
        "\n",
        "def sort_text(text):\n",
        "    tokens = text.split()\n",
        "    sorted_tokens = []\n",
        "    for i in range(len(tokens)):\n",
        "        if len(tokens[i]) > 4:\n",
        "            sorted_tokens.append(tokens[i])\n",
        "    sorted_tokens = sorted(sorted_tokens)\n",
        "    return sorted_tokens\n",
        "\n",
        "print(sort_text(normalize_text(sample_text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnV0wL7KLLVO",
        "outputId": "107fed99-5de4-4075-922f-187062011962"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['####—####', '«нинья»', '«пинта»', '«сантамария»', 'августа', 'америки', 'архипелаге', 'атлантический', 'багамском', 'высадился', 'вышла', 'достигла', 'запад', 'канарских', 'колумб', 'колумба', 'океан', 'октября', 'острова', 'островов', 'открытия', 'официальная', 'па́лосаделафронтера', 'первая', 'пересекла', 'повернула', 'поясе', 'сансальвадор', 'сентября', 'составе', 'субтропическом', 'судах', 'христофор', 'христофора', 'человека', 'экспедиция']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Второе ДЗ"
      ],
      "metadata": {
        "id": "oA24vWwiPm7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "# Функция для для вывода слов с определенной последователньостью гласных\n",
        "def words_with_two_syllables(X, Z, tokens):\n",
        "  two_syllables_list = []\n",
        "  pattern = f\"{X}.*{Z}\"\n",
        "  for i in tokens:\n",
        "    matches = re.findall(pattern, i)\n",
        "    if matches:\n",
        "      two_syllables_list.append(i)\n",
        "  print(f'Слова с последовательностью гласных «{X}», «{Z}»: {two_syllables_list}')\n",
        "\n",
        "def tokenize_homework(wiki_text):\n",
        "  #НОРМАЛИЗАЦИЯ ТЕКСТА\n",
        "  #убираем цифорки\n",
        "  punctuation = string.punctuation\n",
        "  numbers = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"0\"];\n",
        "\n",
        "  #приводим к нижнему регистру\n",
        "  text = wiki_text.lower();\n",
        "\n",
        "  #удаляем цифры\n",
        "  text_no_numbers = ''.join(char for char in text if char not in numbers);\n",
        "\n",
        "  #заменяем все символы пунктуации на пробелы\n",
        "  translator = str.maketrans('', '', punctuation)\n",
        "  normal_text = text_no_numbers.translate(translator)\n",
        "\n",
        "  #Удаление лишних пробелов\n",
        "  normal_text = ' '.join(normal_text.split())\n",
        "\n",
        "  #ТОКЕНИЗАЦИЯ ТЕКСТОВ\n",
        "\n",
        "  #Токенизация по пробелам\n",
        "  tokens = normal_text.split()\n",
        "\n",
        "  return tokens\n",
        "\n",
        "wiki_text = \"\"\"2-я экспедиция (1493—1496 годы), которую Христофор Колумб возглавил уже в чине адмирала, и в должности вице-короля вновь открытых земель, состояла из 17 судов с экипажем свыше 1,5 тыс. человек. 3 ноября 1493 года Колумб открыл острова Доминика и Гваделупа, повернув на Северо-Запад, — ещё около 20 Малых Антильских островов, в том числе Антигуа и Виргинские, а 19 ноября — остров Пуэрто-Рико и подошёл к северному берегу Гаити. 12—29 марта 1494 года Колумб в поисках золота совершил завоевательный поход внутрь Гаити, причём пересёк хребет Кордильера-Сентраль. 29 апреля — 3 мая Колумб с 3 судами прошёл вдоль юго-восточного берега Кубы, повернул от мыса Крус на Юг и 5 мая открыл остров Ямайка. Вернувшись 15 мая к мысу Крус, Колумб прошёл вдоль южного побережья Кубы до 84° западной долготы, обнаружил архипелаг Хардинес-де-ла-Рейна, полуостров Сапата и остров Пинос. 24 июня Христофор Колумб повернул на восток и обследовал 19 августа — 15 сентября весь южный берег Гаити. В 1495 году Христофор Колумб продолжил завоевание Гаити; 10 марта 1496 года оставил остров и 11 июня вернулся в Кастилию.\"\"\"\n",
        "\n",
        "tokens = tokenize_homework(wiki_text)\n",
        "\n",
        "#Слова с последовательностью гласных «А», «У»\n",
        "words_with_two_syllables(\"а\", \"у\", tokens)\n",
        "\n",
        "#Слова с последовательностью гласных «А», «Е»\n",
        "words_with_two_syllables(\"а\", \"е\", tokens)\n",
        "\n",
        "#Слова с последовательностью гласных «Е», «И»\n",
        "words_with_two_syllables(\"е\", \"и\", tokens)\n",
        "\n",
        "#Слова с последовательностью гласных «А», «О»\n",
        "words_with_two_syllables(\"а\", \"о\", tokens)\n",
        "\n",
        "#Слова с последовательностью гласных «Е», «О»\n",
        "words_with_two_syllables(\"е\", \"о\", tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ME-RFZ0vL_AV",
        "outputId": "6d6541da-fb4b-45ad-ac76-69d0ce2c4725"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Слова с последовательностью гласных «а», «у»: ['гваделупа', 'антигуа', 'обнаружил', 'августа']\n",
            "Слова с последовательностью гласных «а», «е»: ['экипажем', 'гваделупа', 'завоевательный', 'кордильерасентраль', 'апреля', 'архипелаг', 'хардинесделарейна', 'завоевание']\n",
            "Слова с последовательностью гласных «е», «и»: ['экспедиция', 'совершил', 'вернувшись', 'завоевание']\n",
            "Слова с последовательностью гласных «а», «о»: ['завоевательный', 'западной', 'завоевание']\n",
            "Слова с последовательностью гласных «е», «о»: ['вицекороля', 'человек', 'северозапад', 'северному', 'обследовал']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Третье ДЗ"
      ],
      "metadata": {
        "id": "W8KDhc9jPo36"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"3-я экспедиция (1498—1500 годы) состояла из 6 судов, 3 из которых сам Христофор Колумб повёл через Атлантический океан близ 10° северной широты. 31 июля 1498 года он открыл остров Тринидад, вошёл с юга в залив Пария, обнаружил устье западного рукава дельты реки Ориноко и полуостров Пария, положив начало открытию Южной Америки. Затем выйдя в Карибское море, Христофор Колумб подходил к полуострову Арая, открыл 15 августа остров Маргарита и 31 августа прибыл в город Санто-Доминго (на острове Гаити). В 1500 году Христофор Колумб был по доносу арестован и отправлен в Кастилию, где был освобождён.\"\"\"\n",
        "def words(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    digits = \"1234567890\"\n",
        "    text = ''.join(char for char in text if char not in digits)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    text = ' '.join(text.split())\n",
        "    for punct in string.punctuation:\n",
        "        text = text.replace(punct, f' {punct} ')\n",
        "\n",
        "    tokens = [token for token in text.split() if token]\n",
        "\n",
        "    dictionary = []\n",
        "\n",
        "    for word in tokens:\n",
        "      if len(word) > 4 :\n",
        "        dictionary.append(word)\n",
        "\n",
        "    dictionary.sort()\n",
        "\n",
        "    return dictionary[:10]\n",
        "\n",
        "task = words(sample_text)\n",
        "\n",
        "print(\"\", task)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I9H4afLqN6Tm",
        "outputId": "f4c956c1-6d0f-4065-fb79-7e3ae1552b73"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " ['августа', 'августа', 'америки', 'арестован', 'атлантический', 'вошёл', 'выйдя', 'гаити', 'город', 'дельты']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Четвертое ДЗ"
      ],
      "metadata": {
        "id": "bNEFM7R-Pq_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"4-я экспедиция (1502—1504 годы). Добившись разрешения продолжать поиски западного пути в Индию, Колумб с 4 судами достиг 15 июня 1502 года острова Мартиника, 30 июля — Гондурасского залива и открыл с 1 августа 1502 по 1 мая 1503 года карибские берега Гондураса, Никарагуа, Коста-Рики и Панамы до залива Ураба. Повернув затем на Север, 25 июня 1503 года потерпел крушение у острова Ямайка; помощь из Санто-Доминго пришла только через год. В Кастилию Христофор Колумб вернулся 7 ноября 1504 года.\"\"\"\n",
        "def counter(text):\n",
        "    text = text.lower()\n",
        "\n",
        "    digits = \"1234567890\"\n",
        "    text = ''.join(char for char in text if char not in digits)\n",
        "\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "\n",
        "    dictionary = {}\n",
        "    for word in text:\n",
        "      if word != \" \":\n",
        "        dictionary[word] = text.count(word)\n",
        "\n",
        "    sorted_dict = sorted(dictionary.items(), key=lambda item: item[1], reverse=True)\n",
        "\n",
        "    return sorted_dict[:5]\n",
        "\n",
        "russian_tokens = counter(sample_text)\n",
        "print(\"Частотный словарь\", russian_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKiYvsUtPI-Y",
        "outputId": "9463f655-dcd9-48a8-ca36-95ec8ecbb26b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Частотный словарь [('а', 40), ('о', 39), ('и', 33), ('р', 24), ('с', 20)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Пятое ДЗ"
      ],
      "metadata": {
        "id": "49sZLufKPtk7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"Открытие Колумбом Америки дало начало Великим географическим открытиям, Колумбовому обмену, трансатлантической работорговле и колониализму, что впоследствии привело Западный мир к господству над всеми остальными.\n",
        "\n",
        "Колумбов обмен привнёс в европейский быт такие вещи как шоколад, табак и другие, а почти безграничные ресурсы из Нового Света сделало Европейские метрополии, в первую очередь Испанию (см. Революция цен), сказочно богатыми.\n",
        "\n",
        "На фоне всего этого произошла Индейская демографическая катастрофа, которая уничтожила почти всех индейцев, а уцелевшие от болезней умерли от принудительных работ.\"\"\"\n",
        "def analyze_text(text):\n",
        "    paragraphs = text.split('\\n\\n')\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for paragraph in paragraphs:\n",
        "        paragraph = paragraph.strip()\n",
        "\n",
        "        sentences = [sentence.strip() for sentence in paragraph.split('.') if sentence.strip()]\n",
        "\n",
        "        num_sentences = len(sentences)\n",
        "        words = paragraph.split()\n",
        "        num_words = len(words)\n",
        "\n",
        "        total_length = sum(len(word) for word in words)\n",
        "        average_length = total_length / num_words if num_words > 0 else 0\n",
        "\n",
        "        results.append({\n",
        "            \"Число предложений\": num_sentences,\n",
        "            \"Количество слов\": num_words,\n",
        "            \"Средняя длина слова\": average_length\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "result = analyze_text(sample_text)\n",
        "for i, res in enumerate(result):\n",
        "    print(f\"Параграф {i + 1}: {res}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ykHHM0FDPtY2",
        "outputId": "b94081ed-cb0d-46ee-9eb5-443c8b745d5f"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Параграф 1: {'Число предложений': 1, 'Количество слов': 24, 'Средняя длина слова': 7.875}\n",
            "Параграф 2: {'Число предложений': 2, 'Количество слов': 32, 'Средняя длина слова': 5.96875}\n",
            "Параграф 3: {'Число предложений': 1, 'Количество слов': 21, 'Средняя длина слова': 6.809523809523809}\n"
          ]
        }
      ]
    }
  ]
}