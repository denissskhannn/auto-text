{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76va5BKKNKcx"
      },
      "source": [
        "**Задание 1: Векторизация текста с использованием Мешка слов (BoW) и TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFPwzZZ4NMIT"
      },
      "source": [
        "1. Создайте корпус из минимум 10 текстовых документов (можно взять новостные статьи, отзывы на товары, статьи по одной тематике).\n",
        "2. Реализуйте предобработку текста, включающую:\n",
        "* Приведение к нижнему регистру\n",
        "* Токенизацию\n",
        "* Удаление стоп-слов и пунктуации\n",
        "* Лемматизацию/стемминг\n",
        "3. Реализуйте модель Мешка слов (BoW) с использованием CountVectorizer из scikit-learn.\n",
        "4. Реализуйте модель TF-IDF с использованием TfidfVectorizer из scikit-learn.\n",
        "5. Найдите 10 самых значимых терминов для каждого документа по обоим подходам и сравните результаты.\n",
        "6. Визуализируйте сходство документов с помощью метрики косинусного расстояния для обоих подходов.\n",
        "7. Прокомментируйте разницу в результатах между BoW и TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7q9vo0RuMxPX",
        "outputId": "729a55d2-ed04-461f-8496-ceec3b4e2f61"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'spacy'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
          ]
        }
      ],
      "source": [
        "!pip install gensim\n",
        "\n",
        "import string\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Загрузка моделей spaCy\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Загрузка стоп-слов\n",
        "nltk.download('stopwords')\n",
        "english_stopwords = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rlE-INI597WL"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "     \"\"\"В Эрмитаже открылась масштабная выставка, посвящённая искусству Древнего Рима. В московском «Гараже» стартовал фестиваль современной уличной культуры. Писательница Мария Степанова стала лауреатом международной литературной премии. В прокат вышла новая экранизация классического романа Достоевского.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"В финале Кубка России по хоккею «Ак Барс» одержал победу над ЦСКА. Российская теннисистка Дарья Касаткина вышла в полуфинал престижного турнира в Дубае. Сборная России по художественной гимнастике завоевала все золотые медали на этапе Кубка мира в Афинах. ФК «Краснодар» одержал важную победу в матче РПЛ, укрепив свои позиции в тройке лидеров.\"\"\",\n",
        "\n",
        "\n",
        " \"\"\"В Великом Новгороде археологи обнаружили уникальную берестяную грамоту XIV века. В ней содержится детальное описание торговой сделки между местным купцом и немецкими гостями. Находка проливает новый свет на экономические связи Новгородской республики с Ганзейским союзом. Учёные уже назвали грамоту сенсационной для изучения средневековой Руси.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"В России утвердили новую стратегию развития системы особо охраняемых природных территорий до 2030 года. Учёные зафиксировали рекордное сокращение площади морского льда в Арктике за всю историю спутниковых наблюдений. В Подмосковье завершили рекультивацию одной из крупнейших свалок «Кучино», на её месте теперь высажен лес. В Сочи начался эксперимент по раздельному сбору органических отходов для последующего компостирования.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"В старинной усадьбе, покинутой десятилетия назад, внезапно загорелись огни в окнах западного флигеля. Местные жители клянутся, что каждую полночь слышат оттуда звуки виолончели, хотя дорогу к дому давно поглотил лес. Вчера утром на пороге обнаружили идеально сложенную стопку писем, адресованных людям, умершим полвека назад. Все конверты были запечатаны чёрным сургучом с оттиском крыла, которого нет ни в одном известном гербе.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"На юге России объявлено экстренное предупреждение из-за аномальной жары, столбики термометров могут подняться выше +40°C. В Москве и области после аномального тепла ожидается резкое похолодание с дождями и мокрым снегом. Ураган \"Дебора\", пришедший с Атлантики, вызвал сильнейшие наводнения в Западной Европе, парализовав транспорт. Синоптики предупреждают, что в центральные регионы на этой неделе придут сильные магнитные бури, что может сказаться на самочувствии метеозависимых людей.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"Известный американский музыкант Мэрилин Мэнсон объявил о возобновлении концертной деятельности и готовит масштабное мировое турне. Ожидается, что тур будет посвящён поддержке его новой студийной пластинки, работа над которой сейчас завершается. После серии судебных процессов и обвинений, певец заявил о возвращении к творчеству. Его фанаты по всему миру с нетерпением ждут анонса дат и городов предстоящих выступлений.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"В Гренландии начато международное расследование после обнаружения следов радиоактивного загрязнения в районе заброшенной военной базы времен Холодной войны. Ученые предполагают, что источником могут быть затонувшие или захороненные радиоизотопные генераторы, оставшиеся от американских или советских объектов. Датские и гренландские власти требуют полной очистки территории, так как таяние льда обнажает потенциально опасные объекты. Результаты расследования могут повлиять на международные соглашения по утилизации военных отходов в Арктике.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"Экономика Китая показала рост выше прогнозов в первом квартале, во многом благодаря росту внутреннего потребления и высокотехнологичному сектору. В Шанхае открылся крупнейший в мире завод по производству аккумуляторов для электромобилей, что укрепило лидерство страны в «зелёной» трансформации. Китайский зонд «Чанъэ-6» успешно стартовал к обратной стороне Луны с целью доставки на Землю уникальных образцов грунта. Власти ужесточили правила использования искусственного интеллекта в сфере финансов для предотвращения рисков и манипуляций на рынке.\"\"\",\n",
        "\n",
        "\n",
        "\"\"\"Запущен интерактивный проект, где юные зрители сами выбирают развитие сюжета спектакля с помощью специальных браслетов. На главной сцене страны прошёл масштабный благотворительный показ для воспитанников детских домов со всей России. Совсем юные актёсы, средний возраст которых — девять лет, представили постановку по классическому произведению, вызвавшую восторг у серьёзных критиков. В региональных театрах начали активно внедрять специальные «тихие часы» с адаптированным звуком и светом для комфорта детей с аутизмом.\"\"\"\n",
        "\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJJHyRxO7sbp",
        "outputId": "6bd02bc4-cfbb-448c-e1c6-47784876300d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'corpus' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 16\u001b[0m\n\u001b[0;32m     14\u001b[0m       normal_text\u001b[38;5;241m.\u001b[39mappend(text)\n\u001b[0;32m     15\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m normal_text\n\u001b[1;32m---> 16\u001b[0m documents \u001b[38;5;241m=\u001b[39m normalize_text(\u001b[43mcorpus\u001b[49m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(documents)\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПредобработанные документы:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'corpus' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "def normalize_text(text):\n",
        "  normal_text = []\n",
        "  for text in corpus:\n",
        "      text = text.lower()\n",
        "      text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "      text = text.replace(\"  \", \" \")\n",
        "      tokens = text.split()\n",
        "      filtered_tokens = [token for token in tokens if token not in english_stopwords]\n",
        "      text = ' '.join(filtered_tokens)\n",
        "\n",
        "      doc = nlp_en(text)\n",
        "      lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "      text = ' '.join(lemmas)\n",
        "      normal_text.append(text)\n",
        "  return normal_text\n",
        "documents = normalize_text(corpus)\n",
        "print(documents)\n",
        "\n",
        "print(\"Предобработанные документы:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"Документ {i}: {doc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y6WGljn8QWdZ",
        "outputId": "d19efdbf-ea44-4d9d-d8ae-92009d9d6ee6"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'CountVectorizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Создание векторизатора\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m count_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m()\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Обучение векторизатора и преобразование документов\u001b[39;00m\n\u001b[0;32m      5\u001b[0m bow_matrix \u001b[38;5;241m=\u001b[39m count_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(documents)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'CountVectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "# Создание векторизатора\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Обучение векторизатора и преобразование документов\n",
        "bow_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Получение списка фичей (слов)\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Преобразование разреженной матрицы в плотную для наглядности\n",
        "bow_df = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=feature_names,\n",
        "    index=[f'Документ {i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "# Вывод матрицы Bag of Words\n",
        "print(bow_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lb-J5IgnQjGk",
        "outputId": "43256c2d-38f4-4ed9-f959-6ede6f5d5d43"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'TfidfVectorizer' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m \u001b[43mTfidfVectorizer\u001b[49m()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Обучение векторизатора и преобразование документов\u001b[39;00m\n\u001b[0;32m      4\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(documents)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'TfidfVectorizer' is not defined"
          ]
        }
      ],
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Обучение векторизатора и преобразование документов\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Преобразование в DataFrame\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
        "    index=[f'Документ {i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "print(tfidf_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqsuP0hxREOh",
        "outputId": "31b797b4-a9eb-4b60-d14b-838a1c949c44"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'np' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bow_sum \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39msum(bow_matrix\u001b[38;5;241m.\u001b[39mtoarray(), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m      4\u001b[0m word_bow_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(feature_names, bow_sum))\n\u001b[0;32m      6\u001b[0m top_bow_words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(word_bow_dict\u001b[38;5;241m.\u001b[39mitems(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[:\u001b[38;5;241m10\u001b[39m]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ],
      "source": [
        "bow_sum = np.sum(bow_matrix.toarray(), axis=0)\n",
        "\n",
        "\n",
        "word_bow_dict = dict(zip(feature_names, bow_sum))\n",
        "\n",
        "top_bow_words = sorted(word_bow_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"=== Топ-10 слов по всей коллекции ===\\n\")\n",
        "print(\"Топ-10 слов по BOW (самые частые):\")\n",
        "for word, count in top_bow_words:\n",
        "    print(f\"- {word}: {count} раз\")\n",
        "\n",
        "tfidf_sum = np.sum(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "word_tfidf_dict = dict(zip(feature_names, tfidf_sum))\n",
        "\n",
        "top_tfidf_words = sorted(word_tfidf_dict.items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"\\nТоп-10 слов по TF-IDF (с наибольшим весом):\")\n",
        "for word, score in top_tfidf_words:\n",
        "    print(f\"- {word}: {score:.4f}\")\n",
        "\n",
        "print(\"\\nСравнение топ-10 слов:\")\n",
        "bow_words = [word for word, _ in top_bow_words]\n",
        "tfidf_words = [word for word, _ in top_tfidf_words]\n",
        "\n",
        "common_words = set(bow_words) & set(tfidf_words)\n",
        "bow_only = set(bow_words) - common_words\n",
        "tfidf_only = set(tfidf_words) - common_words\n",
        "\n",
        "if common_words:\n",
        "    print(f\"Общие слова в обоих топ-10: {', '.join(common_words)}\")\n",
        "if bow_only:\n",
        "    print(f\"Только в BOW топ-10: {', '.join(bow_only)}\")\n",
        "if tfidf_only:\n",
        "    print(f\"Только в TF-IDF топ-10: {', '.join(tfidf_only)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mZ9K9wHoSZnW",
        "outputId": "323ff3d3-a81f-4597-c197-6e8b427f7b1d"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'documents' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 13\u001b[0m\n\u001b[0;32m      9\u001b[0m     tokens \u001b[38;5;241m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokens\n\u001b[1;32m---> 13\u001b[0m processed_corpus \u001b[38;5;241m=\u001b[39m [preprocess_text(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocuments\u001b[49m]\n\u001b[0;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m Word2Vec(\n\u001b[0;32m     16\u001b[0m     sentences\u001b[38;5;241m=\u001b[39mprocessed_corpus,\n\u001b[0;32m     17\u001b[0m     vector_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m,  \u001b[38;5;66;03m# Размерность векторов (обычно от 100 до 300)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     workers \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m,  \u001b[38;5;66;03m# Количество потоков для параллельного обучения\u001b[39;00m\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtext_to_vector\u001b[39m(text, model):\n\u001b[0;32m     33\u001b[0m \n\u001b[0;32m     34\u001b[0m     \u001b[38;5;66;03m# Предобрабатываем предложение\u001b[39;00m\n",
            "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    # Преобразуем в нижний регистр\n",
        "    text = text.lower()\n",
        "\n",
        "    # Удаляем пунктуацию (оставляем только буквы и пробелы)\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Токенизация (разбиение на слова)\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "processed_corpus = [preprocess_text(text) for text in documents]\n",
        "\n",
        "model = Word2Vec(\n",
        "    sentences=processed_corpus,\n",
        "    vector_size = 100,  # Размерность векторов (обычно от 100 до 300)\n",
        "    window = 5,  # Размер контекстного окна (сколько слов слева и справа учитывать)\n",
        "    min_count = 1,  # Минимальная частота слова для включения в модель\n",
        "    sg = 1,  # 1 = Skip-gram, 0 = CBOW\n",
        "    hs = 0,  # 0 = использовать negative sampling, 1 = иерархический softmax\n",
        "    negative = 5,  # Количество \"негативных\" семплов для negative sampling\n",
        "    ns_exponent = 0.75,  # Экспонента для отрицательного сэмплирования\n",
        "    cbow_mean = 1,  # Для CBOW: 1 = использовать среднее, 0 = сумму\n",
        "    alpha = 0.025,  # Начальная скорость обучения\n",
        "    min_alpha = 0.0001,  # Минимальная скорость обучения\n",
        "    seed = 42,  # Для воспроизводимости результатов\n",
        "    workers = 4,  # Количество потоков для параллельного обучения\n",
        ")\n",
        "\n",
        "\n",
        "def text_to_vector(text, model):\n",
        "\n",
        "    # Предобрабатываем предложение\n",
        "    words = preprocess_text(text)\n",
        "\n",
        "    # Отбираем только слова, которые есть в модели\n",
        "    words = [word for word in words if word in model.wv]\n",
        "\n",
        "    if not words:\n",
        "        # Если не нашлось ни одного известного слова, вернем нулевой вектор\n",
        "        return np.zeros(model.vector_size)\n",
        "\n",
        "    # Получаем векторы слов\n",
        "    word_vectors = [model.wv[word] for word in words]\n",
        "\n",
        "    # Возвращаем средний вектор\n",
        "    return np.mean(word_vectors, axis=0)\n",
        "\n",
        "# Создаем векторные представления для всех предложений корпуса\n",
        "text_vectors = [text_to_vector(text, model) for text in documents]\n",
        "\n",
        "def find_similar_sentences(query, corpus, model, top_n=10):\n",
        "    # Преобразуем запрос в вектор\n",
        "    query_vector = text_to_vector(query, model)\n",
        "    query_norm = np.linalg.norm(query_vector)\n",
        "    if query_norm == 0:\n",
        "    # В запросе не оказалось слов из словаря модели\n",
        "        return []\n",
        "\n",
        "    similarities = []\n",
        "    for i, text_vector in enumerate(text_vectors):\n",
        "        denom = query_norm * np.linalg.norm(text_vector)\n",
        "        if denom == 0:\n",
        "            similarity = 0.0\n",
        "        else:\n",
        "            similarity = float(np.dot(query_vector, text_vector) / denom)\n",
        "\n",
        "        similarities.append((documents[i], similarity))\n",
        "\n",
        "    # Сортируем по убыванию сходства\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Возвращаем top_n результатов\n",
        "    return similarities[:top_n]\n",
        "\n",
        "print(\"\\n=== Поиск похожих предложений ===\")\n",
        "for query in corpus:\n",
        "    # Чтобы запрос не был слишком громозким\n",
        "    tokens = query.split()\n",
        "    query = \" \".join(tokens)\n",
        "    print(f\"\\nЗапрос: {query}\")\n",
        "    similar = find_similar_sentences(query, documents, model)\n",
        "    for i, (text, score) in enumerate(similar, 1):\n",
        "        print(f\"{i}. {text[:20]} (сходство: {score:.4f})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kQ-M8a-jU1xN",
        "outputId": "cee1e73d-cb7b-4aa2-b42b-68899eb5f58f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'documents' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m docs_clean \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(preprocess_text(d)) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdocuments\u001b[49m]\n\u001b[0;32m      2\u001b[0m labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mДок \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(docs_clean))]\n\u001b[0;32m      4\u001b[0m bow \u001b[38;5;241m=\u001b[39m CountVectorizer()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'documents' is not defined"
          ]
        }
      ],
      "source": [
        "docs_clean = [\" \".join(preprocess_text(d)) for d in documents]\n",
        "labels = [f\"Док {i+1}\" for i in range(len(docs_clean))]\n",
        "\n",
        "bow = CountVectorizer()\n",
        "X_bow = bow.fit_transform(docs_clean)\n",
        "\n",
        "tfidf = TfidfVectorizer()\n",
        "X_tfidf = tfidf.fit_transform(docs_clean)\n",
        "from sklearn.manifold import MDS\n",
        "from sklearn.metrics.pairwise import cosine_distances\n",
        "\n",
        "def plot_mds(X, labels, title):\n",
        "    D = cosine_distances(X)\n",
        "    coords = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=42).fit_transform(D)\n",
        "\n",
        "    plt.figure(figsize=(7,6))\n",
        "    plt.scatter(coords[:,0], coords[:,1])\n",
        "    for i, lab in enumerate(labels):\n",
        "        plt.text(coords[i,0], coords[i,1], \" \" + lab, va=\"center\")\n",
        "    plt.title(title)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_mds(X_bow, labels, \"BoW косинусное расстояние\")\n",
        "plot_mds(X_tfidf, labels, \"TF-IDF косинусное расстояние\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCUQahT6VNqV"
      },
      "source": [
        "Разница в использовании двух методах есть, BoW часто поднимает самые частотные слова документа, TF-IDF лучше выделяет уникальную лексику, отодвигая более общие слова"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7D5CC2NguY"
      },
      "source": [
        "**Задание 2: Морфологическая разметка текста**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx3HfIUpNxj1"
      },
      "source": [
        "1. Возьмите отрывок текста (минимум 300 слов) на русском и английском языке.\n",
        "2. Для русского языка используйте PyMorphy2 или PyMorphy3, для английского - NLTK или SpaCy для проведения морфологического анализа.\n",
        "3. Выполните следующие операции:\n",
        "* Определите части речи для каждого слова в тексте\n",
        "* Для существительных определите падеж, род и число\n",
        "* Для глаголов определите время, лицо и число\n",
        "* Создайте частотный словарь частей речи в тексте\n",
        "4. Разработайте функцию, которая будет автоматически изменять текст, заменяя все существительные на их форму множественного числа (где возможно).\n",
        "5. Результаты морфологического анализа должны быть представлены в виде таблицы.\n",
        "6. Оцените и прокомментируйте точность определения морфологических характеристик"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fi83NngNNjJ0",
        "outputId": "9b8105d4-3b7a-429f-d0b5-71d4c0ed2572"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "'pip' is not recognized as an internal or external command,\n",
            "operable program or batch file.\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pymorphy3'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install pymorphy3 -q\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpymorphy3\u001b[39;00m\n\u001b[0;32m      4\u001b[0m morph \u001b[38;5;241m=\u001b[39m pymorphy3\u001b[38;5;241m.\u001b[39mMorphAnalyzer(lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mru\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mcollections\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Counter\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pymorphy3'"
          ]
        }
      ],
      "source": [
        "!pip install pymorphy3 -q\n",
        "\n",
        "import pymorphy3\n",
        "morph = pymorphy3.MorphAnalyzer(lang='ru')\n",
        "from collections import Counter\n",
        "\n",
        "text_ru = \"\"\"Жители некогда ничем не примечательного городка Ветвуд-Рок в глубинке столкнулись с феноменом, который из местной страшилки превратился в невыносимую повседневность, а теперь — в источник новой, более глубокой тревоги. На протяжении последних трёх лет здесь фиксировались так называемые «тихие звонки»: телефоны — стационарные и мобильные — принимали входящие вызовы с номеров, принадлежащих давно умершим людям. На том конце провода обычно слышалось лишь прерывистое дыхание, шум, похожий на далёкий ветер в трубе, или шёпот, не складывающийся в слова. Психиатры списывали это на массовую истерию, следователи — на чью-то мрачную шутку, а местные жители молча ставили свечки в церкви и шептались о «порубежье», где стирается грань между мирами.\n",
        "Однако на прошлой неделе звонки внезапно и разом прекратились. Тишина, воцарившаяся в домах, оказалась гуще и невыносимее любого шёпота. «Раньше мы боялись, когда телефон звонил ночью, — делится с нашим корреспондентом пенсионерка Мэлори Вайт, проживающая на улице Уолл. — Теперь мы боимся, когда он молчит. Эта тишина… она не пустая. Она внимательная. Как будто тот, кто звонил, теперь просто стоит за спиной и смотрит, не нуждаясь больше в проводах».\n",
        "Параллельно с этим в городке участились случаи сомнамбулизма. Люди в состоянии глубокого сна выходят из домов и направляются к старому заброшенному кладбищу на окраине, где в XIX веке была часовня, давно ушедшая под землю из-за провала грунта. Они ходят кругами вокруг заросшего рва, а утром просыпаются в своих кроватях с землёй под ногтями и полной амнезией ночных происшествий. Местный священник отец Алексей, обычно скептически относившийся к «бабьим сказкам», отказался комментировать ситуацию, лишь освятив заново стены своего храма.\n",
        "Самое тревожное — изменение поведения домашних животных. Кошки, первые обычно чувствующие незримое, не прячутся, а, наоборот, замирают, уставившись в одну точку в пустой комнате, а затем начинают вилять хвостами, как собаки, будто приветствуя кого-то невидимого. Собаки же погрузились в апатию и отказываются выходить на улицу после заката, жалобно поскуливая у входных дверей.\n",
        "Власти по-прежнему отрицают всё, кроме «серии странных совпадений и психологического заражения». Но группа энтузиастов-исследователей паранормального, прибывшая в городок, уже зафиксировала аномально низкий уровень электромагнитного поля в районе старого кладбища и странные помехи на всех аудиозаписях, сделанных после захода солнца — едва уловимый ритмичный стук, похожий на удары по гробовой крышке. Уэтвуд-Рок погрузился в гнетущее ожидание. Жители понимают, что тишина — это не конец истории. Это затишье перед финальной главой, которую никто не хочет прочесть, но которую все, по зову плоти и крови, будут вынуждены услышать. Пограничье, похоже, не просто открылось — оно начало медленно, но неотвратимо поглощать реальность.\"\"\"\n",
        "\n",
        "text_en = \"\"\"Residents of the once-unremarkable town of Wetwood Rock, in the backcountry, have encountered a phenomenon that has evolved from a local ghost story into an unbearable daily reality, and now—into a source of new, deeper anxiety. For the past three years, so-called \"silent calls\" have been recorded here: landlines and mobile phones would receive incoming calls from numbers belonging to long-deceased people. On the other end of the line, one would usually hear only ragged breathing, a noise resembling distant wind in a pipe, or a whisper that never formed into words. Psychiatrists dismissed it as mass hysteria, investigators—as someone's morbid prank, while the locals silently lit candles in church and whispered about a \"borderland\" where the line between worlds is blurred.\n",
        "However, last week the calls suddenly and completely ceased. The silence that settled over the homes turned out to be thicker and more unbearable than any whisper. \"We used to be afraid when the phone rang at night,\" shares pensioner Mallory White, who lives on Wall Street, with our correspondent. \"Now we're afraid when it's silent. This silence… it's not empty. It's attentive. As if the one who was calling is now simply standing behind you, watching, no longer needing wires.\"\n",
        "Parallel to this, cases of somnambulism have increased in the town. People in a deep sleep state leave their homes and head to the old abandoned cemetery on the outskirts, where in the 19th century there stood a chapel, long since sunk into the ground due to a land collapse. They walk in circles around the overgrown ditch, and in the morning wake up in their beds with dirt under their fingernails and complete amnesia of the night's events. The local priest, Father Alexey, usually skeptical of \"old wives' tales,\" refused to comment on the situation, having only reconsecrated the walls of his church.\n",
        "The most alarming thing is the change in the behavior of pets. Cats, usually the first to sense the unseen, no longer hide but, on the contrary, freeze, staring at one point in an empty room, and then begin to wag their tails like dogs, as if greeting someone invisible. Dogs, meanwhile, have sunk into apathy and refuse to go outside after sunset, whimpering pitifully at the front doors.\n",
        "The authorities still deny everything, except for \"a series of strange coincidences and psychological contagion.\" But a group of paranormal research enthusiasts who arrived in the town has already recorded an abnormally low level of the electromagnetic field in the area of the old cemetery and strange interference on all audio recordings made after sunset—a faint, rhythmic knocking, similar to blows against a coffin lid.\n",
        "Wetwood Rock has sunk into oppressive anticipation. The residents understand that the silence is not the end of the story. It is the calm before the final chapter, which no one wants to read, but which everyone, by the call of flesh and blood, will be forced to hear. The borderland, it seems, did not simply open—it has begun to slowly but inexorably consume reality.\"\"\"\n",
        "\n",
        "\n",
        "def tokenize_ru(text):\n",
        "\n",
        "    words = re.findall(r'\\b[\\w-]+\\b', text)\n",
        "    return words\n",
        "\n",
        "tokens_ru = tokenize_ru(text_ru)\n",
        "\n",
        "analysis_results = []\n",
        "pos_list = []\n",
        "\n",
        "for word_text in tokens_ru:\n",
        "    parsed_word = morph.parse(word_text)[0]\n",
        "\n",
        "    pos = parsed_word.tag.POS if parsed_word.tag.POS else 'UNKN'\n",
        "    pos_list.append(pos)\n",
        "\n",
        "\n",
        "    case, gender, number, tense, person = None, None, None, None, None\n",
        "\n",
        "    if pos == 'NOUN':\n",
        "        case = parsed_word.tag.case\n",
        "        gender = parsed_word.tag.gender\n",
        "        number = parsed_word.tag.number\n",
        "    elif pos == 'VERB' or pos == 'INFN':\n",
        "        tense = parsed_word.tag.tense\n",
        "        person = parsed_word.tag.person\n",
        "        number = parsed_word.tag.number\n",
        "\n",
        "    analysis_results.append({\n",
        "        'Word': word_text,\n",
        "        'POS': pos,\n",
        "        'Case': case,\n",
        "        'Gender': gender,\n",
        "        'Number': number,\n",
        "        'Tense': tense,\n",
        "        'Person': person\n",
        "    })\n",
        "\n",
        "\n",
        "morph_df_ru = pd.DataFrame(analysis_results)\n",
        "print(\"\\n--- Морфологический анализ русского текста ---\")\n",
        "print(morph_df_ru)\n",
        "\n",
        "\n",
        "pos_count = Counter(pos_list)\n",
        "print(\"\\n--- Частота частей речи (Русский) ---\")\n",
        "for pos, count in pos_count.most_common():\n",
        "    print(f\"{pos:10}: {count}\")\n",
        "\n",
        "\n",
        "def pluralize_nouns_ru(text):\n",
        "    words = tokenize_ru(text)\n",
        "    result_words = []\n",
        "    for w in words:\n",
        "        parsed = morph.parse(w.lower())[0] # Parse in lowercase for better matching with dictionary\n",
        "        if 'NOUN' in parsed.tag:\n",
        "            inflected = parsed.inflect({'plur'})\n",
        "            if inflected:\n",
        "                result_words.append(inflected.word)\n",
        "            else:\n",
        "                result_words.append(w)\n",
        "        else:\n",
        "            result_words.append(w)\n",
        "    return ' '.join(result_words)\n",
        "\n",
        "print(\"\\n--- Преобразование существительных в множественное число ---\")\n",
        "print(\"Оригинальный текст: \", text_ru)\n",
        "print(\"Текст с мн.ч. существительных: \", pluralize_nouns_ru(text_ru))\n",
        "\n",
        "print(\"Russian morphological analysis and noun pluralization completed.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5boO8JwhECb",
        "outputId": "edb76a1e-87e3-41a7-f492-788e1de79e1b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nlp_en' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m doc_en \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_en\u001b[49m(text_en)\n\u001b[0;32m      3\u001b[0m analysis_results_en \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      4\u001b[0m pos_list_en \u001b[38;5;241m=\u001b[39m []\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_en' is not defined"
          ]
        }
      ],
      "source": [
        "doc_en = nlp_en(text_en)\n",
        "\n",
        "analysis_results_en = []\n",
        "pos_list_en = []\n",
        "\n",
        "for token in doc_en:\n",
        "    if token.is_punct or token.is_space:\n",
        "        continue\n",
        "\n",
        "    pos = token.pos_\n",
        "    pos_list_en.append(pos)\n",
        "\n",
        "\n",
        "    number, tense, person = None, None, None\n",
        "\n",
        "    if pos == 'NOUN':\n",
        "        if 'Sing' in token.tag_:\n",
        "            number = 'SING'\n",
        "        elif 'Plur' in token.tag_:\n",
        "            number = 'PLUR'\n",
        "\n",
        "    elif pos == 'VERB':\n",
        "\n",
        "        tense = token.morph.get('Tense')\n",
        "        person = token.morph.get('Person')\n",
        "\n",
        "        number = token.morph.get('Number')\n",
        "\n",
        "\n",
        "        tense = ', '.join(tense) if tense else None\n",
        "        person = ', '.join(person) if person else None\n",
        "        number = ', '.join(number) if number else None\n",
        "\n",
        "    analysis_results_en.append({\n",
        "        'Word': token.text,\n",
        "        'POS': pos,\n",
        "        'Number': number,\n",
        "        'Tense': tense,\n",
        "        'Person': person\n",
        "    })\n",
        "\n",
        "morph_df_en = pd.DataFrame(analysis_results_en)\n",
        "print(\"--- Морфологический анализ английского текста ---\")\n",
        "print(morph_df_en.to_string())\n",
        "\n",
        "pos_count_en = Counter(pos_list_en)\n",
        "print(\"\\n--- Частота частей речи (Английский) ---\")\n",
        "for pos, count in pos_count_en.most_common():\n",
        "    print(f\"{pos:10}: {count}\")\n",
        "\n",
        "def pluralize_nouns_en(text):\n",
        "    doc = nlp_en(text)\n",
        "    result_words = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'NOUN' and 'Sing' in token.tag_:\n",
        "            plural_form = token.lemma_ + 's'\n",
        "            result_words.append(plural_form)\n",
        "        else:\n",
        "            result_words.append(token.text)\n",
        "    return ' '.join(result_words)\n",
        "\n",
        "print(\"\\n--- Преобразование существительных в множественное число (Английский) ---\")\n",
        "print(\"Оригинальный текст: \", text_en[:200] + '...')\n",
        "pluralized_text = pluralize_nouns_en(text_en)\n",
        "print(\"Текст с мн.ч. существительных: \", pluralized_text[:200] + '...')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azGFuWTkOCnX"
      },
      "source": [
        "**Задание 3: Синтаксический анализ предложений**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWWlpPkjOFpg"
      },
      "source": [
        "1. Возьмите 2 простых и 3 сложных предложений на русском и английском языке (всего 10 предложений).\n",
        "2. Используйте SpaCy для построения синтаксических деревьев этих предложений.\n",
        "3. Для каждого предложения:\n",
        "* Визуализируйте синтаксическое дерево\n",
        "* Выделите все подлежащие и сказуемые\n",
        "* Найдите все пары слов, связанные отношением определения (прилагательное-существительное)\n",
        "4. Разработайте функцию для извлечения всех объектных и субъектных отношений из предложения в формате (субъект, предикат, объект).\n",
        "5. Объясните, какие трудности возникают при синтаксическом анализе сложных предложений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHi7VumCOcqr",
        "outputId": "21ca65e9-d4eb-442f-cf3f-c9e308bd4622"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ru-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.8.0/ru_core_news_md-3.8.0-py3-none-any.whl (41.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 MB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-md==3.8.0)\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m109.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-md\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-md-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ],
      "source": [
        "# ваш код\n",
        "\n",
        "!python -m spacy download ru_core_news_md\n",
        "!python -m spacy download en_core_web_md\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "YnKtFJfPRH4x"
      },
      "outputs": [],
      "source": [
        "russian_sentences = [\n",
        "    \"Я видел его.\",\n",
        "    \"Они не знают.\",\n",
        "    \"Тихий городок, в котором остановились путники, загудел и наполнился в миг.\",\n",
        "    \"Если ты не увидишь меня завтра, я приду к тебе ночью.\",\n",
        "    \"Что делаеть, если не уверен в правильности лабораторной работы?.\"\n",
        "]\n",
        "english_sentences = [\n",
        "    \"I don't know\",\n",
        "    \"She is scary.\",\n",
        "    \"I have never seen such an impulsive girl that sacrifices everything she has.\",\n",
        "    \"Don't you think it's not applicable to our situation?\",\n",
        "    \"Having you as a friend is the best thing in my life.\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "iXLAre-BSsrM"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'spacy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp_ru \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mru_core_news_md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m nlp_en \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_md\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mspacy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m displacy\n",
            "\u001b[1;31mNameError\u001b[0m: name 'spacy' is not defined"
          ]
        }
      ],
      "source": [
        "nlp_ru = spacy.load(\"ru_core_news_md\")\n",
        "nlp_en = spacy.load(\"en_core_web_md\")\n",
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ICvKBxK2TJG0",
        "outputId": "507ce31c-7e2d-4d39-bdb6-80181290a57f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Визуализация дерева зависимостей для русского языка\n",
            "\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'nlp_ru' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mВизуализация дерева зависимостей для русского языка\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m russian_sentences:\n\u001b[1;32m----> 3\u001b[0m     doc_ru \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_ru\u001b[49m(sentence)\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПредложение \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrussian_sentences\u001b[38;5;241m.\u001b[39mindex(sentence)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     displacy\u001b[38;5;241m.\u001b[39mrender(doc_ru, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m\"\u001b[39m, jupyter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompact\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_ru' is not defined"
          ]
        }
      ],
      "source": [
        "print('Визуализация дерева зависимостей для русского языка\\n')\n",
        "for sentence in russian_sentences:\n",
        "    doc_ru = nlp_ru(sentence)\n",
        "\n",
        "    print(f\"Предложение {russian_sentences.index(sentence) + 1}\")\n",
        "    displacy.render(doc_ru, style=\"dep\", jupyter=True, options={\"distance\": 100, \"compact\": True})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mZojY-D3TYgZ",
        "outputId": "f5b72f1f-6316-4c39-c010-ef2a27da400b"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nlp_en' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m english_sentences:\n\u001b[1;32m----> 2\u001b[0m     doc_en \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_en\u001b[49m(sentence)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentence \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menglish_sentences\u001b[38;5;241m.\u001b[39mindex(sentence)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m     displacy\u001b[38;5;241m.\u001b[39mrender(doc_en, style\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdep\u001b[39m\u001b[38;5;124m\"\u001b[39m, jupyter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistance\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompact\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m})\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_en' is not defined"
          ]
        }
      ],
      "source": [
        "for sentence in english_sentences:\n",
        "    doc_en = nlp_en(sentence)\n",
        "    print(f\"Sentence {english_sentences.index(sentence) + 1}\")\n",
        "    displacy.render(doc_en, style=\"dep\", jupyter=True, options={\"distance\": 100, \"compact\": True})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "zGMJX2UpTmzo"
      },
      "outputs": [],
      "source": [
        "def find_subj_pred_rus(doc):\n",
        "    root = None\n",
        "    pred_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"ROOT\":\n",
        "            root = token\n",
        "            pred_tokens.append(token)\n",
        "            break\n",
        "\n",
        "    if not root:\n",
        "        return [], []\n",
        "\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for t in doc:\n",
        "            if t.dep_ == \"conj\" and t.head in pred_tokens and t.pos_ in {\"VERB\", \"AUX\"}:\n",
        "                if t not in pred_tokens:\n",
        "                    pred_tokens.append(t)\n",
        "                    changed = True\n",
        "\n",
        "    def pred_text(p):\n",
        "        parts = {p}\n",
        "        for ch in p.children:\n",
        "            if ch.dep_ in {\"aux\", \"auxpass\", \"cop\", \"neg\"}:\n",
        "                parts.add(ch)\n",
        "            if ch.dep_ == \"xcomp\" and ch.pos_ in {\"VERB\", \"AUX\"}:\n",
        "                parts.add(ch)\n",
        "        return \" \".join(tok.text for tok in sorted(parts, key=lambda x: x.i))\n",
        "\n",
        "    def subjects(p):\n",
        "        subs = [ch for ch in p.children if ch.dep_ in {\"nsubj\", \"nsubjpass\", \"nsubj:pass\"}]\n",
        "        if subs:\n",
        "            return subs\n",
        "        if p.dep_ == \"conj\":\n",
        "            return subjects(p.head)\n",
        "        return []\n",
        "\n",
        "    all_subj = []\n",
        "    all_pred = []\n",
        "    for p in sorted(pred_tokens, key=lambda x: x.i):\n",
        "        all_pred.append(pred_text(p))\n",
        "        all_subj.extend([s.text for s in subjects(p)])\n",
        "\n",
        "    return all_subj, all_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DVH1phUMTsaX",
        "outputId": "455eeb8b-fb73-49da-8b6d-13c9ba7ca1f5"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nlp_ru' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m russian_sentences:\n\u001b[1;32m----> 2\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_ru\u001b[49m(text)\n\u001b[0;32m      3\u001b[0m     subject, predicate \u001b[38;5;241m=\u001b[39m find_subj_pred_rus(doc)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПредложение: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_ru' is not defined"
          ]
        }
      ],
      "source": [
        "for text in russian_sentences:\n",
        "    doc = nlp_ru(text)\n",
        "    subject, predicate = find_subj_pred_rus(doc)\n",
        "    print(f\"Предложение: {text}\")\n",
        "    print(f\"Подлежащее: {subject}, Сказуемое: {predicate}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7DNf2od5T0pV"
      },
      "outputs": [],
      "source": [
        "def find_subj_pred_eng(doc):\n",
        "    subj = []\n",
        "    pred_tokens = []\n",
        "    root = None\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"ROOT\":\n",
        "            root = token\n",
        "            pred_tokens.append(token)\n",
        "            break\n",
        "    if not root:\n",
        "        return [], []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in [\"aux\", \"auxpass\"] and token.head in [root] + [t for t in pred_tokens]:\n",
        "            pred_tokens.append(token)\n",
        "    pred = [doc[i].text for i in sorted([t.i for t in pred_tokens])]\n",
        "    for token in doc:\n",
        "        if token.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
        "            current = token\n",
        "            while current.head != current:\n",
        "                if current.head in pred_tokens or current.head == root:\n",
        "                    subj.append(token.text)\n",
        "                    break\n",
        "                current = current.head\n",
        "    if not subj:\n",
        "        for token in doc:\n",
        "            if token.dep_ in [\"nsubj\", \"nsubj:pass\"]:\n",
        "                subj.append(token.text)\n",
        "    return subj, pred"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDy7qo8sT1rB",
        "outputId": "e47c4548-8ccc-4474-d6e9-9f7eacf53f39"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nlp_en' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[15], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m english_sentences:\n\u001b[1;32m----> 2\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_en\u001b[49m(text)\n\u001b[0;32m      3\u001b[0m     subject, predicate \u001b[38;5;241m=\u001b[39m find_subj_pred_eng(doc)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПредложение: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_en' is not defined"
          ]
        }
      ],
      "source": [
        "for text in english_sentences:\n",
        "    doc = nlp_en(text)\n",
        "    subject, predicate = find_subj_pred_eng(doc)\n",
        "    print(f\"Предложение: {text}\")\n",
        "    print(f\"Подлежащее: {subject}, Сказуемое: {predicate}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sKAINM7HT9X2",
        "outputId": "e4722754-df24-421f-946d-8ae72f14d308"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Поиск прилагательное-существительное в русских предложениях:\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'nlp_ru' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[16], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПоиск прилагательное-существительное в русских предложениях:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m russian_sentences:\n\u001b[1;32m----> 3\u001b[0m     doc_ru \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_ru\u001b[49m(sentence)\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc_ru:\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mdep_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mamod\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_ru' is not defined"
          ]
        }
      ],
      "source": [
        "print(\"Поиск прилагательное-существительное в русских предложениях:\")\n",
        "for sentence in russian_sentences:\n",
        "    doc_ru = nlp_ru(sentence)\n",
        "    for token in doc_ru:\n",
        "        if token.dep_ == \"amod\":\n",
        "            print(f\"Найден объект: {token.text} — зависит от существительного: {token.head.text}\")\n",
        "\n",
        "print(\"\\nПоиск прилагательное-существительное в английских предложениях:\")\n",
        "for sentence in english_sentences:\n",
        "    doc_en = nlp_en(sentence)\n",
        "    for token in doc_en:\n",
        "        if token.dep_ == \"amod\":\n",
        "            print(f\"Найден объект: {token.text} — зависит от существительного: {token.head.text}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gnn4UUbBUFqd",
        "outputId": "fc327b65-1624-46b0-951c-d74bfc6790a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Примеры объектных и субъектных отношений из русских предложений:\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'nlp_ru' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mПримеры объектных и субъектных отношений из русских предложений:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m russian_sentences:\n\u001b[1;32m---> 23\u001b[0m     \u001b[43mfind_SVO\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrus\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mПримеры объектных и субъектных отношений из английских предложений:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m english_sentences:\n",
            "Cell \u001b[1;32mIn[17], line 3\u001b[0m, in \u001b[0;36mfind_SVO\u001b[1;34m(sentence, lang)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mfind_SVO\u001b[39m(sentence, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m lang \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrus\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m----> 3\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_ru\u001b[49m(sentence)\n\u001b[0;32m      4\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m doc:\n\u001b[0;32m      5\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mdep_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnsubj\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_ru' is not defined"
          ]
        }
      ],
      "source": [
        "def find_SVO(sentence, lang='en'):\n",
        "    if lang == 'rus':\n",
        "        doc = nlp_ru(sentence)\n",
        "        for token in doc:\n",
        "            if token.dep_ == \"nsubj\":\n",
        "                subj = token.text\n",
        "            if token.dep_ == \"obj\":\n",
        "                obj = token.text\n",
        "                predicate = token.head.text\n",
        "                print(f\"{subj} {predicate} {obj} \")\n",
        "    else:\n",
        "        doc = nlp_en(sentence)\n",
        "        for token in doc:\n",
        "            if token.dep_ in {\"nsubj\", \"nsubjpass\", \"csubj\", \"csubjpass\"}:\n",
        "                subj = token.text\n",
        "                predicate = token.head.text\n",
        "            if token.dep_ in {\"dobj\", \"iobj\", \"obj\"}:\n",
        "                obj = token.text\n",
        "                print(f\"{subj} {predicate} {obj}\")\n",
        "\n",
        "print(\"Примеры объектных и субъектных отношений из русских предложений:\")\n",
        "for sentence in russian_sentences:\n",
        "    find_SVO(sentence, 'rus')\n",
        "print(\"\\nПримеры объектных и субъектных отношений из английских предложений:\")\n",
        "for sentence in english_sentences:\n",
        "    find_SVO(sentence, 'en')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UeWX_jKIUObK"
      },
      "source": [
        "Одна из трудностей заключалась в том, что в русском языке подлежащее может быть выражено разными частями речи. Кроме того, предложение может и не иметь подлежащего вовсе, но при этом составлять цельное предложение и оставаться самостоятельной частью сложного предложения. Создать правила и условия для каждого случая достаточно трудная и местами возможно невыполнимая задача для SpaCY. В английском языке тоже есть свои проблемы. Сложные предложения могут скрывать отношения между подлежащим и сказуемым. Без дополнительных функций или условий сложно правильно определить предлоги и артикли. Но в целом результат получился хорошим."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz9fITygOcK7"
      },
      "source": [
        "**Задание 4: Распознавание именованных сущностей (Named Entity Recognition)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bBTyWvAOojo"
      },
      "source": [
        "1. Подготовьте корпус из 10 новостных текстов, содержащий различные типы именованных сущностей (имена людей, организации, географические названия, даты и т.д.) на английском или русском языке.\n",
        "2. Используйте SpaCy для автоматического распознавания именованных сущностей.\n",
        "3. Реализуйте свой простой метод для распознавания имен людей и географических названий с помощью регулярных выражений и словарей.\n",
        "4. Сравните результаты работы SpaCy и вашего метода:\n",
        "* Рассчитайте точность (precision), полноту (recall) и F1-меру для вашего метода относительно результатов SpaCy\n",
        "* Проанализируйте ошибки обоих подходов, какие типы ошибок характерны для каждого подхода\n",
        "5. Представьте сравнение результатов в виде таблицы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "HFxhvnXJPI4d"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Stories.txt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[18], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ваш код\u001b[39;00m\n\u001b[0;32m      3\u001b[0m news_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStories.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnews_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m      5\u001b[0m     news_text \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n",
            "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:310\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[0;32m    304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    308\u001b[0m     )\n\u001b[1;32m--> 310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io_open(file, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Stories.txt'"
          ]
        }
      ],
      "source": [
        "# ваш код\n",
        "\n",
        "news_text = \"Stories.txt\"\n",
        "with open(news_text, 'r', encoding='utf-8') as file:\n",
        "    news_text = file.read()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "27O8fiOUXoxU",
        "outputId": "5624c4d9-d2bf-45ee-ee27-9f43fdc83cd3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'nlp_en' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[19], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m     entities \u001b[38;5;241m=\u001b[39m [(ent\u001b[38;5;241m.\u001b[39mtext, ent\u001b[38;5;241m.\u001b[39mlabel_) \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments]\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m entities\n\u001b[1;32m----> 7\u001b[0m entities_spacy \u001b[38;5;241m=\u001b[39m \u001b[43mspacy_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity, entity_type \u001b[38;5;129;01min\u001b[39;00m entities_spacy:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentity\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mentity_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
            "Cell \u001b[1;32mIn[19], line 2\u001b[0m, in \u001b[0;36mspacy_ner\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mspacy_ner\u001b[39m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m----> 2\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mnlp_en\u001b[49m(news_text)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# Извлечение сущностей\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     entities \u001b[38;5;241m=\u001b[39m [(ent\u001b[38;5;241m.\u001b[39mtext, ent\u001b[38;5;241m.\u001b[39mlabel_) \u001b[38;5;28;01mfor\u001b[39;00m ent \u001b[38;5;129;01min\u001b[39;00m doc\u001b[38;5;241m.\u001b[39ments]\n",
            "\u001b[1;31mNameError\u001b[0m: name 'nlp_en' is not defined"
          ]
        }
      ],
      "source": [
        "def spacy_ner(text, language='english'):\n",
        "    doc = nlp_en(news_text)\n",
        "    # Извлечение сущностей\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "entities_spacy = spacy_ner(text)\n",
        "for entity, entity_type in entities_spacy:\n",
        "        print(f\"- {entity}: {entity_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5gB48phBZQbN"
      },
      "outputs": [],
      "source": [
        "def improved_rule_based_ner(text, language='russian'):\n",
        "    \"\"\"\n",
        "    Улучшенная реализация распознавания именованных сущностей на основе правил\n",
        "    \"\"\"\n",
        "    # Начните с копирования и расширения базовой функции rule_based_ner\n",
        "    entities = []\n",
        "\n",
        "    # Токенизация текста\n",
        "    if language == 'russian':\n",
        "        words = word_tokenize(text, language='russian')\n",
        "    else:\n",
        "        words = word_tokenize(text, language='english')\n",
        "\n",
        "    # 1. Новые шаблоны для дополнительных типов сущностей\n",
        "    date_patterns = [\n",
        "        r'\\d{1,2}\\s(?:January|February|March|April|May|June|July|August|September|October|November|December|Jan|Feb|Mar|Apr|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\\s\\d{4}',\n",
        "        r'\\d{1,2}\\s(?:января|февраля|марта|апреля|мая|июня|июля|августа|сентября|октября|ноября|декабря|янв|февр|мар|апр|авг|сент|окт|нояб|дек|)\\s\\d{4}',\n",
        "        r'\\d{1,2}/\\d{1,2}/\\d{4}',\n",
        "        r'\\d{1,2}\\.\\d{1,2}\\.\\d{4}'\n",
        "    ]\n",
        "\n",
        "    money_patterns = [\n",
        "        r'$\\d+(?:,\\d+)*(?:\\.\\d+)?',\n",
        "        r'\\d+(?:,\\d+)*(?:\\.\\d+)?\\s(?:USD|EUR|GBP|RUB|руб\\.|долларов|евро)',\n",
        "        r'\\d+\\s(?:млн|млрд|тыс\\.)\\s(?:USD|EUR|GBP|RUB|руб\\.|долларов|евро)'\n",
        "    ]\n",
        "    # Проценты\n",
        "    percent_patterns = [\n",
        "        r'\\b\\d{1,3}(?:[.,]\\d+)?\\s?%',\n",
        "        r'\\b\\d{1,3}(?:[.,]\\d+)?\\s?(?:[-–—]\\s?\\d{1,3}(?:[.,]\\d+)?)\\s?%',\n",
        "        r'(?i)\\b\\d{1,3}(?:[.,]\\d+)?\\s?(?:проц(?:ент(?:а|ов)?)?|pct|percent)\\b',\n",
        "    ]\n",
        "\n",
        "    # Адреса электронной почты\n",
        "    email_patterns = [\n",
        "        r'(?i)\\b[a-z0-9._%+-]+@[a-z0-9.-]+\\.[a-z]{2,}\\b',\n",
        "        r'(?i)\\b[\\w.!#$%&\\'*+/=?^`{|}~-]+@[\\w-]+(?:\\.[\\w-]+)+\\b',\n",
        "    ]\n",
        "\n",
        "    # URL\n",
        "    url_patterns = [\n",
        "        r'(?i)\\b(?:https?://|ftp://|www\\.)[^\\s<>\"\\']+',\n",
        "\n",
        "        r'(?i)\\b(?:[a-z0-9-]+\\.)+[a-z]{2,}(?:/[^\\s<>\"\\']*)?',\n",
        "    ]\n",
        "\n",
        "\n",
        "    # 2. Расширим словари\n",
        "    person_prefixes = {'Mr.', 'Mrs.', 'Dr.', 'Prof.', 'Ms.', 'Sir', 'Lord', 'President', 'Duke', 'Duchess', 'Count', 'Countess', 'Senator', 'General', 'Marquis',\n",
        "                       'Г-н', 'Г-жа', 'Доктор', 'Профессор', 'Господин', 'Госпожа',  'Лорд', 'Президент', 'Граф', 'Графиня', 'Герцог', 'Герцогиня', 'Сенатор', 'Маркиз', 'Генерал'}\n",
        "\n",
        "    location_prefixes = {'in', 'at', 'from', 'around', 'infront', 'beside', 'behinde', 'to', 'в', 'из', 'на', 'через', 'между', 'по', 'под', 'над', 'за', 'во'}\n",
        "\n",
        "    organizations = {'Apple', 'Google', 'Microsoft', 'IBM', 'Facebook', 'Twitter', 'NASA', 'FBI', 'CIA', 'Xiomi', 'ASUS', 'Nvidea', 'Lenovo', 'Sumsung', 'NASA', 'TESLA', 'X', 'New York Times',\n",
        "                    'ООН', 'Газпром', 'Сбербанк', 'Яндекс', 'Роснефть', 'МГУ', 'РЖД', 'ФСБ', 'РИА Новости', 'Бурисма', 'Ozon', 'Wildberries', 'Озон', 'WB', 'ВБ', 'Рив Гош' }\n",
        "\n",
        "    famous_people = {'Трамп', 'Дональд Трамп','Путин','Владимир Путин','Зеленский','Владимир Зеленский','Эммануэль Макрон','Кир Стармер',\n",
        "                     'Trump', 'Donald Trump','Putin','Vladimir Putin','Zelensky','Vladimir Zelensky','Emmanuel Macron','Keir Starmer'}\n",
        "\n",
        "    political_sides = {'Кремль', 'Белый Дом', 'Kremlin', 'White House'}\n",
        "\n",
        "    # 3. Реализуем контекстное распознавание\n",
        "\n",
        "    # Поиск дат\n",
        "    for pattern in date_patterns:\n",
        "        for match in re.finditer(pattern, text):\n",
        "            entities.append((match.group(), 'DATE'))\n",
        "\n",
        "    # Поиск денежных сумм\n",
        "    for pattern in money_patterns:\n",
        "        for match in re.finditer(pattern, text):\n",
        "            entities.append((match.group(), 'MONEY'))\n",
        "\n",
        "    # Поиск процентов\n",
        "    for pattern in percent_patterns:\n",
        "        for match in re.finditer(pattern, text):\n",
        "            entities.append((match.group(), 'PERCENT'))\n",
        "\n",
        "    # Поиск адреса электронной почты\n",
        "    for pattern in email_patterns:\n",
        "        for match in re.finditer(pattern, text):\n",
        "            entities.append((match.group(), 'EMAIL'))\n",
        "\n",
        "    # Поиск URL\n",
        "    for pattern in url_patterns:\n",
        "        for match in re.finditer(pattern, text):\n",
        "            entities.append((match.group(), 'URL'))\n",
        "\n",
        "    # Поиск людей, мест и организаций\n",
        "    i = 0\n",
        "    while i < len(words):\n",
        "        # Поиск людей\n",
        "        if i < len(words) - 1 and words[i] in person_prefixes:\n",
        "            # Если слово после префикса начинается с заглавной буквы\n",
        "            if words[i+1][0].isupper():\n",
        "                # Собираем полное имя\n",
        "                name_parts = []\n",
        "                j = i + 1\n",
        "                while j < len(words) and words[j][0].isupper() and words[j] not in string.punctuation:\n",
        "                    name_parts.append(words[j])\n",
        "                    j += 1\n",
        "                if name_parts:\n",
        "                    entities.append((' '.join([words[i]] + name_parts), 'PERSON'))\n",
        "                    i = j\n",
        "                    continue\n",
        "\n",
        "        # Поиск организаций\n",
        "        if words[i] in organizations:\n",
        "            entities.append((words[i], 'ORGANIZATION'))\n",
        "\n",
        "        # Поиск знаменитостей\n",
        "        if words[i] in famous_people:\n",
        "            entities.append((words[i], 'NOTABLE'))\n",
        "\n",
        "        # Поиск политических сторон\n",
        "        if words[i] in political_sides:\n",
        "            entities.append((words[i], 'POLITICAL SIDES'))\n",
        "\n",
        "        # Поиск мест\n",
        "        if i < len(words) - 1 and words[i] in location_prefixes:\n",
        "            if words[i+1][0].isupper():\n",
        "                entities.append((words[i+1], 'LOCATION'))\n",
        "\n",
        "        i += 1\n",
        "    return entities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq5LtGQ9ZRBL",
        "outputId": "d2b22853-5a00-44e9-bbc0-bdecfac8b091"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'word_tokenize' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m rule_entities \u001b[38;5;241m=\u001b[39m \u001b[43mimproved_rule_based_ner\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnews_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEnglish parallel text:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m entity, entity_type \u001b[38;5;129;01min\u001b[39;00m rule_entities:\n",
            "Cell \u001b[1;32mIn[20], line 10\u001b[0m, in \u001b[0;36mimproved_rule_based_ner\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Токенизация текста\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m language \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m---> 10\u001b[0m     words \u001b[38;5;241m=\u001b[39m \u001b[43mword_tokenize\u001b[49m(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrussian\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     12\u001b[0m     words \u001b[38;5;241m=\u001b[39m word_tokenize(text, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'word_tokenize' is not defined"
          ]
        }
      ],
      "source": [
        "rule_entities = improved_rule_based_ner(news_text)\n",
        "\n",
        "print(\"\\nEnglish parallel text:\")\n",
        "for entity, entity_type in rule_entities:\n",
        "    print(f\"- {entity}: {entity_type}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0yuROvt_bE_Z"
      },
      "outputs": [],
      "source": [
        "def evaluate_ner(predicted, gold_standard):\n",
        "    # Преобразуем списки в множества для удобства сравнения\n",
        "    predicted_set = set(predicted)\n",
        "    gold_set = set(gold_standard)\n",
        "\n",
        "    # Рассчитываем истинно положительные, ложно положительные и ложно отрицательные\n",
        "    true_positives = len(predicted_set.intersection(gold_set))\n",
        "    false_positives = len(predicted_set - gold_set)\n",
        "    false_negatives = len(gold_set - predicted_set)\n",
        "\n",
        "    # Рассчитываем точность, полноту и F1-меру\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "F7OMGWSQbgJ_"
      },
      "outputs": [],
      "source": [
        "def print_evaluation_results(method_name, precision, recall, f1):\n",
        "    print(f\"\\nРезультаты для {method_name}:\")\n",
        "    print('-' * 50)\n",
        "    print(f\"Точность (Precision): {precision:.4f}\")\n",
        "    print(f\"Полнота (Recall): {recall:.4f}\")\n",
        "    print(f\"F1-мера: {f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "o4AkJs3wbiVc"
      },
      "outputs": [],
      "source": [
        "gold_standard = [\n",
        "(\"Barack Obama\", \"PERSON\"),\n",
        "(\"Angela Merkel\", \"PERSON\"),\n",
        "(\"June 2015\", \"DATE\"),\n",
        "(\"Apple\", \"ORGANISATION\"),\n",
        "(\"iPhone\", \"PRODUCT\"),\n",
        "(\"Cupertino\", \"LOCATION\"),\n",
        "(\"Harvard University\", \"ORGANISATION\"),\n",
        "(\"Nature\", \"ORGANISATION\"),\n",
        "(\"Volga River\", \"LOCATION\"),\n",
        "(\"Europe\", \"LOCATION\"),\n",
        "(\"World War II\", \"EVENT\"),\n",
        "(\"1945\", \"DATE\"),\n",
        "(\"Elon Musk\", \"PERSON\"),\n",
        "(\"Tesla\", \"ORGANISATION\"),\n",
        "(\"SpaceX\", \"ORGANISATION\"),\n",
        "(\"John\", \"PERSON\"),\n",
        "(\"Google\", \"ORGANISATION\"),\n",
        "(\"New York Stock Exchange\", \"ORGANISATION\"),\n",
        "(\"France\", \"LOCATION\"),\n",
        "(\"Germany\", \"LOCATION\"),\n",
        "(\"Paris\", \"LOCATION\"),\n",
        "(\"Everest\", \"LOCATION\"),\n",
        "(\"Himalayas\", \"LOCATION\"),\n",
        "(\"«War and Peace»\", \"WORK_OF_ART\"),\n",
        "(\"By Leo Tolstoy\", \"PERSON\"),\n",
        "(\"World Health Organization\", \"ORGANISATION\"),\n",
        "(\"COVID-19\", \"EVENT\"),\n",
        "(\"Friday\", \"DATE\"),\n",
        "(\"UAE\", \"LOCATION\"),\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2DzxyjZbkTH",
        "outputId": "465633fb-9ec6-423d-c5f2-9c302b74011a"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'entities_spacy' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[25], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Оценка spaCy NER\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m spacy_precision, spacy_recall, spacy_f1 \u001b[38;5;241m=\u001b[39m evaluate_ner(\u001b[43mentities_spacy\u001b[49m, gold_standard)\n\u001b[0;32m      4\u001b[0m print_evaluation_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaCy NER\u001b[39m\u001b[38;5;124m\"\u001b[39m, spacy_precision, spacy_recall, spacy_f1)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'entities_spacy' is not defined"
          ]
        }
      ],
      "source": [
        "# Оценка spaCy NER\n",
        "spacy_precision, spacy_recall, spacy_f1 = evaluate_ner(entities_spacy, gold_standard)\n",
        "\n",
        "print_evaluation_results(\"spaCy NER\", spacy_precision, spacy_recall, spacy_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVD0nnwIbure",
        "outputId": "bb271c2a-24a6-466d-ebb7-ec00b826e39f"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'rule_entities' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[26], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Оценка rule-based NER\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m rule_precision, rule_recall, rule_f1 \u001b[38;5;241m=\u001b[39m evaluate_ner(\u001b[43mrule_entities\u001b[49m, gold_standard)\n\u001b[0;32m      4\u001b[0m print_evaluation_results(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRule-based\u001b[39m\u001b[38;5;124m\"\u001b[39m, rule_precision, rule_recall, rule_f1)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'rule_entities' is not defined"
          ]
        }
      ],
      "source": [
        "# Оценка rule-based NER\n",
        "rule_precision, rule_recall, rule_f1 = evaluate_ner(rule_entities, gold_standard)\n",
        "\n",
        "print_evaluation_results(\"Rule-based\", rule_precision, rule_recall, rule_f1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "GleD2MDJbviC",
        "outputId": "e2201ec7-1b41-4174-9ee4-45a35315b015"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pandas'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Таблица сравнения\u001b[39;00m\n\u001b[0;32m      4\u001b[0m results_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([\n\u001b[0;32m      5\u001b[0m     {\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspaCy NER\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m     }\n\u001b[0;32m     17\u001b[0m ])\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Таблица сравнения\n",
        "results_df = pd.DataFrame([\n",
        "    {\n",
        "        \"Method\": \"spaCy NER\",\n",
        "        \"Precision\": spacy_precision,\n",
        "        \"Recall\": spacy_recall,\n",
        "        \"F1\": spacy_f1,\n",
        "    },\n",
        "    {\n",
        "        \"Method\": \"Rule-based\",\n",
        "        \"Precision\": rule_precision,\n",
        "        \"Recall\": rule_recall,\n",
        "        \"F1\": rule_f1,\n",
        "    }\n",
        "])\n",
        "\n",
        "\n",
        "results_df[[\"Precision\",\"Recall\",\"F1\"]] = results_df[[\"Precision\",\"Recall\",\"F1\"]].round(4)\n",
        "results_df"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
