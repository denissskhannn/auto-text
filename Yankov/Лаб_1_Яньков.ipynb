{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VL_YISip2qbr"
      },
      "source": [
        "**Задание 0. Загрузите необходимые библиотеки**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 0.0/12.8 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/12.8 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.5/12.8 MB 2.1 MB/s eta 0:00:06\n",
            "     --- ------------------------------------ 1.0/12.8 MB 1.9 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 1.3/12.8 MB 1.9 MB/s eta 0:00:07\n",
            "     ----- ---------------------------------- 1.8/12.8 MB 1.8 MB/s eta 0:00:07\n",
            "     ------ --------------------------------- 2.1/12.8 MB 1.8 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 2.6/12.8 MB 1.9 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 2.6/12.8 MB 1.9 MB/s eta 0:00:06\n",
            "     --------- ------------------------------ 2.9/12.8 MB 1.5 MB/s eta 0:00:07\n",
            "     --------- ------------------------------ 3.1/12.8 MB 1.5 MB/s eta 0:00:07\n",
            "     ---------- ----------------------------- 3.4/12.8 MB 1.5 MB/s eta 0:00:07\n",
            "     ----------- ---------------------------- 3.7/12.8 MB 1.5 MB/s eta 0:00:07\n",
            "     ------------ --------------------------- 3.9/12.8 MB 1.4 MB/s eta 0:00:07\n",
            "     ------------- -------------------------- 4.2/12.8 MB 1.5 MB/s eta 0:00:06\n",
            "     ------------- -------------------------- 4.5/12.8 MB 1.4 MB/s eta 0:00:06\n",
            "     -------------- ------------------------- 4.7/12.8 MB 1.4 MB/s eta 0:00:06\n",
            "     --------------- ------------------------ 5.0/12.8 MB 1.4 MB/s eta 0:00:06\n",
            "     ---------------- ----------------------- 5.2/12.8 MB 1.4 MB/s eta 0:00:06\n",
            "     ----------------- ---------------------- 5.5/12.8 MB 1.4 MB/s eta 0:00:06\n",
            "     ------------------ --------------------- 5.8/12.8 MB 1.4 MB/s eta 0:00:06\n",
            "     ------------------ --------------------- 6.0/12.8 MB 1.4 MB/s eta 0:00:05\n",
            "     ------------------- -------------------- 6.3/12.8 MB 1.4 MB/s eta 0:00:05\n",
            "     --------------------- ------------------ 6.8/12.8 MB 1.4 MB/s eta 0:00:05\n",
            "     --------------------- ------------------ 6.8/12.8 MB 1.4 MB/s eta 0:00:05\n",
            "     ---------------------- ----------------- 7.3/12.8 MB 1.4 MB/s eta 0:00:04\n",
            "     ----------------------- ---------------- 7.6/12.8 MB 1.4 MB/s eta 0:00:04\n",
            "     ------------------------ --------------- 7.9/12.8 MB 1.4 MB/s eta 0:00:04\n",
            "     ------------------------- -------------- 8.1/12.8 MB 1.4 MB/s eta 0:00:04\n",
            "     -------------------------- ------------- 8.4/12.8 MB 1.4 MB/s eta 0:00:04\n",
            "     --------------------------- ------------ 8.7/12.8 MB 1.4 MB/s eta 0:00:04\n",
            "     ---------------------------- ----------- 9.2/12.8 MB 1.4 MB/s eta 0:00:03\n",
            "     ----------------------------- ---------- 9.4/12.8 MB 1.4 MB/s eta 0:00:03\n",
            "     ------------------------------ --------- 9.7/12.8 MB 1.4 MB/s eta 0:00:03\n",
            "     ------------------------------- -------- 10.0/12.8 MB 1.4 MB/s eta 0:00:03\n",
            "     -------------------------------- ------- 10.5/12.8 MB 1.4 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 10.7/12.8 MB 1.4 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 11.3/12.8 MB 1.4 MB/s eta 0:00:02\n",
            "     ------------------------------------ --- 11.5/12.8 MB 1.4 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 12.1/12.8 MB 1.4 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 12.3/12.8 MB 1.5 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 12.8/12.8 MB 1.5 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "     ---------------------------------------- 0.0/15.3 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.3/15.3 MB ? eta -:--:--\n",
            "     - -------------------------------------- 0.5/15.3 MB 1.1 MB/s eta 0:00:14\n",
            "     -- ------------------------------------- 1.0/15.3 MB 1.7 MB/s eta 0:00:09\n",
            "     --- ------------------------------------ 1.3/15.3 MB 1.6 MB/s eta 0:00:09\n",
            "     ---- ----------------------------------- 1.6/15.3 MB 1.6 MB/s eta 0:00:09\n",
            "     ---- ----------------------------------- 1.8/15.3 MB 1.5 MB/s eta 0:00:09\n",
            "     ----- ---------------------------------- 2.1/15.3 MB 1.4 MB/s eta 0:00:10\n",
            "     ------ --------------------------------- 2.4/15.3 MB 1.4 MB/s eta 0:00:09\n",
            "     ------ --------------------------------- 2.6/15.3 MB 1.5 MB/s eta 0:00:09\n",
            "     -------- ------------------------------- 3.1/15.3 MB 1.5 MB/s eta 0:00:09\n",
            "     -------- ------------------------------- 3.4/15.3 MB 1.5 MB/s eta 0:00:08\n",
            "     --------- ------------------------------ 3.7/15.3 MB 1.5 MB/s eta 0:00:08\n",
            "     ---------- ----------------------------- 4.2/15.3 MB 1.5 MB/s eta 0:00:08\n",
            "     ----------- ---------------------------- 4.5/15.3 MB 1.5 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 4.7/15.3 MB 1.5 MB/s eta 0:00:08\n",
            "     ------------ --------------------------- 4.7/15.3 MB 1.5 MB/s eta 0:00:08\n",
            "     ------------- -------------------------- 5.0/15.3 MB 1.4 MB/s eta 0:00:08\n",
            "     ------------- -------------------------- 5.2/15.3 MB 1.4 MB/s eta 0:00:08\n",
            "     -------------- ------------------------- 5.5/15.3 MB 1.4 MB/s eta 0:00:08\n",
            "     --------------- ------------------------ 5.8/15.3 MB 1.4 MB/s eta 0:00:07\n",
            "     --------------- ------------------------ 6.0/15.3 MB 1.3 MB/s eta 0:00:07\n",
            "     --------------- ------------------------ 6.0/15.3 MB 1.3 MB/s eta 0:00:07\n",
            "     ---------------- ----------------------- 6.3/15.3 MB 1.3 MB/s eta 0:00:07\n",
            "     ----------------- ---------------------- 6.6/15.3 MB 1.3 MB/s eta 0:00:07\n",
            "     ----------------- ---------------------- 6.6/15.3 MB 1.3 MB/s eta 0:00:07\n",
            "     ----------------- ---------------------- 6.8/15.3 MB 1.2 MB/s eta 0:00:07\n",
            "     ------------------ --------------------- 7.1/15.3 MB 1.2 MB/s eta 0:00:07\n",
            "     ------------------- -------------------- 7.3/15.3 MB 1.2 MB/s eta 0:00:07\n",
            "     ------------------- -------------------- 7.3/15.3 MB 1.2 MB/s eta 0:00:07\n",
            "     ------------------- -------------------- 7.6/15.3 MB 1.2 MB/s eta 0:00:07\n",
            "     --------------------- ------------------ 8.1/15.3 MB 1.2 MB/s eta 0:00:06\n",
            "     --------------------- ------------------ 8.4/15.3 MB 1.2 MB/s eta 0:00:06\n",
            "     ---------------------- ----------------- 8.7/15.3 MB 1.2 MB/s eta 0:00:06\n",
            "     ----------------------- ---------------- 8.9/15.3 MB 1.2 MB/s eta 0:00:06\n",
            "     ------------------------ --------------- 9.2/15.3 MB 1.2 MB/s eta 0:00:05\n",
            "     ------------------------- -------------- 9.7/15.3 MB 1.3 MB/s eta 0:00:05\n",
            "     -------------------------- ------------- 10.0/15.3 MB 1.3 MB/s eta 0:00:05\n",
            "     -------------------------- ------------- 10.0/15.3 MB 1.3 MB/s eta 0:00:05\n",
            "     -------------------------- ------------- 10.2/15.3 MB 1.2 MB/s eta 0:00:05\n",
            "     --------------------------- ------------ 10.5/15.3 MB 1.2 MB/s eta 0:00:04\n",
            "     ---------------------------- ----------- 10.7/15.3 MB 1.2 MB/s eta 0:00:04\n",
            "     ---------------------------- ----------- 11.0/15.3 MB 1.2 MB/s eta 0:00:04\n",
            "     ----------------------------- ---------- 11.3/15.3 MB 1.2 MB/s eta 0:00:04\n",
            "     ------------------------------ --------- 11.5/15.3 MB 1.2 MB/s eta 0:00:04\n",
            "     ------------------------------ --------- 11.8/15.3 MB 1.2 MB/s eta 0:00:03\n",
            "     ------------------------------- -------- 12.1/15.3 MB 1.2 MB/s eta 0:00:03\n",
            "     -------------------------------- ------- 12.3/15.3 MB 1.2 MB/s eta 0:00:03\n",
            "     -------------------------------- ------- 12.6/15.3 MB 1.2 MB/s eta 0:00:03\n",
            "     --------------------------------- ------ 12.8/15.3 MB 1.2 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 12.8/15.3 MB 1.2 MB/s eta 0:00:02\n",
            "     ---------------------------------- ----- 13.1/15.3 MB 1.2 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 13.6/15.3 MB 1.2 MB/s eta 0:00:02\n",
            "     ----------------------------------- ---- 13.6/15.3 MB 1.2 MB/s eta 0:00:02\n",
            "     ------------------------------------ --- 13.9/15.3 MB 1.2 MB/s eta 0:00:02\n",
            "     ------------------------------------- -- 14.2/15.3 MB 1.2 MB/s eta 0:00:01\n",
            "     ------------------------------------- -- 14.4/15.3 MB 1.2 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 14.7/15.3 MB 1.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------  14.9/15.3 MB 1.2 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 15.3/15.3 MB 1.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pymorphy3>=1.0.0 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from ru-core-news-sm==3.8.0) (2.0.6)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "import string\n",
        "import pandas as pd\n",
        "import spacy\n",
        "!python -m spacy download en_core_web_sm \n",
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "english_stemmer = SnowballStemmer(\"english\")\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YUM4z_92XzR"
      },
      "source": [
        "**Задание 1. Загрузка данных**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwqI1lLF2j3t"
      },
      "source": [
        "Вставьте текст для обработки согласно вашему варианту"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B2Tsy8T62ZRo"
      },
      "outputs": [],
      "source": [
        "text = '''На глубине более 3000 метров, где давление превышает 300 атмосфер, существуют удивительные экосистемы, независимые от солнечного света! \n",
        "Гидротермальные источники (hydrothermal vents) с температурой до +400°C формируют \"оазисы жизни\" вокруг черных курильщиков. \n",
        "В 2023 году международная экспедиция DeepSea Explorer обнаружила 17 ранее неизвестных видов глубоководных организмов в Марианской впадине. \n",
        "Особый интерес вызвали бактерии Thermococcus barophilus, способные выживать при давлении в 1200 бар и температуре 95°C! \n",
        "Около 76% глубоководных созданий обладают способностью к биолюминесценции — преобразованию химической энергии в свет с КПД до 98%. \n",
        "\"Механизм light production у морских организмов в 5-8 раз эффективнее любых искусственных источников света,\" — отмечает профессор Морозова. \n",
        "Медузы вида Aequorea victoria производят белок GFP (Green Fluorescent Protein), который стал незаменимым инструментом в генетических исследованиях и принес своим открывателям Нобелевскую премию 2008 года. \n",
        "Компания Bio-Lumina разрабатывает технологию StreetGlow™ — живые фонари на основе генетически модифицированных водорослей, светящихся без электричества в течение 12-14 часов каждую ночь!'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYmTHvNm2ph6"
      },
      "source": [
        "**Задание 2. Нормализация текста.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgLHFLVp3Apg"
      },
      "source": [
        "Приведите текст к нижнему регистру, удалите лишние пробоелы, переносы строк, спецсимволы, пунктуацию, обработайте цифры."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiyJLZrm3Z8s"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "в году искусственный интеллект ai совершил революцию в медицинской диагностике нейросети типа medvision способны обнаруживать раковые клетки с точностью до превосходя опытных онкологов на группа исследователей из mit и сколтеха разработала алгоритм deepdiagnosis который анализирует мрт снимки в раз быстрее человека мы наблюдаем начало новой эры в здравоохранении — заявил профессор иванов на конференции ai med однако сохраняются этические вопросы кто несет ответственность за ошибку ии диагноста согласно опросу только пациентов доверяют ai заключениям без подтверждения врача интеграция machine learning в больницах требует пересмотра медицинских протоколов и подготовки специалистов способных эффективно взаимодействовать с умными помощниками в россии к году планируется внедрить ai системы в федеральных медцентров\n"
          ]
        }
      ],
      "source": [
        "# ваш код\n",
        "def normalizing_text(text):\n",
        "    text = text.lower()\n",
        "    text = ''.join(char for char in text if not char.isdigit())\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    text = text.translate(translator)\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "normalized_text = normalizing_text(text)\n",
        "print(normalized_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6evm-5d3dPD"
      },
      "source": [
        "**Задание 3. Токенизация**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O90JD4L83gbj"
      },
      "source": [
        "Токенизируйте текст."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QjGIQM3F3udR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['в', 'году', 'искусственный', 'интеллект', 'ai', 'совершил', 'революцию', 'в', 'медицинской', 'диагностике', 'нейросети', 'типа', 'medvision', 'способны', 'обнаруживать', 'раковые', 'клетки', 'с', 'точностью', 'до', 'превосходя', 'опытных', 'онкологов', 'на', 'группа', 'исследователей', 'из', 'mit', 'и', 'сколтеха', 'разработала', 'алгоритм', 'deepdiagnosis', 'который', 'анализирует', 'мрт', 'снимки', 'в', 'раз', 'быстрее', 'человека', 'мы', 'наблюдаем', 'начало', 'новой', 'эры', 'в', 'здравоохранении', '—', 'заявил', 'профессор', 'иванов', 'на', 'конференции', 'ai', 'med', 'однако', 'сохраняются', 'этические', 'вопросы', 'кто', 'несет', 'ответственность', 'за', 'ошибку', 'ии', 'диагноста', 'согласно', 'опросу', 'только', 'пациентов', 'доверяют', 'ai', 'заключениям', 'без', 'подтверждения', 'врача', 'интеграция', 'machine', 'learning', 'в', 'больницах', 'требует', 'пересмотра', 'медицинских', 'протоколов', 'и', 'подготовки', 'специалистов', 'способных', 'эффективно', 'взаимодействовать', 'с', 'умными', 'помощниками', 'в', 'россии', 'к', 'году', 'планируется', 'внедрить', 'ai', 'системы', 'в', 'федеральных', 'медцентров']\n"
          ]
        }
      ],
      "source": [
        "# ваш код\n",
        "def tokenization(text):\n",
        "    return text.split()\n",
        "\n",
        "tokens = tokenization(normalized_text)\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x0ED6dCL3vrB"
      },
      "source": [
        "**Задание 4. Удаление стоп-слов**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awJNjV8y30zn"
      },
      "source": [
        "Выведите 2 списка - 1. Очищенных токенов, 2. Список удаленных стоп-слов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9uFV9dG39L2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Очищенные токены: ['году', 'искусственный', 'интеллект', 'ai', 'совершил', 'революцию', 'медицинской', 'диагностике', 'нейросети', 'типа', 'medvision', 'способны', 'обнаруживать', 'раковые', 'клетки', 'точностью', 'превосходя', 'опытных', 'онкологов', 'группа', 'исследователей', 'mit', 'сколтеха', 'разработала', 'алгоритм', 'deepdiagnosis', 'который', 'анализирует', 'мрт', 'снимки', 'быстрее', 'человека', 'наблюдаем', 'начало', 'новой', 'эры', 'здравоохранении', '—', 'заявил', 'профессор', 'иванов', 'конференции', 'ai', 'med', 'однако', 'сохраняются', 'этические', 'вопросы', 'несет', 'ответственность', 'ошибку', 'ии', 'диагноста', 'согласно', 'опросу', 'пациентов', 'доверяют', 'ai', 'заключениям', 'подтверждения', 'врача', 'интеграция', 'machine', 'learning', 'больницах', 'требует', 'пересмотра', 'медицинских', 'протоколов', 'подготовки', 'специалистов', 'способных', 'эффективно', 'взаимодействовать', 'умными', 'помощниками', 'россии', 'году', 'планируется', 'внедрить', 'ai', 'системы', 'федеральных', 'медцентров']\n",
            "Удаленные стоп-слова: ['в', 'в', 'с', 'до', 'на', 'из', 'и', 'в', 'раз', 'мы', 'в', 'на', 'кто', 'за', 'только', 'без', 'в', 'и', 'с', 'в', 'к', 'в']\n"
          ]
        }
      ],
      "source": [
        "# ваш код\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "def removing_stopwords(tokens):\n",
        "    russian_stopwords = stopwords.words('russian')\n",
        "    cleaned_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "    removed_stopwords = [token for token in tokens if token in russian_stopwords]\n",
        "    return cleaned_tokens, removed_stopwords\n",
        "\n",
        "cleaned_tokens, removed_stopwords = removing_stopwords(tokens)\n",
        "print(\"Очищенные токены:\", cleaned_tokens)\n",
        "print(\"Удаленные стоп-слова:\", removed_stopwords)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbV8P56O3-ZO"
      },
      "source": [
        "**Задание 5. Лемматизация и стемминг**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OZ1A8VhK4HTk"
      },
      "source": [
        "Примените к токенам алгоритмы лемматизации и стемминга. Выведите 2 списка - 1. Лемматизированные токены 2. Стемматизированные токены"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrCmA4tt4WH6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['год', 'искусствен', 'интеллект', 'ai', 'соверш', 'революц', 'медицинск', 'диагностик', 'нейросет', 'тип', 'medvision', 'способн', 'обнаружива', 'раков', 'клетк', 'точност', 'превосход', 'опытн', 'онколог', 'групп', 'исследовател', 'mit', 'сколтех', 'разработа', 'алгоритм', 'deepdiagnosis', 'котор', 'анализир', 'мрт', 'снимк', 'быстр', 'человек', 'наблюда', 'нача', 'нов', 'эр', 'здравоохранен', 'заяв', 'профессор', 'иван', 'конференц', 'ai', 'med', 'однак', 'сохраня', 'этическ', 'вопрос', 'несет', 'ответствен', 'ошибк', 'и', 'диагност', 'согласн', 'опрос', 'пациент', 'доверя', 'ai', 'заключен', 'подтвержден', 'врач', 'интеграц', 'machine', 'learning', 'больниц', 'треб', 'пересмотр', 'медицинск', 'протокол', 'подготовк', 'специалист', 'способн', 'эффективн', 'взаимодействова', 'умн', 'помощник', 'росс', 'год', 'планир', 'внедр', 'ai', 'систем', 'федеральн', 'медцентр']\n",
            "['год', 'искусственный', 'интеллект', 'ai', 'совершить', 'революция', 'медицинский', 'диагностика', 'нейросети', 'тип', 'medvision', 'способный', 'обнаруживать', 'раковый', 'клетка', 'точность', 'превосходить', 'опытный', 'онколог', 'группа', 'исследователь', 'mit', 'сколтеха', 'разработать', 'алгоритм', 'deepdiagnosis', 'который', 'анализировать', 'мрт', 'снимок', 'быстрый', 'человек', 'наблюдать', 'начало', 'новый', 'эра', 'здравоохранение', 'заявить', 'профессор', 'иван', 'конференция', 'ai', 'med', 'однако', 'сохраняться', 'этический', 'вопрос', 'нести', 'ответственность', 'ошибка', 'ии', 'диагност', 'согласно', 'опрос', 'пациент', 'доверять', 'ai', 'заключение', 'подтверждение', 'врач', 'интеграция', 'machine', 'learning', 'больницах', 'требовать', 'пересмотр', 'медицинский', 'протокол', 'подготовка', 'специалист', 'способный', 'эффективно', 'взаимодействовать', 'умный', 'помощник', 'россия', 'год', 'планироваться', 'внедрить', 'ai', 'система', 'федеральный', 'медцентров']\n"
          ]
        }
      ],
      "source": [
        "# ваш код\n",
        "joined_tokens = ' '.join(cleaned_tokens)\n",
        "doc = nlp_ru(joined_tokens)\n",
        "lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "words = re.findall(r'\\b\\w+\\b', joined_tokens)\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "print(stems)\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_R2xPrh4bW6"
      },
      "source": [
        "**Задание 6. Напишите функцию для препроцессинга текста**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "drJR_Rff4j_g"
      },
      "source": [
        "Объедините все шаги в одну функцию. Выведите результат с лемматизированным списком"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Лемматизированные токены (83):\n",
            "['год', 'искусственный', 'интеллект', 'ai', 'совершить', 'революция', 'медицинский', 'диагностика', 'нейросети', 'тип', 'medvision', 'способный', 'обнаруживать', 'раковый', 'клетка', 'точность', 'превосходить', 'опытный', 'онколог', 'группа', 'исследователь', 'mit', 'сколтеха', 'разработать', 'алгоритм', 'deepdiagnosis', 'который', 'анализировать', 'мрт', 'снимок', 'быстрый', 'человек', 'наблюдать', 'начало', 'новый', 'эра', 'здравоохранение', 'заявить', 'профессор', 'иван', 'конференция', 'ai', 'med', 'однако', 'сохраняться', 'этический', 'вопрос', 'нести', 'ответственность', 'ошибка', 'ии', 'диагност', 'согласно', 'опрос', 'пациент', 'доверять', 'ai', 'заключение', 'подтверждение', 'врач', 'интеграция', 'machine', 'learning', 'больницах', 'требовать', 'пересмотр', 'медицинский', 'протокол', 'подготовка', 'специалист', 'способный', 'эффективно', 'взаимодействовать', 'умный', 'помощник', 'россия', 'год', 'планироваться', 'внедрить', 'ai', 'система', 'федеральный', 'медцентров']\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text_simple(text):\n",
        "    # Нормализация\n",
        "    text = text.lower()\n",
        "    text = ''.join(char for char in text if not char.isdigit())\n",
        "    translator = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n",
        "    text = text.translate(translator)\n",
        "    text = ' '.join(text.split())\n",
        "    \n",
        "    # Токенизация\n",
        "    tokens = text.split()\n",
        "    \n",
        "    # Удаление стоп-слов\n",
        "    russian_stopwords = set(stopwords.words('russian'))\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "    \n",
        "    # Лемматизация\n",
        "    joined_tokens = ' '.join(filtered_tokens)\n",
        "    doc = nlp_ru(joined_tokens)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    \n",
        "    return lemmas\n",
        "\n",
        "lemmas = preprocess_text_simple(text)\n",
        "print(f\"Лемматизированный список токенов ({len(lemmas)}):\")\n",
        "print(lemmas)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
