{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Эта тетрадка содержит примеры и упражнения по двум основным методам векторизации текста: Bag of Words (мешок слов) и TF-IDF (Term Frequency-Inverse Document Frequency)."
      ],
      "metadata": {
        "id": "owKq2QMu-_V-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "C-tsQpvKAdW9"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Часть 1: Исходные данные**"
      ],
      "metadata": {
        "id": "WEJQsdB__X3v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4n2kaiB-y9C"
      },
      "outputs": [],
      "source": [
        "# Наш корпус документов\n",
        "documents = [\n",
        "    \"Машинное обучение - это подраздел искусственного интеллекта.\",\n",
        "    \"Нейронные сети широко используются в машинном обучении.\",\n",
        "    \"Глубокое обучение основано на многослойных нейронных сетях.\",\n",
        "    \"Искусственный интеллект имитирует когнитивные функции человека.\",\n",
        "    \"Компьютерное зрение и обработка языка - примеры применения ИИ.\"\n",
        "]\n",
        "\n",
        "# Вывод документов с номерами\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"Документ {i}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Часть 2: Предобработка**"
      ],
      "metadata": {
        "id": "tApzmP2y_80E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(documents)):\n",
        "    # Приведение к нижнему регистру\n",
        "    documents[i] = documents[i].lower()\n",
        "    # Удаление пунктуации\n",
        "    documents[i] = re.sub(r'[^\\w\\s]', '', documents[i])\n",
        "\n",
        "  # Вывод предобработанных документов\n",
        "print(\"Предобработанные документы:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"Документ {i}: {doc}\")"
      ],
      "metadata": {
        "id": "h8f1OVNaAAeP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Часть 3: Модель Bag of Words (мешок слов)**"
      ],
      "metadata": {
        "id": "XakwSM_Z_dqJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer  # Для создания Bag of Words"
      ],
      "metadata": {
        "id": "2pdp6lFOB3mC"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3.1 Создание матрицы Bag of Words"
      ],
      "metadata": {
        "id": "PHP7yxKQCEpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание векторизатора\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Обучение векторизатора и преобразование документов\n",
        "bow_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Получение списка фичей (слов)\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Преобразование разреженной матрицы в плотную для наглядности\n",
        "bow_df = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=feature_names,\n",
        "    index=[f'Документ {i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "# Вывод матрицы Bag of Words\n",
        "print(\"Матрица Bag of Words:\")\n",
        "print(bow_df)"
      ],
      "metadata": {
        "id": "scdI5kc4B4-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Получение списка фичей (слов)\n",
        "\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "Что происходит:\n",
        "Этот шаг извлекает из обученного CountVectorizer все уникальные слова (или n-граммы), которые были использованы при создании матрицы Bag of Words.\n",
        "\n",
        "Зачем это нужно:\n",
        "* Интерпретация результатов: Без этого шага у нас была бы только числовая матрица, но мы не знали бы, какой столбец соответствует какому слову.\n",
        "* Понимание словаря: Метод get_feature_names_out() возвращает список всех уникальных терминов, которые векторизатор извлек из документов и включил в свой словарь.\n",
        "* Подготовка к визуализации: Имена признаков необходимы для создания понятной таблицы с метками строк и столбцов."
      ],
      "metadata": {
        "id": "PMK9JL-1C4Dn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Часть 4: Модель TF-IDF**"
      ],
      "metadata": {
        "id": "_MtVs8JuDTt6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.1 Создание матрицы TF-IDF"
      ],
      "metadata": {
        "id": "TyAEZkC_Dmrx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "HUpUVp_iD2xf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Создание TF-IDF векторизатора\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Обучение векторизатора и преобразование документов\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Преобразование в DataFrame\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
        "    index=[f'Документ {i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "# Вывод матрицы TF-IDF\n",
        "print(\"Матрица TF-IDF:\")\n",
        "print(tfidf_df)"
      ],
      "metadata": {
        "id": "nauTaaSvDonR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Часть 5. Сравнение результатов**"
      ],
      "metadata": {
        "id": "keRydTjVHBdq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "SjuAChaKPomZ"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Суммируем значения по всем документам для BOW\n",
        "# Для BOW просто суммируем частоты слов по всем документам\n",
        "bow_sum = np.sum(bow_matrix.toarray(), axis=0)\n",
        "\n",
        "# Создаем словарь слово -> суммарная частота\n",
        "word_bow_dict = dict(zip(feature_names, bow_sum))\n",
        "\n",
        "# Находим топ-3 слова по BOW (самые частотные в коллекции)\n",
        "top_bow_words = sorted(word_bow_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "print(\"=== Топ-3 слова по всей коллекции ===\\n\")\n",
        "print(\"Топ-3 слова по BOW (самые частые в коллекции):\")\n",
        "for word, count in top_bow_words:\n",
        "    print(f\"- {word}: {count} раз\")\n",
        "\n",
        "# Для TF-IDF нужно суммировать значения по всем документам\n",
        "tfidf_sum = np.sum(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "# Создаем словарь слово -> суммарный TF-IDF вес\n",
        "word_tfidf_dict = dict(zip(feature_names, tfidf_sum))\n",
        "\n",
        "# Находим топ-3 слова по TF-IDF (с наибольшим суммарным весом по всей коллекции)\n",
        "top_tfidf_words = sorted(word_tfidf_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "print(\"\\nТоп-3 слова по TF-IDF (с наибольшим весом по всей коллекции):\")\n",
        "for word, score in top_tfidf_words:\n",
        "    print(f\"- {word}: {score:.4f}\")\n",
        "\n",
        "# Показываем разницу между списками\n",
        "print(\"\\nСравнение топ-3 слов:\")\n",
        "bow_words = [word for word, _ in top_bow_words]\n",
        "tfidf_words = [word for word, _ in top_tfidf_words]\n",
        "\n",
        "common_words = set(bow_words) & set(tfidf_words)\n",
        "bow_only = set(bow_words) - common_words\n",
        "tfidf_only = set(tfidf_words) - common_words\n",
        "\n",
        "if common_words:\n",
        "    print(f\"Общие слова в обоих топ-3: {', '.join(common_words)}\")\n",
        "if bow_only:\n",
        "    print(f\"Только в BOW топ-3: {', '.join(bow_only)}\")\n",
        "if tfidf_only:\n",
        "    print(f\"Только в TF-IDF топ-3: {', '.join(tfidf_only)}\")"
      ],
      "metadata": {
        "id": "GAbzPK8YL_KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Самостоятельная работа**"
      ],
      "metadata": {
        "id": "ycGpAdTzQHs_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ЗАДАНИЕ:\n",
        "1. Предобработайте эти документы (удалите стоп-слова, приведите к нижнему регистру)\n",
        "2. Создайте матрицу Bag of Words\n",
        "3. Создайте матрицу TF-IDF\n",
        "4. Найдите и выведите топ-3 важных слова для каждого документа по Bag of Words\n",
        "5. Найдите и выведите топ-3 важных слова для каждого документа по TF-IDF\n",
        "6. Проанализируйте разницу между результатами и объясните её\n"
      ],
      "metadata": {
        "id": "pLOsSo-xQO_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Создаем коллекцию документов\n",
        "exercise_documents = [\n",
        "    \"Япония поражает сочетанием древних традиций и футуристических технологий. Токио с его небоскребами и неоновыми огнями соседствует с тихими храмами и садами. Весной цветение сакуры превращает страну в розовое облако. Японская кухня, от суши до рамена, предлагает уникальные вкусовые ощущения. Синкансэны позволяют комфортно перемещаться между городами.\",\n",
        "\n",
        "    \"Исландия — страна потрясающих природных контрастов. Ледники соседствуют с действующими вулканами, а горячие гейзеры бьют среди снежных равнин. Северное сияние зимой и незаходящее солнце летом создают ощущение другой планеты. Голубая лагуна с ее геотермальными водами — идеальное место для расслабления после долгих пеших походов по национальным паркам.\",\n",
        "\n",
        "    \"Таиланд привлекает путешественников белоснежными пляжами и кристально чистой водой. Острова Пхукет и Самуи предлагают роскошные курорты и активный отдых. Бангкок поражает контрастами: золотые храмы соседствуют с оживленными рынками и современными торговыми центрами. Тайская кухня с ее острыми ароматами и экзотическими фруктами — отдельное гастрономическое путешествие.\",\n",
        "\n",
        "    \"Италия — настоящий музей под открытым небом. Рим хранит наследие древней империи в Колизее и Форуме. Венеция очаровывает каналами и гондолами. Флоренция — сокровищница искусства эпохи Возрождения. Побережье Амальфи и озеро Комо предлагают живописные пейзажи. Итальянская кухня, от пасты до джелато, заслуженно считается одной из лучших в мире.\",\n",
        "\n",
        "    \"Перу хранит тайны древних цивилизаций. Мачу-Пикчу, затерянный город инков, привлекает туристов со всего мира. Линии Наска, гигантские рисунки на плато, до сих пор остаются загадкой. Озеро Титикака поражает своими плавучими островами, на которых живут местные племена. Перуанская кухня, с ее свежими морепродуктами и разнообразием картофеля, переживает всемирное признание.\",\n",
        "\n",
        "    \"Марокко — это калейдоскоп красок и ароматов. Медины Феса и Марракеша с их узкими улочками и шумными базарами погружают в атмосферу арабских сказок. Пустыня Сахара предлагает незабываемые ночи под звездами в берберских лагерях. Атласские горы привлекают любителей трекинга. Марокканская кухня славится тажинами и кус-кусом, приправленными экзотическими специями.\",\n",
        "\n",
        "    \"Новая Зеландия — рай для любителей природы и активного отдыха. Фьорды Милфорд-Саунд, ледники Южных Альп и гейзеры Роторуа демонстрируют разнообразие ландшафтов. Хоббитон, декорации из фильмов «Властелин колец», привлекают поклонников Толкина. Маори, коренное население, сохраняет свою уникальную культуру. Адреналиновые развлечения, от банджи-джампинга до рафтинга, доступны по всей стране.\",\n",
        "\n",
        "    \"Кения предлагает классическое африканское сафари. Масаи-Мара, Амбосели и Цаво — национальные парки, где можно увидеть «большую пятерку» африканских животных в их естественной среде. Ежегодная миграция антилоп гну — одно из самых впечатляющих природных зрелищ. Пляжи Момбасы с коралловыми рифами идеальны для дайвинга и снорклинга. Племена масаи и самбуру сохраняют традиционный образ жизни.\",\n",
        "\n",
        "    \"Вьетнам сочетает богатую историю и динамичное настоящее. Бухта Халонг с ее карстовыми островами — природное чудо. Хойан очаровывает древними улочками и бумажными фонариками. Дельта Меконга предлагает возможность познакомиться с сельской жизнью. Вьетнамская кухня, от фо до свежих спринг-роллов, покоряет своей свежестью. Система туннелей Ку-Чи напоминает о недавней войне.\",\n",
        "\n",
        "    \"Антарктида — последний неосвоенный континент, привлекающий самых отважных путешественников. Круизы из Ушуаи позволяют увидеть айсберги, пингвинов и китов. Пересечение пролива Дрейка — настоящее испытание для морских путешественников. Исследовательские станции разных стран ведут научную работу в суровых условиях. Полуночное солнце летом создает сюрреалистичные пейзажи ледяной пустыни.\"\n",
        "]"
      ],
      "metadata": {
        "id": "csxiQZNhQWHb"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
      ],
      "metadata": {
        "id": "ajlISOZNRL7y"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка стоп-слов\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"[^а-яё\\s]\", \" \", text)\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in russian_stopwords]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "processed_docs = [preprocess_text(doc) for doc in exercise_documents]\n",
        "\n",
        "# Матрица Bag of Words\n",
        "bow_vectorizer = CountVectorizer()\n",
        "bow_matrix = bow_vectorizer.fit_transform(processed_docs)\n",
        "bow_df = pd.DataFrame(bow_matrix.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Матрица TF-IDF\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(processed_docs)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
        "\n",
        "# Топ-3 слова по Bag of Words\n",
        "print(\"Топ-3 слова по Bag of Words\\n\")\n",
        "\n",
        "for i, row in bow_df.iterrows():\n",
        "    top_words = row.sort_values(ascending=False).head(3)\n",
        "    print(f\"Документ {i+1}:\")\n",
        "    for word, value in top_words[top_words > 0].items():\n",
        "        print(f\"{word}: {value}\")\n",
        "    print()\n",
        "\n",
        "# Топ-3 слова по TF-IDF\n",
        "print(\"Топ-3 слова по TF-IDF\\n\")\n",
        "\n",
        "for i, row in tfidf_df.iterrows():\n",
        "    top_words = row.sort_values(ascending=False).head(3)\n",
        "    print(f\"Документ {i+1}:\")\n",
        "    for word, value in top_words.items():\n",
        "        print(f\"{word}: {value:.4f}\")\n",
        "    print()"
      ],
      "metadata": {
        "id": "3Pg0Halp2uhO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbf8309f-ae1c-4800-b160-2cc4f568534f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Топ-3 слова по Bag of Words\n",
            "\n",
            "Документ 1:\n",
            "футуристических: 1\n",
            "технологий: 1\n",
            "городами: 1\n",
            "\n",
            "Документ 2:\n",
            "ледники: 1\n",
            "незаходящее: 1\n",
            "идеальное: 1\n",
            "\n",
            "Документ 3:\n",
            "экзотическими: 1\n",
            "чистой: 1\n",
            "торговыми: 1\n",
            "\n",
            "Документ 4:\n",
            "возрождения: 1\n",
            "гондолами: 1\n",
            "считается: 1\n",
            "\n",
            "Документ 5:\n",
            "инков: 1\n",
            "местные: 1\n",
            "цивилизаций: 1\n",
            "\n",
            "Документ 6:\n",
            "кус: 1\n",
            "ночи: 1\n",
            "кусом: 1\n",
            "\n",
            "Документ 7:\n",
            "банджи: 1\n",
            "южных: 1\n",
            "толкина: 1\n",
            "\n",
            "Документ 8:\n",
            "масаи: 2\n",
            "национальные: 1\n",
            "впечатляющих: 1\n",
            "\n",
            "Документ 9:\n",
            "чудо: 1\n",
            "хойан: 1\n",
            "чи: 1\n",
            "\n",
            "Документ 10:\n",
            "путешественников: 2\n",
            "увидеть: 1\n",
            "суровых: 1\n",
            "\n",
            "Топ-3 слова по TF-IDF\n",
            "\n",
            "Документ 1:\n",
            "цветение: 0.1748\n",
            "вкусовые: 0.1748\n",
            "весной: 0.1748\n",
            "\n",
            "Документ 2:\n",
            "вулканами: 0.1707\n",
            "лагуна: 0.1707\n",
            "расслабления: 0.1707\n",
            "\n",
            "Документ 3:\n",
            "чистой: 0.1729\n",
            "торговыми: 0.1729\n",
            "водой: 0.1729\n",
            "\n",
            "Документ 4:\n",
            "флоренция: 0.1693\n",
            "колизее: 0.1693\n",
            "каналами: 0.1693\n",
            "\n",
            "Документ 5:\n",
            "сих: 0.1618\n",
            "гигантские: 0.1618\n",
            "инков: 0.1618\n",
            "\n",
            "Документ 6:\n",
            "шумными: 0.1676\n",
            "кус: 0.1676\n",
            "ночи: 0.1676\n",
            "\n",
            "Документ 7:\n",
            "банджи: 0.1604\n",
            "южных: 0.1604\n",
            "толкина: 0.1604\n",
            "\n",
            "Документ 8:\n",
            "масаи: 0.3111\n",
            "национальные: 0.1556\n",
            "впечатляющих: 0.1556\n",
            "\n",
            "Документ 9:\n",
            "роллов: 0.1609\n",
            "халонг: 0.1609\n",
            "чудо: 0.1609\n",
            "\n",
            "Документ 10:\n",
            "путешественников: 0.2760\n",
            "суровых: 0.1623\n",
            "сюрреалистичные: 0.1623\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Метод TF-IDF более точно выявляет ключевые слова и организует поиск именно в конкретном документе, а Baf of Words выявил и общие слова, не столь важные для документа. Например, в документе 4 топ-3 слова по TF-IDF максимально точно подходят как ключевые, а из слов по Bag of Words только 1 слово - гондолами."
      ],
      "metadata": {
        "id": "Wm43qgw07HwD"
      }
    }
  ]
}