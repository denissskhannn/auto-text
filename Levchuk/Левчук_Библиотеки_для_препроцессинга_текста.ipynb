{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "В этой практической тетрадке мы изучим различные методы нормализации текста – стемматизацию и лемматизацию – и сравним, как работают основные библиотеки обработки естественного языка для русского и английского языков."
      ],
      "metadata": {
        "id": "qgpxdmVlmRLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Мы рассмотрим:\n",
        "\n",
        "* Разницу между стемматизацией и лемматизацией\n",
        "* Сравнение работы библиотек NLTK, spaCy, PyMorphy2 и Natasha\n",
        "* Создание комплексных пайплайнов препроцессинга для разных языков\n",
        "* Практические упражнения для закрепления материала"
      ],
      "metadata": {
        "id": "XAy7q_PxmWFd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Установка необходимых библиотек\n",
        "!pip install nltk spacy pymorphy2 natasha\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import spacy\n",
        "import pymorphy2\n",
        "from natasha import Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, Doc"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XpmGCPeXmcbl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f3c616d3-8517-431f-ee25-4135481bdb47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Collecting pymorphy2\n",
            "  Downloading pymorphy2-0.9.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting natasha\n",
            "  Downloading natasha-1.6.0-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Collecting dawg-python>=0.7.1 (from pymorphy2)\n",
            "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl.metadata (7.0 kB)\n",
            "Collecting pymorphy2-dicts-ru<3.0,>=2.4 (from pymorphy2)\n",
            "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl.metadata (2.1 kB)\n",
            "Collecting docopt>=0.6 (from pymorphy2)\n",
            "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting razdel>=0.5.0 (from natasha)\n",
            "  Downloading razdel-0.5.0-py3-none-any.whl.metadata (10.0 kB)\n",
            "Collecting navec>=0.9.0 (from natasha)\n",
            "  Downloading navec-0.10.0-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting slovnet>=0.6.0 (from natasha)\n",
            "  Downloading slovnet-0.6.0-py3-none-any.whl.metadata (34 kB)\n",
            "Collecting yargy>=0.16.0 (from natasha)\n",
            "  Downloading yargy-0.16.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting ipymarkup>=0.8.0 (from natasha)\n",
            "  Downloading ipymarkup-0.9.0-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting intervaltree>=3 (from ipymarkup>=0.8.0->natasha)\n",
            "  Downloading intervaltree-3.1.0.tar.gz (32 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in /usr/local/lib/python3.12/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.5/55.5 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading natasha-1.6.0-py3-none-any.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
            "Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
            "Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
            "Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
            "Downloading slovnet-0.6.0-py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yargy-0.16.0-py3-none-any.whl (33 kB)\n",
            "Building wheels for collected packages: docopt, intervaltree\n",
            "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13706 sha256=877e68e0f2efb1652b7d550677d996fe8205e157586c4257eadb735f6062e19a\n",
            "  Stored in directory: /root/.cache/pip/wheels/1a/bf/a1/4cee4f7678c68c5875ca89eaccf460593539805c3906722228\n",
            "  Building wheel for intervaltree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for intervaltree: filename=intervaltree-3.1.0-py2.py3-none-any.whl size=26098 sha256=840ed4a36cf401c0f2b06025ecc2b75c629d435d1158e902d02b24101ec3ed1b\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/c3/c3/238bf93c243597857edd94ddb0577faa74a8e16e9585896e83\n",
            "Successfully built docopt intervaltree\n",
            "Installing collected packages: razdel, pymorphy2-dicts-ru, docopt, dawg-python, pymorphy2, navec, intervaltree, yargy, slovnet, ipymarkup, natasha\n",
            "Successfully installed dawg-python-0.7.2 docopt-0.6.2 intervaltree-3.1.0 ipymarkup-0.9.0 natasha-1.6.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.6.0 yargy-0.16.0\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m103.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m100.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pymorphy3>=1.0.0 (from ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3-2.0.6-py3-none-any.whl.metadata (2.4 kB)\n",
            "Collecting dawg2-python>=0.8.0 (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading dawg2_python-0.9.0-py3-none-any.whl.metadata (7.5 kB)\n",
            "Collecting pymorphy3-dicts-ru (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0)\n",
            "  Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl.metadata (2.0 kB)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Downloading pymorphy3-2.0.6-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dawg2_python-0.9.0-py3-none-any.whl (9.3 kB)\n",
            "Downloading pymorphy3_dicts_ru-2.4.417150.4580142-py2.py3-none-any.whl (8.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymorphy3-dicts-ru, dawg2-python, pymorphy3, ru-core-news-sm\n",
            "Successfully installed dawg2-python-0.9.0 pymorphy3-2.0.6 pymorphy3-dicts-ru-2.4.417150.4580142 ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Сравнение стемматизации и лемматизации**"
      ],
      "metadata": {
        "id": "ORdenrTamgWN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Тестовые тексты\n",
        "russian_text = \"\"\"\n",
        "В России проживают народы с разными традициями и верованиями.\n",
        "Русские люди всегда с интересом относились к культуре соседних народов.\n",
        "Петр Первый ввел многие европейские обычаи.\n",
        "Екатерина Великая переписывалась с французскими просветителями.\n",
        "Современные россияне любят путешествовать по разным странам мира.\n",
        "\"\"\"\n",
        "\n",
        "english_text = \"\"\"\n",
        "The United States is home to people with diverse traditions and beliefs.\n",
        "Americans have always been interested in the cultures of neighboring countries.\n",
        "George Washington established many governmental customs.\n",
        "Thomas Jefferson corresponded with French intellectuals.\n",
        "Modern Americans enjoy traveling to different countries around the world.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "qqJqaLM6mprF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.1 Стемматизация с помощью NLTK. Стемматизация - это процесс нахождения основы слова путем отбрасывания аффиксов (окончаний, суффиксов). Стемматизаторы используют набор правил без словарей.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x07ZPxQImzj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "\n",
        "# Создаем стемматизаторы для английского и русского языков\n",
        "english_stemmer = SnowballStemmer(\"english\")\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "# Функция для токенизации и стемматизации\n",
        "def stem_text(text, stemmer):\n",
        "    # Приводим к нижнему регистру и токенизируем\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    # Применяем стемматизацию\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "    return stemmed_words\n",
        "\n",
        "# Стемматизация русского текста\n",
        "russian_stemmed = stem_text(russian_text, russian_stemmer)\n",
        "print(\"Стемматизированные русские слова:\")\n",
        "for i, word in enumerate(russian_stemmed[:15]):  # Выводим первые 15 слов\n",
        "    print(f\"{i+1}. {word}\")\n",
        "\n",
        "# Стемматизация английского текста\n",
        "english_stemmed = stem_text(english_text, english_stemmer)\n",
        "print(\"\\nСтемматизированные английские слова:\")\n",
        "for i, word in enumerate(english_stemmed[:15]):  # Выводим первые 15 слов\n",
        "    print(f\"{i+1}. {word}\")"
      ],
      "metadata": {
        "id": "D9_rxdQAm46d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cd12aee-0c21-4456-b9d7-7f34d11cfa6c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Стемматизированные русские слова:\n",
            "1. в\n",
            "2. росс\n",
            "3. прожива\n",
            "4. народ\n",
            "5. с\n",
            "6. разн\n",
            "7. традиц\n",
            "8. и\n",
            "9. верован\n",
            "10. русск\n",
            "11. люд\n",
            "12. всегд\n",
            "13. с\n",
            "14. интерес\n",
            "15. относ\n",
            "\n",
            "Стемматизированные английские слова:\n",
            "1. the\n",
            "2. unit\n",
            "3. state\n",
            "4. is\n",
            "5. home\n",
            "6. to\n",
            "7. peopl\n",
            "8. with\n",
            "9. divers\n",
            "10. tradit\n",
            "11. and\n",
            "12. belief\n",
            "13. american\n",
            "14. have\n",
            "15. alway\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2 Лемматизация с помощью разных библиотек. Лемматизация - это процесс приведения слова к его словарной (нормальной) форме. Лемматизаторы используют словари и учитывают грамматические особенности."
      ],
      "metadata": {
        "id": "5RQGjFJ-nBEk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2.1 NLTK (только для английского)"
      ],
      "metadata": {
        "id": "sUhOl_A9nFIE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "QMz6k24ynhut",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a5325f5-1699-4a26-9108-5c4a32039fa1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация лемматизатора\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Функция для конвертации POS-тегов NLTK в формат WordNet\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # По умолчанию существительное\n",
        "\n",
        "# Функция для лемматизации с учетом части речи\n",
        "def lemmatize_with_pos(text):\n",
        "    # Токенизация и определение части речи\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tagged = pos_tag(tokens)\n",
        "\n",
        "    # Лемматизация с учетом части речи\n",
        "    lemmas = []\n",
        "    for word, tag in tagged:\n",
        "        if word.isalpha():  # Исключаем числа и знаки препинания\n",
        "            wordnet_pos = get_wordnet_pos(tag)\n",
        "            lemmas.append(lemmatizer.lemmatize(word, wordnet_pos))\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация английского текста\n",
        "english_lemmas_nltk = lemmatize_with_pos(english_text)\n",
        "print(\"Лемматизированные английские слова (NLTK):\")\n",
        "for i, word in enumerate(english_lemmas_nltk[:15]):\n",
        "    print(f\"{i+1}. {word}\")\n",
        "\n",
        "# Примечание: NLTK не имеет встроенного лемматизатора для русского языка\n",
        "print(\"\\nNLTK не поддерживает лемматизацию русского языка напрямую.\")"
      ],
      "metadata": {
        "id": "cplx5w3znCe1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ba6bfaf-772a-467b-b2d7-94350ff835e9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные английские слова (NLTK):\n",
            "1. the\n",
            "2. united\n",
            "3. state\n",
            "4. be\n",
            "5. home\n",
            "6. to\n",
            "7. people\n",
            "8. with\n",
            "9. diverse\n",
            "10. tradition\n",
            "11. and\n",
            "12. belief\n",
            "13. american\n",
            "14. have\n",
            "15. always\n",
            "\n",
            "NLTK не поддерживает лемматизацию русского языка напрямую.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2.2 spaCy (для обоих языков)"
      ],
      "metadata": {
        "id": "EC1ibf9HoP0E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка моделей spaCy\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "# Функция для лемматизации с помощью spaCy\n",
        "def lemmatize_with_spacy(text, nlp):\n",
        "    doc = nlp(text)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация английского текста\n",
        "english_lemmas_spacy = lemmatize_with_spacy(english_text, nlp_en)\n",
        "print(\"Лемматизированные английские слова (spaCy):\")\n",
        "for i, word in enumerate(english_lemmas_spacy[:15]):\n",
        "    print(f\"{i+1}. {word}\")\n",
        "\n",
        "# Лемматизация русского текста\n",
        "russian_lemmas_spacy = lemmatize_with_spacy(russian_text, nlp_ru)\n",
        "print(\"\\nЛемматизированные русские слова (spaCy):\")\n",
        "for i, word in enumerate(russian_lemmas_spacy[:15]):\n",
        "    print(f\"{i+1}. {word}\")"
      ],
      "metadata": {
        "id": "NEAcyouloRVU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62a01ba8-ec6e-44f5-87e4-ef49f7d477e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные английские слова (spaCy):\n",
            "1. the\n",
            "2. United\n",
            "3. States\n",
            "4. be\n",
            "5. home\n",
            "6. to\n",
            "7. people\n",
            "8. with\n",
            "9. diverse\n",
            "10. tradition\n",
            "11. and\n",
            "12. belief\n",
            "13. Americans\n",
            "14. have\n",
            "15. always\n",
            "\n",
            "Лемматизированные русские слова (spaCy):\n",
            "1. в\n",
            "2. россия\n",
            "3. проживать\n",
            "4. народ\n",
            "5. с\n",
            "6. разный\n",
            "7. традиция\n",
            "8. и\n",
            "9. верование\n",
            "10. русский\n",
            "11. человек\n",
            "12. всегда\n",
            "13. с\n",
            "14. интерес\n",
            "15. относиться\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2.3 PyMorphy2 (для русского языка)"
      ],
      "metadata": {
        "id": "n-0rEI35pz4j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "# Функция для лемматизации с помощью PyMorphy2\n",
        "def lemmatize_with_pymorphy(text):\n",
        "    # Токенизация\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "    # Лемматизация\n",
        "    lemmas = [morph.parse(word)[0].normal_form for word in words]\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация русского текста\n",
        "russian_lemmas_pymorphy = lemmatize_with_pymorphy(russian_text)\n",
        "print(\"Лемматизированные русские слова (PyMorphy2):\")\n",
        "for i, word in enumerate(russian_lemmas_pymorphy[:15]):\n",
        "    print(f\"{i+1}. {word}\")"
      ],
      "metadata": {
        "id": "JskPV0C3p0UT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd083216-d23b-48d9-92b6-3878787c6e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные русские слова (PyMorphy2):\n",
            "1. в\n",
            "2. россия\n",
            "3. проживать\n",
            "4. народ\n",
            "5. с\n",
            "6. разный\n",
            "7. традиция\n",
            "8. и\n",
            "9. верование\n",
            "10. русский\n",
            "11. человек\n",
            "12. всегда\n",
            "13. с\n",
            "14. интерес\n",
            "15. относиться\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.2.4 Natasha (для русского языка)"
      ],
      "metadata": {
        "id": "cAdupAM1qYi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Инициализация компонентов Natasha\n",
        "segmenter = Segmenter()\n",
        "morph_vocab = MorphVocab()\n",
        "emb = NewsEmbedding()\n",
        "morph_tagger = NewsMorphTagger(emb)\n",
        "\n",
        "# Функция для лемматизации с помощью Natasha\n",
        "def lemmatize_with_natasha(text):\n",
        "    doc = Doc(text)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "\n",
        "    lemmas = []\n",
        "    for token in doc.tokens:\n",
        "        if token.text.isalpha():  # Исключаем числа и знаки препинания\n",
        "            token.lemmatize(morph_vocab)\n",
        "            lemmas.append(token.lemma)\n",
        "\n",
        "    return lemmas\n",
        "\n",
        "# Лемматизация русского текста\n",
        "russian_lemmas_natasha = lemmatize_with_natasha(russian_text)\n",
        "print(\"Лемматизированные русские слова (Natasha):\")\n",
        "for i, word in enumerate(russian_lemmas_natasha[:15]):\n",
        "    print(f\"{i+1}. {word}\")"
      ],
      "metadata": {
        "id": "JFPv5xZFqaAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c98bef99-1b06-45d8-d3fb-f867fdb7d7cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные русские слова (Natasha):\n",
            "1. в\n",
            "2. россия\n",
            "3. проживать\n",
            "4. народ\n",
            "5. с\n",
            "6. разный\n",
            "7. традиция\n",
            "8. и\n",
            "9. верование\n",
            "10. русский\n",
            "11. человек\n",
            "12. всегда\n",
            "13. с\n",
            "14. интерес\n",
            "15. относиться\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.3 Сравнительная таблица результатов"
      ],
      "metadata": {
        "id": "6LjqZiOKqi4r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Для наглядности сравним результаты обработки одних и тех же слов разными методами."
      ],
      "metadata": {
        "id": "U9vTvNd9qjbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Выбираем несколько интересных слов из русского текста для сравнения\n",
        "russian_words = [\"России\", \"проживают\", \"народы\", \"традициями\", \"верованиями\",\n",
        "                \"русские\", \"людей\", \"относились\", \"культуре\", \"Петр\", \"ввел\", \"обычаи\"]\n",
        "\n",
        "# Создаем DataFrame для сравнения\n",
        "comparison_ru = []\n",
        "for word in russian_words:\n",
        "    stem = russian_stemmer.stem(word.lower())\n",
        "    lemma_spacy = None\n",
        "\n",
        "    # Находим лемму в spaCy\n",
        "    doc = nlp_ru(word)\n",
        "    for token in doc:\n",
        "        lemma_spacy = token.lemma_\n",
        "\n",
        "    lemma_pymorphy = morph.parse(word.lower())[0].normal_form\n",
        "\n",
        "    # Находим лемму в Natasha\n",
        "    doc = Doc(word)\n",
        "    doc.segment(segmenter)\n",
        "    doc.tag_morph(morph_tagger)\n",
        "    for token in doc.tokens:\n",
        "        token.lemmatize(morph_vocab)\n",
        "        lemma_natasha = token.lemma\n",
        "\n",
        "    comparison_ru.append({\n",
        "        \"Слово\": word,\n",
        "        \"Стем (NLTK)\": stem,\n",
        "        \"Лемма (spaCy)\": lemma_spacy,\n",
        "        \"Лемма (PyMorphy2)\": lemma_pymorphy,\n",
        "        \"Лемма (Natasha)\": lemma_natasha\n",
        "    })\n",
        "\n",
        "# Создаем таблицу для русских слов\n",
        "comparison_ru_df = pd.DataFrame(comparison_ru)\n",
        "print(\"Сравнение методов для русского языка:\")\n",
        "comparison_ru_df"
      ],
      "metadata": {
        "id": "MBJxuEUlqlkr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "5b166b15-7325-4e8a-de61-1d446dcdbadf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сравнение методов для русского языка:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Слово Стем (NLTK) Лемма (spaCy) Лемма (PyMorphy2) Лемма (Natasha)\n",
              "0        России        росс        россия            россия          россия\n",
              "1     проживают     прожива     проживать         проживать       проживать\n",
              "2        народы       народ         народ             народ           народ\n",
              "3    традициями      традиц      традиция          традиция        традиция\n",
              "4   верованиями     верован     верование         верование       верование\n",
              "5       русские       русск       русские           русский         русский\n",
              "6         людей         люд       человек           человек         человек\n",
              "7    относились       относ    относиться        относиться      относиться\n",
              "8      культуре     культур      культура          культура        культура\n",
              "9          Петр        петр          пётр              пётр            петр\n",
              "10         ввел        ввел        ввести            ввести          ввести\n",
              "11       обычаи       обыча        обычай            обычай          обычай"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-85c591bc-256d-4b17-8cc8-edab2e506002\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Слово</th>\n",
              "      <th>Стем (NLTK)</th>\n",
              "      <th>Лемма (spaCy)</th>\n",
              "      <th>Лемма (PyMorphy2)</th>\n",
              "      <th>Лемма (Natasha)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>России</td>\n",
              "      <td>росс</td>\n",
              "      <td>россия</td>\n",
              "      <td>россия</td>\n",
              "      <td>россия</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>проживают</td>\n",
              "      <td>прожива</td>\n",
              "      <td>проживать</td>\n",
              "      <td>проживать</td>\n",
              "      <td>проживать</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>народы</td>\n",
              "      <td>народ</td>\n",
              "      <td>народ</td>\n",
              "      <td>народ</td>\n",
              "      <td>народ</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>традициями</td>\n",
              "      <td>традиц</td>\n",
              "      <td>традиция</td>\n",
              "      <td>традиция</td>\n",
              "      <td>традиция</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>верованиями</td>\n",
              "      <td>верован</td>\n",
              "      <td>верование</td>\n",
              "      <td>верование</td>\n",
              "      <td>верование</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>русские</td>\n",
              "      <td>русск</td>\n",
              "      <td>русские</td>\n",
              "      <td>русский</td>\n",
              "      <td>русский</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>людей</td>\n",
              "      <td>люд</td>\n",
              "      <td>человек</td>\n",
              "      <td>человек</td>\n",
              "      <td>человек</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>относились</td>\n",
              "      <td>относ</td>\n",
              "      <td>относиться</td>\n",
              "      <td>относиться</td>\n",
              "      <td>относиться</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>культуре</td>\n",
              "      <td>культур</td>\n",
              "      <td>культура</td>\n",
              "      <td>культура</td>\n",
              "      <td>культура</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Петр</td>\n",
              "      <td>петр</td>\n",
              "      <td>пётр</td>\n",
              "      <td>пётр</td>\n",
              "      <td>петр</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ввел</td>\n",
              "      <td>ввел</td>\n",
              "      <td>ввести</td>\n",
              "      <td>ввести</td>\n",
              "      <td>ввести</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>обычаи</td>\n",
              "      <td>обыча</td>\n",
              "      <td>обычай</td>\n",
              "      <td>обычай</td>\n",
              "      <td>обычай</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85c591bc-256d-4b17-8cc8-edab2e506002')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-85c591bc-256d-4b17-8cc8-edab2e506002 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-85c591bc-256d-4b17-8cc8-edab2e506002');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-498a2066-ce34-4070-b6c7-d393f4845772\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-498a2066-ce34-4070-b6c7-d393f4845772')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-498a2066-ce34-4070-b6c7-d393f4845772 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_4b68c02e-cf8b-4a31-b00a-ee285cbcbbff\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison_ru_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_4b68c02e-cf8b-4a31-b00a-ee285cbcbbff button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('comparison_ru_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison_ru_df",
              "summary": "{\n  \"name\": \"comparison_ru_df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"\\u0421\\u043b\\u043e\\u0432\\u043e\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"\\u0432\\u0432\\u0435\\u043b\",\n          \"\\u041f\\u0435\\u0442\\u0440\",\n          \"\\u0420\\u043e\\u0441\\u0441\\u0438\\u0438\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0421\\u0442\\u0435\\u043c (NLTK)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"\\u0432\\u0432\\u0435\\u043b\",\n          \"\\u043f\\u0435\\u0442\\u0440\",\n          \"\\u0440\\u043e\\u0441\\u0441\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0435\\u043c\\u043c\\u0430 (spaCy)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"\\u0432\\u0432\\u0435\\u0441\\u0442\\u0438\",\n          \"\\u043f\\u0451\\u0442\\u0440\",\n          \"\\u0440\\u043e\\u0441\\u0441\\u0438\\u044f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0435\\u043c\\u043c\\u0430 (PyMorphy2)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"\\u0432\\u0432\\u0435\\u0441\\u0442\\u0438\",\n          \"\\u043f\\u0451\\u0442\\u0440\",\n          \"\\u0440\\u043e\\u0441\\u0441\\u0438\\u044f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0435\\u043c\\u043c\\u0430 (Natasha)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"\\u0432\\u0432\\u0435\\u0441\\u0442\\u0438\",\n          \"\\u043f\\u0435\\u0442\\u0440\",\n          \"\\u0440\\u043e\\u0441\\u0441\\u0438\\u044f\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Выбираем несколько интересных слов из английского текста для сравнения\n",
        "english_words = [\"States\", \"people\", \"diverse\", \"traditions\", \"beliefs\",\n",
        "                \"Americans\", \"interested\", \"cultures\", \"neighboring\", \"established\", \"corresponded\", \"traveling\"]\n",
        "\n",
        "# Создаем DataFrame для сравнения\n",
        "comparison_en = []\n",
        "for word in english_words:\n",
        "    stem = english_stemmer.stem(word.lower())\n",
        "\n",
        "    # Находим лемму в NLTK\n",
        "    pos = pos_tag([word.lower()])[0][1]\n",
        "    wordnet_pos = get_wordnet_pos(pos)\n",
        "    lemma_nltk = lemmatizer.lemmatize(word.lower(), wordnet_pos)\n",
        "\n",
        "    # Находим лемму в spaCy\n",
        "    doc = nlp_en(word)\n",
        "    for token in doc:\n",
        "        lemma_spacy = token.lemma_\n",
        "\n",
        "    comparison_en.append({\n",
        "        \"Слово\": word,\n",
        "        \"Стем (NLTK)\": stem,\n",
        "        \"Лемма (NLTK)\": lemma_nltk,\n",
        "        \"Лемма (spaCy)\": lemma_spacy\n",
        "    })\n",
        "\n",
        "# Создаем таблицу для английских слов\n",
        "comparison_en_df = pd.DataFrame(comparison_en)\n",
        "print(\"Сравнение методов для английского языка:\")\n",
        "comparison_en_df"
      ],
      "metadata": {
        "id": "dHPkDhnlqyAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 443
        },
        "outputId": "4f7c0ace-bb27-404b-c0bb-ca4504335570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Сравнение методов для английского языка:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Слово Стем (NLTK) Лемма (NLTK) Лемма (spaCy)\n",
              "0         States       state        state         state\n",
              "1         people       peopl       people        people\n",
              "2        diverse      divers      diverse       diverse\n",
              "3     traditions      tradit    tradition     tradition\n",
              "4        beliefs      belief       belief        belief\n",
              "5      Americans    american     american      american\n",
              "6     interested    interest   interested    interested\n",
              "7       cultures      cultur      culture       culture\n",
              "8    neighboring    neighbor     neighbor      neighbor\n",
              "9    established   establish    establish     establish\n",
              "10  corresponded  correspond   correspond    correspond\n",
              "11     traveling      travel       travel        travel"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-508a3a74-31c1-431d-8864-9a46e8f9e455\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Слово</th>\n",
              "      <th>Стем (NLTK)</th>\n",
              "      <th>Лемма (NLTK)</th>\n",
              "      <th>Лемма (spaCy)</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>States</td>\n",
              "      <td>state</td>\n",
              "      <td>state</td>\n",
              "      <td>state</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>people</td>\n",
              "      <td>peopl</td>\n",
              "      <td>people</td>\n",
              "      <td>people</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>diverse</td>\n",
              "      <td>divers</td>\n",
              "      <td>diverse</td>\n",
              "      <td>diverse</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>traditions</td>\n",
              "      <td>tradit</td>\n",
              "      <td>tradition</td>\n",
              "      <td>tradition</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>beliefs</td>\n",
              "      <td>belief</td>\n",
              "      <td>belief</td>\n",
              "      <td>belief</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Americans</td>\n",
              "      <td>american</td>\n",
              "      <td>american</td>\n",
              "      <td>american</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>interested</td>\n",
              "      <td>interest</td>\n",
              "      <td>interested</td>\n",
              "      <td>interested</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>cultures</td>\n",
              "      <td>cultur</td>\n",
              "      <td>culture</td>\n",
              "      <td>culture</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>neighboring</td>\n",
              "      <td>neighbor</td>\n",
              "      <td>neighbor</td>\n",
              "      <td>neighbor</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>established</td>\n",
              "      <td>establish</td>\n",
              "      <td>establish</td>\n",
              "      <td>establish</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>corresponded</td>\n",
              "      <td>correspond</td>\n",
              "      <td>correspond</td>\n",
              "      <td>correspond</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>traveling</td>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "      <td>travel</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-508a3a74-31c1-431d-8864-9a46e8f9e455')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-508a3a74-31c1-431d-8864-9a46e8f9e455 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-508a3a74-31c1-431d-8864-9a46e8f9e455');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-07f3763b-a009-4d39-9779-96970a95abce\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-07f3763b-a009-4d39-9779-96970a95abce')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-07f3763b-a009-4d39-9779-96970a95abce button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_eb359194-1e92-48f9-ac42-0371d6a8a4b9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison_en_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_eb359194-1e92-48f9-ac42-0371d6a8a4b9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('comparison_en_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison_en_df",
              "summary": "{\n  \"name\": \"comparison_en_df\",\n  \"rows\": 12,\n  \"fields\": [\n    {\n      \"column\": \"\\u0421\\u043b\\u043e\\u0432\\u043e\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"corresponded\",\n          \"established\",\n          \"States\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u0421\\u0442\\u0435\\u043c (NLTK)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"correspond\",\n          \"establish\",\n          \"state\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0435\\u043c\\u043c\\u0430 (NLTK)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"correspond\",\n          \"establish\",\n          \"state\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"\\u041b\\u0435\\u043c\\u043c\\u0430 (spaCy)\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 12,\n        \"samples\": [\n          \"correspond\",\n          \"establish\",\n          \"state\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. Комплексные пайплайны препроцессинга текста**"
      ],
      "metadata": {
        "id": "L6ti34MVq4xS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.1 Пайплайн для русского языка с PyMorphy2"
      ],
      "metadata": {
        "id": "jqmpnF2Tq7jy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pymorphy2\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "EiCoa-CmrFa7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка стоп-слов\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "\n",
        "# Инициализация анализатора\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "\n",
        "def preprocess_russian_text(text):\n",
        "    \"\"\"\n",
        "    Полный пайплайн предобработки русского текста с использованием PyMorphy2\n",
        "    \"\"\"\n",
        "    # Шаг 1: Приведение текста к нижнему регистру\n",
        "    text = text.lower()\n",
        "    print(f\"1. Приведение к нижнему регистру: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 2: Замена 'ё' на 'е'\n",
        "    text = text.replace('ё', 'е')\n",
        "    print(f\"2. Замена 'ё' на 'е': {text[:50]}...\")\n",
        "\n",
        "    # Шаг 3: Удаление цифр, знаков препинания и лишних пробелов\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)  # Удаление знаков препинания\n",
        "    text = re.sub(r'\\d+', ' ', text)      # Удаление цифр\n",
        "    text = re.sub(r'\\s+', ' ', text)      # Удаление лишних пробелов\n",
        "    text = text.strip()                    # Удаление пробелов в начале и конце\n",
        "    print(f\"3. Очистка текста: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 4: Токенизация\n",
        "    tokens = text.split()\n",
        "    print(f\"4. Токенизация: получено {len(tokens)} токенов\")\n",
        "\n",
        "    # Шаг 5: Удаление стоп-слов\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "    print(f\"5. Удаление стоп-слов: осталось {len(filtered_tokens)} токенов\")\n",
        "\n",
        "    # Шаг 6: Лемматизация с PyMorphy2\n",
        "    lemmas = []\n",
        "    for token in filtered_tokens:\n",
        "        parsed = morph.parse(token)[0]\n",
        "        lemmas.append(parsed.normal_form)\n",
        "\n",
        "    print(f\"6. Лемматизация: получено {len(lemmas)} лемм\")\n",
        "\n",
        "    # Шаг 7: Удаление слишком коротких слов\n",
        "    final_lemmas = [lemma for lemma in lemmas if len(lemma) > 2]\n",
        "    print(f\"7. Фильтрация коротких слов: итоговое количество {len(final_lemmas)} лемм\")\n",
        "\n",
        "    return final_lemmas\n",
        "\n",
        "# Применение пайплайна к русскому тексту\n",
        "print(\"Комплексный пайплайн для русского текста:\")\n",
        "preprocessed_russian = preprocess_russian_text(russian_text)\n",
        "print(\"\\nРезультат:\")\n",
        "print(preprocessed_russian)"
      ],
      "metadata": {
        "id": "uOLOXZbLq-Xz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c49128b6-63b1-4f93-edcc-51e2a7bdf6e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Комплексный пайплайн для русского текста:\n",
            "1. Приведение к нижнему регистру: \n",
            "в россии проживают народы с разными традициями и ...\n",
            "2. Замена 'ё' на 'е': \n",
            "в россии проживают народы с разными традициями и ...\n",
            "3. Очистка текста: в россии проживают народы с разными традициями и в...\n",
            "4. Токенизация: получено 39 токенов\n",
            "5. Удаление стоп-слов: осталось 31 токенов\n",
            "6. Лемматизация: получено 31 лемм\n",
            "7. Фильтрация коротких слов: итоговое количество 31 лемм\n",
            "\n",
            "Результат:\n",
            "['россия', 'проживать', 'народ', 'разный', 'традиция', 'верование', 'русский', 'человек', 'интерес', 'относиться', 'культура', 'соседний', 'народ', 'пётр', 'первый', 'ввести', 'многие', 'европейский', 'обычай', 'екатерина', 'великий', 'переписываться', 'французский', 'просветитель', 'современный', 'россиянин', 'любить', 'путешествовать', 'разный', 'страна', 'мир']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.2 Пайплайн для английского языка с spaCy"
      ],
      "metadata": {
        "id": "SiC4PNPlrQAy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "avOTgnkUrRBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Загрузка стоп-слов и модели spaCy\n",
        "english_stopwords = set(stopwords.words('english'))\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def preprocess_english_text(text):\n",
        "    \"\"\"\n",
        "    Полный пайплайн предобработки английского текста с использованием spaCy\n",
        "    \"\"\"\n",
        "    # Шаг 1: Приведение текста к нижнему регистру\n",
        "    text = text.lower()\n",
        "    print(f\"1. Приведение к нижнему регистру: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 2: Расширение сокращений (контракций)\n",
        "    text = re.sub(r\"n't\", \" not\", text)\n",
        "    text = re.sub(r\"'ve\", \" have\", text)\n",
        "    text = re.sub(r\"'re\", \" are\", text)\n",
        "    text = re.sub(r\"'m\", \" am\", text)\n",
        "    text = re.sub(r\"'ll\", \" will\", text)\n",
        "    text = re.sub(r\"'d\", \" would\", text)\n",
        "    print(f\"2. Расширение контракций: {text[:50]}...\")\n",
        "\n",
        "    # Шаг 3: Обработка текста с помощью spaCy\n",
        "    doc = nlp_en(text)\n",
        "    print(f\"3. Обработка с spaCy: получено {len(doc)} токенов\")\n",
        "\n",
        "    # Шаг 4: Фильтрация и лемматизация\n",
        "    lemmas = []\n",
        "    for token in doc:\n",
        "        # Проверяем, что токен не является стоп-словом, пунктуацией или числом\n",
        "        if (not token.is_stop and\n",
        "            not token.is_punct and\n",
        "            not token.is_digit and\n",
        "            token.is_alpha):  # Только буквенные токены\n",
        "\n",
        "            # Получаем лемму и проверяем, что она не является специальным токеном spaCy\n",
        "            lemma = token.lemma_\n",
        "            if lemma != '-PRON-':  # spaCy иногда использует -PRON- для местоимений\n",
        "                lemmas.append(lemma)\n",
        "\n",
        "    print(f\"4. Фильтрация и лемматизация: получено {len(lemmas)} лемм\")\n",
        "\n",
        "    # Шаг 5: Удаление слишком коротких слов\n",
        "    final_lemmas = [lemma for lemma in lemmas if len(lemma) > 2]\n",
        "    print(f\"5. Фильтрация коротких слов: итоговое количество {len(final_lemmas)} лемм\")\n",
        "\n",
        "    return final_lemmas\n",
        "\n",
        "# Применение пайплайна к английскому тексту\n",
        "print(\"Комплексный пайплайн для английского текста:\")\n",
        "preprocessed_english = preprocess_english_text(english_text)\n",
        "print(\"\\nРезультат:\")\n",
        "print(preprocessed_english)"
      ],
      "metadata": {
        "id": "7cbkaplkrVCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "312906cf-ebbd-4a17-874d-4afb2bfd4d2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Комплексный пайплайн для английского текста:\n",
            "1. Приведение к нижнему регистру: \n",
            "the united states is home to people with diverse ...\n",
            "2. Расширение контракций: \n",
            "the united states is home to people with diverse ...\n",
            "3. Обработка с spaCy: получено 56 токенов\n",
            "4. Фильтрация и лемматизация: получено 29 лемм\n",
            "5. Фильтрация коротких слов: итоговое количество 29 лемм\n",
            "\n",
            "Результат:\n",
            "['united', 'states', 'home', 'people', 'diverse', 'tradition', 'belief', 'americans', 'interested', 'culture', 'neighboring', 'country', 'george', 'washington', 'establish', 'governmental', 'custom', 'thomas', 'jefferson', 'correspond', 'french', 'intellectual', 'modern', 'americans', 'enjoy', 'travel', 'different', 'country', 'world']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2.3 Пайплайн для смешанного текста (русский + английский)"
      ],
      "metadata": {
        "id": "EViy6ISyraCK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок 1: Импорт необходимых библиотек и инициализация"
      ],
      "metadata": {
        "id": "UIbjAyRlCFmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "import pymorphy2\n",
        "\n",
        "# Загрузим необходимые ресурсы\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Инициализация анализаторов\n",
        "morph = pymorphy2.MorphAnalyzer()\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "english_stemmer = SnowballStemmer(\"english\")\n",
        "\n",
        "# Списки стоп-слов\n",
        "russian_stop_words = set(stopwords.words('russian'))\n",
        "english_stop_words = set(stopwords.words('english'))\n"
      ],
      "metadata": {
        "id": "cS3MzLG0BNSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Исходный текст\n",
        "mixed_text = \"\"\"\n",
        "<p>В современном мире digital-технологии стали неотъемлемой частью жизни.</p>\n",
        "<strong>Information technologies</strong> изменили способы коммуникации между людьми в 2023 году.\n",
        "CEO крупных IT-компаний (более 500 сотрудников) регулярно проводят online-конференции.\n",
        "<a href=\"https://example.com\">Developers</a> из разных стран сотрудничают в open-source проектах.\n",
        "Искусственный интеллект и <em>machine learning</em> находят применение в разных сферах.\n",
        "Для связи: contact@tech-info.com или посетите https://tech-conference.org.\n",
        "<div class=\"footer\">\n",
        "  © 2023 AI-News. Телефон: +7 (123) 456-78-90. #искусственныйинтеллект #bigdata\n",
        "</div>\n",
        "Data-scientists и ML-инженеры работают с web-приложениями и AI-моделями.\n",
        "Кибер-безопасность и e-commerce — важные направления IT-индустрии.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "M2gIZ8yTCr9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок 2: Нормализация текста"
      ],
      "metadata": {
        "id": "HWkQF_PhCMk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_mixed_text(text):\n",
        "    \"\"\"\n",
        "    Нормализует смешанный текст, удаляя HTML-теги, URL, email, специальные символы\n",
        "    и выполняя базовые преобразования для стандартизации текста.\n",
        "\n",
        "    Args:\n",
        "        text (str): Исходный смешанный текст с \"шумом\"\n",
        "\n",
        "    Returns:\n",
        "        str: Нормализованный текст\n",
        "    \"\"\"\n",
        "    # Шаг 1: Удаление HTML-тегов\n",
        "    # Находит и удаляет всё между < и >, включая скрипты и стили\n",
        "    text = re.sub(r'<[^>]+>', ' ', text)\n",
        "\n",
        "    # Шаг 2: Удаление URL-адресов\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', ' ', text)\n",
        "\n",
        "    # Шаг 3: Удаление email-адресов\n",
        "    text = re.sub(r'\\S+@\\S+\\.\\S+', ' ', text)\n",
        "\n",
        "    # Шаг 4: Удаление хештегов\n",
        "    text = re.sub(r'#\\S+', ' ', text)\n",
        "\n",
        "    # Шаг 5: Удаление номеров телефонов (различные форматы)\n",
        "    text = re.sub(r'\\+\\d+\\s*[\\(\\)]*\\s*\\d+[\\s\\-\\(\\)]*\\d+[\\s\\-\\(\\)]*\\d+', ' ', text)\n",
        "\n",
        "    # Шаг 6: Удаление символов копирайта, специальных символов\n",
        "    text = re.sub(r'[©®™℠]+', ' ', text)\n",
        "\n",
        "    # Шаг 7: Замена скобок и их содержимого на пробел\n",
        "    text = re.sub(r'\\([^)]*\\)', ' ', text)\n",
        "\n",
        "    # Шаг 8: Приведение к нижнему регистру\n",
        "    text = text.lower()\n",
        "\n",
        "    # Шаг 9: Замена буквы \"ё\" на \"е\" для унификации русского текста\n",
        "    text = text.replace('ё', 'е')\n",
        "\n",
        "    # Шаг 10: Обработка пунктуации\n",
        "    # Сохраняем дефисы внутри слов, но удаляем остальную пунктуацию\n",
        "    # Сначала заменим дефисы временным маркером\n",
        "    text = re.sub(r'(\\w)-(\\w)', r'\\1HYPHEN\\2', text)\n",
        "\n",
        "    # Удаляем пунктуацию кроме временных маркеров дефисов\n",
        "    text = re.sub(r'[^\\w\\sHYPHEN]', ' ', text)\n",
        "\n",
        "    # Возвращаем дефисы\n",
        "    text = re.sub(r'HYPHEN', '-', text)\n",
        "\n",
        "    # Шаг 11: Удаление цифр, но сохранение слов с цифрами (например, ML-инженеры5)\n",
        "    text = re.sub(r'\\b\\d+\\b', ' ', text)\n",
        "\n",
        "    # Шаг 12: Удаление избыточных пробелов (в том числе в начале и конце строк)\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Тест функции\n",
        "normalized_text = normalize_mixed_text(mixed_text)\n",
        "print(\"Нормализованный текст:\")\n",
        "print(normalized_text)"
      ],
      "metadata": {
        "id": "cfC2GfQ2Br8k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d2d1e8e-7c2c-4de2-b44a-503f251867a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Нормализованный текст:\n",
            "в современном мире digital-технологии стали неотъемлемой частью жизни information technologies изменили способы коммуникации между людьми в году ceo крупных it-компаний регулярно проводят online-конференции developers из разных стран сотрудничают в open-source проектах искусственный интеллект и machine learning находят применение в разных сферах для связи или посетите ai-news телефон data-scientists и ml-инженеры работают с web-приложениями и ai-моделями кибер-безопасность и e-commerce важные направления it-индустрии\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок 3: Определение алфавита символа и токенизация"
      ],
      "metadata": {
        "id": "vMtLsyCMDZ9G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_cyrillic(char):\n",
        "    \"\"\"Проверяет, является ли символ кириллическим.\"\"\"\n",
        "    return 'а' <= char <= 'я' or char == 'ё'\n",
        "\n",
        "def is_latin(char):\n",
        "    \"\"\"Проверяет, является ли символ латинским.\"\"\"\n",
        "    return 'a' <= char <= 'z'\n",
        "\n",
        "def classify_token(token):\n",
        "    \"\"\"Определяет язык токена: 'russian', 'english' или 'mixed'.\"\"\"\n",
        "    if not token:\n",
        "        return None\n",
        "\n",
        "    # Удаляем все не-буквенные символы для определения языка\n",
        "    letters = [c for c in token.lower() if c.isalpha()]\n",
        "    if not letters:\n",
        "        return None\n",
        "\n",
        "    cyrillic_count = sum(1 for c in letters if is_cyrillic(c))\n",
        "    latin_count = sum(1 for c in letters if is_latin(c))\n",
        "\n",
        "    if cyrillic_count > 0 and latin_count == 0:\n",
        "        return 'russian'\n",
        "    elif latin_count > 0 and cyrillic_count == 0:\n",
        "        return 'english'\n",
        "    elif cyrillic_count > 0 and latin_count > 0:\n",
        "        return 'mixed'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def split_hyphenated_words(token):\n",
        "    \"\"\"Разбивает слова с дефисом, если они состоят из смешанных символов.\"\"\"\n",
        "    if '-' not in token:\n",
        "        return [token]\n",
        "\n",
        "    # Проверяем, есть ли смешанные символы в токене\n",
        "    if classify_token(token) != 'mixed':\n",
        "        return [token]\n",
        "\n",
        "    # Разбиваем по дефису\n",
        "    parts = token.split('-')\n",
        "    result = []\n",
        "\n",
        "    for part in parts:\n",
        "        if part:  # Игнорируем пустые части\n",
        "            result.append(part)\n",
        "\n",
        "    return result\n",
        "\n",
        "def tokenize_and_classify(text):\n",
        "    \"\"\"Токенизирует текст и классифицирует токены по языку.\"\"\"\n",
        "    # Сначала разделим текст на слова, сохраняя дефисы внутри слов\n",
        "    raw_tokens = re.findall(r'\\b[\\w\\-]+\\b', text)\n",
        "\n",
        "    russian_tokens = []\n",
        "    english_tokens = []\n",
        "\n",
        "    for token in raw_tokens:\n",
        "        # Проверяем, нужно ли разделить слово с дефисом\n",
        "        split_tokens = split_hyphenated_words(token)\n",
        "\n",
        "        for t in split_tokens:\n",
        "            lang = classify_token(t)\n",
        "            if lang == 'russian':\n",
        "                russian_tokens.append(t)\n",
        "            elif lang == 'english':\n",
        "                english_tokens.append(t)\n",
        "\n",
        "    return russian_tokens, english_tokens\n",
        "\n",
        "# Применяем токенизацию и классификацию\n",
        "russian_tokens, english_tokens = tokenize_and_classify(normalized_text)\n",
        "\n",
        "print(\"\\nРусские токены:\")\n",
        "print(russian_tokens)\n",
        "print(\"\\nАнглийские токены:\")\n",
        "print(english_tokens)"
      ],
      "metadata": {
        "id": "0fKwh812DcQO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7ceef5c9-7cbc-4f23-dc37-976adb8f99d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Русские токены:\n",
            "['в', 'современном', 'мире', 'технологии', 'стали', 'неотъемлемой', 'частью', 'жизни', 'изменили', 'способы', 'коммуникации', 'между', 'людьми', 'в', 'году', 'крупных', 'компаний', 'регулярно', 'проводят', 'конференции', 'из', 'разных', 'стран', 'сотрудничают', 'в', 'проектах', 'искусственный', 'интеллект', 'и', 'находят', 'применение', 'в', 'разных', 'сферах', 'для', 'связи', 'или', 'посетите', 'телефон', 'и', 'инженеры', 'работают', 'с', 'приложениями', 'и', 'моделями', 'кибер-безопасность', 'и', 'важные', 'направления', 'индустрии']\n",
            "\n",
            "Английские токены:\n",
            "['digital', 'information', 'technologies', 'ceo', 'it', 'online', 'developers', 'open-source', 'machine', 'learning', 'ai-news', 'data-scientists', 'ml', 'web', 'ai', 'e-commerce', 'it']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок 4: Обработка токенов по языковым правилам"
      ],
      "metadata": {
        "id": "GaZMugJFDtHQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_russian_tokens(tokens):\n",
        "    \"\"\"Обрабатывает русские токены по пайплайну для русского языка.\"\"\"\n",
        "    # Фильтрация стоп-слов\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stop_words]\n",
        "\n",
        "    # Лемматизация с помощью pymorphy2\n",
        "    lemmatized_tokens = []\n",
        "    for token in filtered_tokens:\n",
        "        # Получаем нормальную форму\n",
        "        lemma = morph.parse(token)[0].normal_form\n",
        "        lemmatized_tokens.append(lemma)\n",
        "\n",
        "    return lemmatized_tokens\n",
        "\n",
        "def process_english_tokens(tokens):\n",
        "    \"\"\"Обрабатывает английские токены по пайплайну для английского языка.\"\"\"\n",
        "    # Обработка сокращений и апострофов\n",
        "    expanded_tokens = []\n",
        "    for token in tokens:\n",
        "        # Обработка притяжательных форм и сокращений\n",
        "        token = re.sub(r'\\'s$|\\'$', '', token)  # Удаляем 's и '\n",
        "        expanded_tokens.append(token)\n",
        "\n",
        "    # Фильтрация стоп-слов\n",
        "    filtered_tokens = [token for token in expanded_tokens if token not in english_stop_words]\n",
        "\n",
        "    # Стемматизация\n",
        "    stemmed_tokens = [english_stemmer.stem(token) for token in filtered_tokens]\n",
        "\n",
        "    return stemmed_tokens\n",
        "\n",
        "# Обработка токенов по языковым правилам\n",
        "processed_russian = process_russian_tokens(russian_tokens)\n",
        "processed_english = process_english_tokens(english_tokens)\n",
        "\n",
        "print(\"\\nОбработанные русские токены:\")\n",
        "print(processed_russian)\n",
        "print(\"\\nОбработанные английские токены:\")\n",
        "print(processed_english)"
      ],
      "metadata": {
        "id": "_Fon7yaBDtuL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890de968-fe2e-4fde-87e6-1576fec33e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Обработанные русские токены:\n",
            "['современный', 'мир', 'технология', 'стать', 'неотъемлемый', 'часть', 'жизнь', 'изменить', 'способ', 'коммуникация', 'человек', 'год', 'крупный', 'компания', 'регулярно', 'проводить', 'конференция', 'разный', 'страна', 'сотрудничать', 'проект', 'искусственный', 'интеллект', 'находить', 'применение', 'разный', 'сфера', 'связь', 'посетить', 'телефон', 'инженер', 'работать', 'приложение', 'модель', 'кибер-безопасность', 'важный', 'направление', 'индустрия']\n",
            "\n",
            "Обработанные английские токены:\n",
            "['digit', 'inform', 'technolog', 'ceo', 'onlin', 'develop', 'open-sourc', 'machin', 'learn', 'ai-new', 'data-scientist', 'ml', 'web', 'ai', 'e-commerc']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Блок 5: Финальная обработка и объединение результатов"
      ],
      "metadata": {
        "id": "RrZFA-rID2XX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def final_processing(russian_tokens, english_tokens):\n",
        "    \"\"\"Выполняет постобработку и объединяет результаты.\"\"\"\n",
        "    # Фильтрация коротких слов (менее 3 символов)\n",
        "    filtered_russian = [token for token in russian_tokens if len(token) >= 3]\n",
        "    filtered_english = [token for token in english_tokens if len(token) >= 3]\n",
        "\n",
        "    # Удаление дубликатов\n",
        "    unique_russian = list(dict.fromkeys(filtered_russian))\n",
        "    unique_english = list(dict.fromkeys(filtered_english))\n",
        "\n",
        "    return {\n",
        "        'russian_tokens': unique_russian,\n",
        "        'english_tokens': unique_english,\n",
        "        'stats': {\n",
        "            'russian_count': len(unique_russian),\n",
        "            'english_count': len(unique_english),\n",
        "            'total_unique': len(unique_russian) + len(unique_english)\n",
        "        }\n",
        "    }\n",
        "\n",
        "# Финальная обработка\n",
        "result = final_processing(processed_russian, processed_english)\n",
        "\n",
        "print(\"\\nФинальный результат:\")\n",
        "print(\"Русские слова:\", result['russian_tokens'])\n",
        "print(\"Английские слова:\", result['english_tokens'])\n",
        "print(\"\\nСтатистика:\")\n",
        "print(f\"Количество русских слов: {result['stats']['russian_count']}\")\n",
        "print(f\"Количество английских слов: {result['stats']['english_count']}\")\n",
        "print(f\"Общее количество уникальных слов: {result['stats']['total_unique']}\")"
      ],
      "metadata": {
        "id": "ulvjX8hDD4LJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b0e750e-4239-424c-c8da-29715df0704b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Финальный результат:\n",
            "Русские слова: ['современный', 'мир', 'технология', 'стать', 'неотъемлемый', 'часть', 'жизнь', 'изменить', 'способ', 'коммуникация', 'человек', 'год', 'крупный', 'компания', 'регулярно', 'проводить', 'конференция', 'разный', 'страна', 'сотрудничать', 'проект', 'искусственный', 'интеллект', 'находить', 'применение', 'сфера', 'связь', 'посетить', 'телефон', 'инженер', 'работать', 'приложение', 'модель', 'кибер-безопасность', 'важный', 'направление', 'индустрия']\n",
            "Английские слова: ['digit', 'inform', 'technolog', 'ceo', 'onlin', 'develop', 'open-sourc', 'machin', 'learn', 'ai-new', 'data-scientist', 'web', 'e-commerc']\n",
            "\n",
            "Статистика:\n",
            "Количество русских слов: 37\n",
            "Количество английских слов: 13\n",
            "Общее количество уникальных слов: 50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Практические упражнения:**"
      ],
      "metadata": {
        "id": "JwJWCho_Eebo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 1: Базовая стемматизация русского текста с помощью NLTK"
      ],
      "metadata": {
        "id": "xThTr52IEvhr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание:\n",
        "Напишите функцию, которая принимает русский текст, выполняет токенизацию и применяет стемматизацию с использованием SnowballStemmer из библиотеки NLTK. Функция должна возвращать список стемматизированных слов.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Используйте SnowballStemmer(\"russian\") из NLTK\n",
        "* Удалите все знаки препинания перед стемматизацией\n",
        "* Приведите слова к нижнему регистру\n",
        "* Выведите исходные слова и их стеммы в виде таблицы для сравнения"
      ],
      "metadata": {
        "id": "8b-U9dWlEyUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "\n",
        "russian_stemmer = SnowballStemmer(\"russian\")\n",
        "\n",
        "def stem_russian_text(text, stemmer):\n",
        "    # Удаление знаков препинания\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    # Нижний регистр и токенизация\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    # Стемматизация\n",
        "    stemmed_words = [stemmer.stem(word) for word in words]\n",
        "\n",
        "    df = pd.DataFrame({\n",
        "        'Исходное слово': words,\n",
        "        'Стемма': stemmed_words\n",
        "    })\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    return stemmed_words\n",
        "\n",
        "# Текст для проверки\n",
        "russian_text = \"\"\"\n",
        "Программирование - это искусство создания программ для компьютеров.\n",
        "Программисты разрабатывают различные приложения, которые используются миллионами людей.\n",
        "Хорошие разработчики всегда учатся новому и совершенствуют свои навыки программирования.\n",
        "\"\"\"\n",
        "# Стемматизация русского текста\n",
        "russian_stemmed = stem_russian_text(russian_text, russian_stemmer)"
      ],
      "metadata": {
        "id": "kG7vf-0jE6US",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ab7cb65-6702-4402-89b4-861cb6d0238f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Исходное слово         Стемма\n",
            "программирование программирован\n",
            "             это             эт\n",
            "       искусство       искусств\n",
            "        создания         создан\n",
            "        программ       программ\n",
            "             для            для\n",
            "     компьютеров      компьютер\n",
            "    программисты    программист\n",
            "   разрабатывают    разрабатыва\n",
            "       различные        различн\n",
            "      приложения       приложен\n",
            "         которые          котор\n",
            "    используются        использ\n",
            "      миллионами        миллион\n",
            "           людей            люд\n",
            "         хорошие          хорош\n",
            "    разработчики    разработчик\n",
            "          всегда          всегд\n",
            "          учатся           учат\n",
            "          новому            нов\n",
            "               и              и\n",
            "  совершенствуют    совершенств\n",
            "            свои            сво\n",
            "          навыки          навык\n",
            "программирования программирован\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 2: Сравнение стемматизации и лемматизации английского текста"
      ],
      "metadata": {
        "id": "rHBKG099FGa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание: Реализуйте две функции, которые обрабатывают один и тот же английский текст с использованием PorterStemmer и WordNetLemmatizer из NLTK соответственно. Сравните результаты и определите, какой метод лучше сохраняет семантику слов.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Для стемматизации используйте PorterStemmer из NLTK\n",
        "* Для лемматизации используйте WordNetLemmatizer с определением части речи\n",
        "* Создайте таблицу сравнения из трех колонок: исходное слово, стемма, лемма\n",
        "* Выделите случаи, где результаты стемматизации и лемматизации существенно различаются"
      ],
      "metadata": {
        "id": "bRCilNwJFJKu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus import wordnet\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "id": "CNkbfkB4JJiK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Функция для конвертации POS-тегов NLTK в формат WordNet\n",
        "def get_wordnet_pos(tag):\n",
        "    if tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN  # По умолчанию существительное\n",
        "\n",
        "#Инициализация лемматизатора и стемматизатора\n",
        "porter = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def stem_text(text, stemmer):\n",
        "    # Стемматизация\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "    stems = [stemmer.stem(word) for word in words]\n",
        "    return words, stems\n",
        "\n",
        "def lemmatize_text(text, lemmatizer):\n",
        "    # Лемматизация\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator).lower()\n",
        "    tokens = word_tokenize(text)\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
        "    return tokens, lemmas\n",
        "\n",
        "# Текст для проверки\n",
        "english_text = \"\"\"\n",
        "Natural language processing helps computers understand and generate human language.\n",
        "Researchers are developing better models that can recognize patterns in texts.\n",
        "The running applications are processing huge amounts of data daily.\n",
        "Many universities are teaching students how to build better language models.\n",
        "\"\"\"\n",
        "orig_words, stems = stem_text(english_text, porter)\n",
        "orig_words2, lemmas = lemmatize_text(english_text, lemmatizer)\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'Исходное слово': orig_words,\n",
        "    'Стемма': stems,\n",
        "    'Лемма': lemmas\n",
        "})\n",
        "df['Есть существенные различия?'] = df['Стемма'] != df['Лемма']\n",
        "print(df.to_string(index=False))"
      ],
      "metadata": {
        "id": "uuAXLHprFaqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44e7fe3d-f33f-4f6e-efec-d606388d2db7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Исходное слово     Стемма       Лемма  Есть существенные различия?\n",
            "       natural      natur     natural                         True\n",
            "      language    languag    language                         True\n",
            "    processing    process  processing                         True\n",
            "         helps       help        help                        False\n",
            "     computers     comput    computer                         True\n",
            "    understand understand  understand                        False\n",
            "           and        and         and                        False\n",
            "      generate      gener    generate                         True\n",
            "         human      human       human                        False\n",
            "      language    languag    language                         True\n",
            "   researchers   research  researcher                         True\n",
            "           are        are          be                         True\n",
            "    developing    develop     develop                        False\n",
            "        better     better        good                         True\n",
            "        models      model       model                        False\n",
            "          that       that        that                        False\n",
            "           can        can         can                        False\n",
            "     recognize     recogn   recognize                         True\n",
            "      patterns    pattern     pattern                        False\n",
            "            in         in          in                        False\n",
            "         texts       text        text                        False\n",
            "           the        the         the                        False\n",
            "       running        run     running                         True\n",
            "  applications     applic application                         True\n",
            "           are        are          be                         True\n",
            "    processing    process     process                        False\n",
            "          huge       huge        huge                        False\n",
            "       amounts     amount      amount                        False\n",
            "            of         of          of                        False\n",
            "          data       data        data                        False\n",
            "         daily      daili       daily                         True\n",
            "          many       mani        many                         True\n",
            "  universities    univers  university                         True\n",
            "           are        are          be                         True\n",
            "      teaching      teach       teach                        False\n",
            "      students    student     student                        False\n",
            "           how        how         how                        False\n",
            "            to         to          to                        False\n",
            "         build      build       build                        False\n",
            "        better     better        good                         True\n",
            "      language    languag    language                         True\n",
            "        models      model       model                        False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 3: Лемматизация русского текста с использованием pymorphy2 и определение частей речи"
      ],
      "metadata": {
        "id": "JS5vnJXZFhPl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание: Напишите функцию, которая выполняет морфологический анализ русского текста с помощью библиотеки pymorphy2. Функция должна возвращать для каждого слова его лемму (нормальную форму) и часть речи.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Используйте pymorphy2.MorphAnalyzer()\n",
        "* Обработайте токенизированный текст, игнорируя пунктуацию и числа\n",
        "* Для каждого слова определите нормальную форму и часть речи\n",
        "* Посчитайте частоту встречаемости каждой части речи в тексте\n",
        "* Найдите слова, имеющие омонимичные разборы, и предложите способ разрешения неоднозначности"
      ],
      "metadata": {
        "id": "EW0VZZMaFiBQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pymorphy3"
      ],
      "metadata": {
        "id": "O_67i2WNuaid"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import string\n",
        "import pandas as pd\n",
        "import pymorphy3\n",
        "from collections import Counter\n",
        "\n",
        "# Инициализация анализатора\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "\n",
        "def lemmatize_with_pymorphy(text):\n",
        "    # Очистка текста\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator).lower()\n",
        "\n",
        "    # Токенизация\n",
        "    words = re.findall(r'\\b[а-яё]+\\b', text)\n",
        "\n",
        "    # Подсчёт частоты слов\n",
        "    word_counts = Counter(words)\n",
        "\n",
        "    # Анализ каждого слова\n",
        "    analysis_results = []\n",
        "    for word in words:\n",
        "        parses = morph.parse(word)\n",
        "        best_parse = parses[0]\n",
        "        analysis_results.append({\n",
        "            'Слово': word,\n",
        "            'Лемма': best_parse.normal_form,\n",
        "            'Часть речи': best_parse.tag.POS,\n",
        "            'Количество разборов': len(parses),\n",
        "            'Частота в тексте': word_counts[word]\n",
        "        })\n",
        "\n",
        "    # Таблица\n",
        "    df = pd.DataFrame(analysis_results)\n",
        "\n",
        "    # Частота частей речи\n",
        "    pos_counts = Counter(df['Часть речи'])\n",
        "\n",
        "    # Поиск омонимичных слов\n",
        "    homonyms = df[df['Количество разборов'] > 1]['Слово'].unique()\n",
        "\n",
        "    # Вывод\n",
        "    print(\"Лемматизированные русские слова:\")\n",
        "    for i, lemma in enumerate(df['Лемма'][:15], 1):\n",
        "        print(f\"{i}. {lemma}\")\n",
        "\n",
        "    print(\"\\nМорфологический анализ:\\n\")\n",
        "    print(df.to_string(index=False))\n",
        "\n",
        "    print(\"\\nЧастота частей речи:\")\n",
        "    for pos, count in pos_counts.items():\n",
        "        print(f\"{pos}: {count}\")\n",
        "\n",
        "    return df, pos_counts\n",
        "\n",
        "# Текст для проверки\n",
        "russian_text = \"\"\"\n",
        "Машинное обучение становится всё более популярным в современном мире.\n",
        "Компании используют алгоритмы машинного обучения для анализа данных и принятия решений.\n",
        "Инженеры по данным обучают модели на больших наборах данных, чтобы повысить их точность.\n",
        "Эти технологии меняют нашу жизнь, делая её более удобной и эффективной.\n",
        "\"\"\"\n",
        "df, pos_counts = lemmatize_with_pymorphy(russian_text)"
      ],
      "metadata": {
        "id": "e9TfNGJrFtDl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cf2781a-bb44-4c31-e84b-daaa23e422f8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные русские слова:\n",
            "1. машинный\n",
            "2. обучение\n",
            "3. становиться\n",
            "4. всё\n",
            "5. более\n",
            "6. популярный\n",
            "7. в\n",
            "8. современный\n",
            "9. мир\n",
            "10. компания\n",
            "11. использовать\n",
            "12. алгоритм\n",
            "13. машинный\n",
            "14. обучение\n",
            "15. для\n",
            "\n",
            "Морфологический анализ:\n",
            "\n",
            "      Слово        Лемма Часть речи  Количество разборов  Частота в тексте\n",
            "   машинное     машинный       ADJF                    2                 1\n",
            "   обучение     обучение       NOUN                    2                 1\n",
            " становится  становиться       VERB                    1                 1\n",
            "        всё          всё       PRCL                    3                 1\n",
            "      более        более       ADVB                    1                 2\n",
            " популярным   популярный       ADJF                    3                 1\n",
            "          в            в       PREP                   13                 1\n",
            "современном  современный       ADJF                    2                 1\n",
            "       мире          мир       NOUN                    2                 1\n",
            "   компании     компания       NOUN                    5                 1\n",
            " используют использовать       VERB                    2                 1\n",
            "  алгоритмы     алгоритм       NOUN                    2                 1\n",
            "  машинного     машинный       ADJF                    3                 1\n",
            "   обучения     обучение       NOUN                    3                 1\n",
            "        для          для       PREP                    2                 1\n",
            "    анализа       анализ       NOUN                    1                 1\n",
            "     данных       данные       NOUN                    8                 2\n",
            "          и            и       CONJ                   15                 2\n",
            "   принятия     принятие       NOUN                    3                 1\n",
            "    решений      решение       NOUN                    1                 1\n",
            "   инженеры      инженер       NOUN                    1                 1\n",
            "         по           по       PREP                   19                 1\n",
            "     данным       данные       NOUN                    7                 1\n",
            "    обучают      обучать       VERB                    1                 1\n",
            "     модели       модель       NOUN                    9                 1\n",
            "         на           на       PREP                    3                 1\n",
            "    больших      больший       ADJF                    6                 1\n",
            "    наборах        набор       NOUN                    1                 1\n",
            "     данных       данные       NOUN                    8                 2\n",
            "      чтобы        чтобы       CONJ                    2                 1\n",
            "   повысить     повысить       INFN                    1                 1\n",
            "         их          они       NPRO                   26                 1\n",
            "   точность     точность       NOUN                    2                 1\n",
            "        эти         этот       ADJF                    2                 1\n",
            " технологии   технология       NOUN                    5                 1\n",
            "     меняют       менять       VERB                    1                 1\n",
            "       нашу          наш       ADJF                    1                 1\n",
            "      жизнь        жизнь       NOUN                    2                 1\n",
            "      делая       делать       GRND                    1                 1\n",
            "         её           её       ADJF                   26                 1\n",
            "      более        более       ADVB                    1                 2\n",
            "    удобной      удобный       ADJF                    4                 1\n",
            "          и            и       CONJ                   15                 2\n",
            "эффективной  эффективный       ADJF                    4                 1\n",
            "\n",
            "Частота частей речи:\n",
            "ADJF: 10\n",
            "NOUN: 17\n",
            "VERB: 4\n",
            "PRCL: 1\n",
            "ADVB: 2\n",
            "PREP: 4\n",
            "CONJ: 3\n",
            "INFN: 1\n",
            "NPRO: 1\n",
            "GRND: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 4: Обработка смешанного русско-английского текста с использованием spaCy"
      ],
      "metadata": {
        "id": "3Gh_zSekF1EL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание: Разработайте систему для обработки смешанного русско-английского текста с использованием библиотеки spaCy. Ваша программа должна определять язык каждого слова, применять соответствующую модель для лемматизации и выполнять частеречную разметку.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Загрузите модели en_core_web_sm и ru_core_news_sm из spaCy\n",
        "* Реализуйте алгоритм определения языка для каждого токена (можно использовать множества символов кириллицы и латиницы)\n",
        "* Обработайте каждый токен соответствующей языковой моделью\n",
        "* Сформируйте два отдельных набора данных: лемматизированный русский и английский текст\n",
        "* Определите наиболее часто встречающиеся леммы для каждого языка"
      ],
      "metadata": {
        "id": "fHzPZi7bF1sB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n",
        "!python -m spacy download ru_core_news_sm"
      ],
      "metadata": {
        "id": "D3iMQcu6t6AP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa883091-8e81-4d58-d4a2-927581e9a259",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.19.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.3.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.3)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m100.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ru-core-news-sm==3.8.0) (2.0.6)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "Installing collected packages: ru-core-news-sm\n",
            "Successfully installed ru-core-news-sm-3.8.0\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "\n",
        "# Загрузка моделей spaCy\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")\n",
        "\n",
        "def detect_language(word):\n",
        "    if re.match(r'^[а-яА-ЯёЁ]+$', word):\n",
        "        return 'ru'\n",
        "    elif re.match(r'^[a-zA-Z]+$', word):\n",
        "        return 'en'\n",
        "    else:\n",
        "        return 'other'\n",
        "\n",
        "def lemmatize_mixed_text(text, nlp_en, nlp_ru):\n",
        "    # Токенизация\n",
        "    tokens = re.findall(r'\\b[а-яА-ЯёЁa-zA-Z]+\\b', text)\n",
        "\n",
        "    results = []\n",
        "    ru_words, en_words = [], []\n",
        "\n",
        "    for token in tokens:\n",
        "        lang = detect_language(token)\n",
        "        if lang == 'ru':\n",
        "            ru_words.append(token)\n",
        "        elif lang == 'en':\n",
        "            en_words.append(token)\n",
        "        results.append({'Слово': token, 'Язык': lang})\n",
        "\n",
        "    # Обработка каждого языка своей моделью\n",
        "    ru_doc = nlp_ru(\" \".join(ru_words))\n",
        "    en_doc = nlp_en(\" \".join(en_words))\n",
        "\n",
        "    # Леммы и части речи\n",
        "    ru_data = [{'Слово': token.text, 'Лемма': token.lemma_, 'Часть речи': token.pos_}\n",
        "               for token in ru_doc] if ru_doc else []\n",
        "    en_data = [{'Слово': token.text, 'Лемма': token.lemma_, 'Часть речи': token.pos_}\n",
        "               for token in en_doc] if en_doc else []\n",
        "\n",
        "    # Таблицы\n",
        "    df_ru = pd.DataFrame(ru_data)\n",
        "    df_en = pd.DataFrame(en_data)\n",
        "\n",
        "    # Подсчёт частоты лемм\n",
        "    ru_lemma_counts = Counter(df_ru['Лемма'])\n",
        "    en_lemma_counts = Counter(df_en['Лемма'])\n",
        "\n",
        "    # Вывод\n",
        "    print(\"Лемматизированный английский текст:\")\n",
        "    print(df_en.to_string(index=False))\n",
        "\n",
        "    print(\"\\nЛемматизированный русский текст:\")\n",
        "    print(df_ru.to_string(index=False))\n",
        "\n",
        "    print(\"\\nНаиболее частые русские леммы:\")\n",
        "    for lemma, count in ru_lemma_counts.most_common(5):\n",
        "        print(f\"{lemma}: {count}\")\n",
        "\n",
        "    print(\"\\nНаиболее частые английские леммы:\")\n",
        "    for lemma, count in en_lemma_counts.most_common(5):\n",
        "        print(f\"{lemma}: {count}\")\n",
        "\n",
        "    return df_ru, df_en, ru_lemma_counts, en_lemma_counts\n",
        "\n",
        "# Текст для проверки\n",
        "mixed_text = \"\"\"\n",
        "Data science становится ключевой компетенцией в современном IT-мире.\n",
        "Специалисты в области data engineering играют важную роль в создании инфраструктуры для обработки и хранения больших объемов информации.\n",
        "Machine learning инженеры и аналитики данных востребованы на рынке труда.\n",
        "Компании активно внедряют artificial intelligence решения для оптимизации бизнес-процессов.\n",
        "Работодатели ищут специалистов, владеющих языком программирования Python и имеющих опыт работы с big data.\n",
        "\"\"\"\n",
        "df_ru, df_en, ru_lemma_counts, en_lemma_counts = lemmatize_mixed_text(mixed_text, nlp_en, nlp_ru)"
      ],
      "metadata": {
        "id": "0g_p-Y8KF-fh",
        "outputId": "d0896b6e-e867-470c-cec1-db1f3c526dda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный английский текст:\n",
            "       Слово        Лемма Часть речи\n",
            "        Data         data       NOUN\n",
            "     science      science       NOUN\n",
            "          IT           IT      PROPN\n",
            "        data        datum       NOUN\n",
            " engineering     engineer       VERB\n",
            "     Machine      machine       NOUN\n",
            "    learning        learn       VERB\n",
            "  artificial   artificial        ADJ\n",
            "intelligence intelligence       NOUN\n",
            "      Python       Python      PROPN\n",
            "         big          big        ADJ\n",
            "        data        datum       NOUN\n",
            "\n",
            "Лемматизированный русский текст:\n",
            "           Слово            Лемма Часть речи\n",
            "      становится      становиться       VERB\n",
            "        ключевой         ключевой        ADJ\n",
            "    компетенцией      компетенция       NOUN\n",
            "               в                в        ADP\n",
            "     современном      современный        ADJ\n",
            "            мире              мир       NOUN\n",
            "     Специалисты       специалист       NOUN\n",
            "               в                в        ADP\n",
            "         области          область       NOUN\n",
            "          играют           играть       VERB\n",
            "          важную           важный        ADJ\n",
            "            роль             роль       NOUN\n",
            "               в                в        ADP\n",
            "        создании         создание       NOUN\n",
            "  инфраструктуры   инфраструктура       NOUN\n",
            "             для              для        ADP\n",
            "       обработки        обработка       NOUN\n",
            "               и                и      CCONJ\n",
            "        хранения         хранение       NOUN\n",
            "         больших          больший        ADJ\n",
            "         объемов            объём       NOUN\n",
            "      информации       информация       NOUN\n",
            "        инженеры          инженер       NOUN\n",
            "               и                и      CCONJ\n",
            "       аналитики         аналитик       NOUN\n",
            "          данных           данных       NOUN\n",
            "    востребованы     востребовать       VERB\n",
            "              на               на        ADP\n",
            "           рынке            рынок       NOUN\n",
            "           труда             труд       NOUN\n",
            "        Компании         компания      PROPN\n",
            "         активно          активно        ADV\n",
            "        внедряют         внедрять       VERB\n",
            "         решения          решение       NOUN\n",
            "             для              для        ADP\n",
            "     оптимизации      оптимизация       NOUN\n",
            "          бизнес           бизнес       NOUN\n",
            "       процессов          процесс       NOUN\n",
            "    Работодатели     работодатель       NOUN\n",
            "            ищут           искать       VERB\n",
            "    специалистов       специалист       NOUN\n",
            "       владеющих          владеть       VERB\n",
            "          языком             язык       NOUN\n",
            "программирования программирование       NOUN\n",
            "               и                и      CCONJ\n",
            "         имеющих            иметь       VERB\n",
            "            опыт             опыт       NOUN\n",
            "          работы           работа       NOUN\n",
            "               с                с        ADP\n",
            "\n",
            "Наиболее частые русские леммы:\n",
            "в: 3\n",
            "и: 3\n",
            "специалист: 2\n",
            "для: 2\n",
            "становиться: 1\n",
            "\n",
            "Наиболее частые английские леммы:\n",
            "datum: 2\n",
            "data: 1\n",
            "science: 1\n",
            "IT: 1\n",
            "engineer: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Упражнение 5*: Применение библиотеки Natasha для анализа именованных сущностей и лемматизации русского текста"
      ],
      "metadata": {
        "id": "sLVbeZHcGGK7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Задание: Используйте библиотеку Natasha для выделения именованных сущностей (NER) и лемматизации русского текста. Создайте функцию, которая принимает текст, выделяет и классифицирует в нём именованные сущности (имена людей, организации, локации и т.д.), а также выполняет лемматизацию всех слов.\n",
        "\n",
        "Требования:\n",
        "\n",
        "* Используйте Segmenter, MorphVocab, NewsEmbedding, NewsMorphTagger, NewsSyntaxParser, NewsNERTagger из библиотеки Natasha\n",
        "* Выделите все именованные сущности в тексте и определите их тип\n",
        "* Выполните лемматизацию всех слов, учитывая контекст (используя синтаксический анализ)\n",
        "* Сформируйте структурированный результат, включающий: список всех именованных сущностей с указанием их типа, список всех слов с их леммами, статистику по типам найденных сущностей"
      ],
      "metadata": {
        "id": "fs2q0cZOGLR7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "\n",
        "# Текст для проверки\n",
        "\n",
        "russian_ner_text = \"\"\"\n",
        "Президент России Владимир Путин провел совещание с правительством в Москве.\n",
        "Компания Яндекс представила новую технологию для анализа данных.\n",
        "Сотрудники Московского государственного университета и Института проблем искусственного интеллекта РАН\n",
        "разработали алгоритм для обработки естественного языка.\n",
        "Исследователи из Санкт-Петербурга и их коллеги из Казани опубликовали результаты исследований\n",
        "в журнале \"Компьютерная лингвистика и интеллектуальные технологии\".\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "vkD1Mfj-GdAX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}