{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 0. Загрузите необходимые библиотеки**"
      ],
      "metadata": {
        "id": "VL_YISip2qbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk spacy pymorphy2 natasha\n",
        "!python -m spacy download ru_core_news_sm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import spacy\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "nltk.download('stopwords')\n",
        "russian_stopwords = set(stopwords.words('russian'))\n",
        "nlp_ru = spacy.load(\"ru_core_news_sm\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "slYHHNxbw2_i",
        "outputId": "b83942c8-118e-4001-97a0-b99aad218f7b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.11)\n",
            "Requirement already satisfied: pymorphy2 in /usr/local/lib/python3.12/dist-packages (0.9.1)\n",
            "Requirement already satisfied: natasha in /usr/local/lib/python3.12/dist-packages (1.6.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.15)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.13)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.10)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.2)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.3)\n",
            "Requirement already satisfied: typer-slim<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.12.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: dawg-python>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (0.7.2)\n",
            "Requirement already satisfied: pymorphy2-dicts-ru<3.0,>=2.4 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (2.4.417127.4579844)\n",
            "Requirement already satisfied: docopt>=0.6 in /usr/local/lib/python3.12/dist-packages (from pymorphy2) (0.6.2)\n",
            "Requirement already satisfied: razdel>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from natasha) (0.5.0)\n",
            "Requirement already satisfied: navec>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from natasha) (0.10.0)\n",
            "Requirement already satisfied: slovnet>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from natasha) (0.6.0)\n",
            "Requirement already satisfied: yargy>=0.16.0 in /usr/local/lib/python3.12/dist-packages (from natasha) (0.16.0)\n",
            "Requirement already satisfied: ipymarkup>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from natasha) (0.9.0)\n",
            "Requirement already satisfied: intervaltree>=3 in /usr/local/lib/python3.12/dist-packages (from ipymarkup>=0.8.0->natasha) (3.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.41.4)\n",
            "Requirement already satisfied: typing-extensions>=4.14.1 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.11.12)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.3)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.4.2->spacy) (7.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.12/dist-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.4.2->spacy) (2.0.1)\n",
            "Collecting ru-core-news-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_sm-3.8.0/ru_core_news_sm-3.8.0-py3-none-any.whl (15.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.3/15.3 MB\u001b[0m \u001b[31m46.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pymorphy3>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ru-core-news-sm==3.8.0) (2.0.6)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in /usr/local/lib/python3.12/dist-packages (from pymorphy3>=1.0.0->ru-core-news-sm==3.8.0) (75.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 1. Загрузка данных**"
      ],
      "metadata": {
        "id": "0YUM4z_92XzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вставьте текст для обработки согласно вашему варианту"
      ],
      "metadata": {
        "id": "hwqI1lLF2j3t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"\"\"Человеческий мозг — удивительный \"компьютер\", хранящий около 2,5 петабайт информации (это ≈3 млн часов видео в HD-качестве)!\n",
        "\n",
        "Процесс формирования воспоминаний включает 3 стадии: кодирование, консолидацию и извлечение.\n",
        "\n",
        "Гиппокамп играет ключевую роль в переводе кратковременной памяти в долговременную.\n",
        "\n",
        "\"Memory consolidation происходит преимущественно во время сна, когда мозг «перезаписывает» дневные впечатления,\" — объясняет доктор Миронова.\n",
        "\n",
        "В 2022 году эксперимент с использованием технологии optogenetics позволил учёным MIT \"включать\" и \"выключать\" конкретные воспоминания у мышей!\n",
        "\n",
        "Нейромедиатор ацетилхолин критически важен для работы памяти — его дефицит связан с болезнью Альцгеймера.\n",
        "\n",
        "Современные исследования фокусируются на разработке методов усиления памяти с помощью brain-computer interfaces.\n",
        "\n",
        "Представьте: скоро мы сможем загружать информацию прямо в мозг, как в фильме \"Матрица\" (1999)!\"\"\""
      ],
      "metadata": {
        "id": "B2Tsy8T62ZRo"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 2. Нормализация текста.**"
      ],
      "metadata": {
        "id": "NYmTHvNm2ph6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Приведите текст к нижнему регистру, удалите лишние пробелы, переносы строк, спецсимволы, пунктуацию, обработайте цифры."
      ],
      "metadata": {
        "id": "XgLHFLVp3Apg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "import re\n",
        "text = text.lower()\n",
        "text = re.sub(r'\\s+', ' ', text)\n",
        "text = re.sub(r'\\d+', ' ', text)\n",
        "text = re.sub(r'\\n', ' ', text)\n",
        "text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "text = re.sub(r'[^\\w\\s]', '', text)\n",
        "text = \" \".join(text.split())"
      ],
      "metadata": {
        "id": "KiyJLZrm3Z8s"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 3. Токенизация**"
      ],
      "metadata": {
        "id": "I6evm-5d3dPD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Токенизируйте текст."
      ],
      "metadata": {
        "id": "O90JD4L83gbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "tokens = text.split()"
      ],
      "metadata": {
        "id": "QjGIQM3F3udR"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 4. Удаление стоп-слов**"
      ],
      "metadata": {
        "id": "x0ED6dCL3vrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Выведите 2 списка - 1. Очищенных токенов, 2. Список удаленных стоп-слов"
      ],
      "metadata": {
        "id": "awJNjV8y30zn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "stop_words = [token for token in tokens if token in russian_stopwords]\n",
        "print(f'Очищенные токены:\\n{filtered_tokens}')\n",
        "print(f'\\nУдаленные стоп-слова:\\n{stop_words}')"
      ],
      "metadata": {
        "id": "c9uFV9dG39L2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c077a1e-77c5-437b-8854-5213bdd26b8f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Очищенные токены:\n",
            "['человеческий', 'мозг', 'удивительный', 'компьютер', 'хранящий', 'около', 'петабайт', 'информации', 'это', 'млн', 'часов', 'видео', 'hd', 'качестве', 'процесс', 'формирования', 'воспоминаний', 'включает', 'стадии', 'кодирование', 'консолидацию', 'извлечение', 'гиппокамп', 'играет', 'ключевую', 'роль', 'переводе', 'кратковременной', 'памяти', 'долговременную', 'memory', 'consolidation', 'происходит', 'преимущественно', 'время', 'сна', 'мозг', 'перезаписывает', 'дневные', 'впечатления', 'объясняет', 'доктор', 'миронова', 'году', 'эксперимент', 'использованием', 'технологии', 'optogenetics', 'позволил', 'учёным', 'mit', 'включать', 'выключать', 'конкретные', 'воспоминания', 'мышей', 'нейромедиатор', 'ацетилхолин', 'критически', 'важен', 'работы', 'памяти', 'дефицит', 'связан', 'болезнью', 'альцгеймера', 'современные', 'исследования', 'фокусируются', 'разработке', 'методов', 'усиления', 'памяти', 'помощью', 'brain', 'computer', 'interfaces', 'представьте', 'скоро', 'сможем', 'загружать', 'информацию', 'прямо', 'мозг', 'фильме', 'матрица']\n",
            "\n",
            "Удаленные стоп-слова:\n",
            "['в', 'и', 'в', 'в', 'во', 'когда', 'в', 'с', 'и', 'у', 'для', 'его', 'с', 'на', 'с', 'мы', 'в', 'как', 'в']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 5. Лемматизация и стемминг**"
      ],
      "metadata": {
        "id": "sbV8P56O3-ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Примените к токенам алгоритмы лемматизации и стемминга. Выведите 2 списка - 1. Лемматизированные токены 2. Стемматизированные токены"
      ],
      "metadata": {
        "id": "OZ1A8VhK4HTk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "new_tokens = ' '.join(filtered_tokens)\n",
        "doc = nlp_ru(new_tokens)\n",
        "lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "\n",
        "stemmer = SnowballStemmer(\"russian\")\n",
        "words = re.findall(r'\\b\\w+\\b', new_tokens)\n",
        "stems = [stemmer.stem(word) for word in words]\n",
        "print(f'Лемматизированные токены:\\n{lemmas}')\n",
        "print(f'\\nСтемматизированнные токены:\\n{stems}')"
      ],
      "metadata": {
        "id": "OrCmA4tt4WH6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9c385bd-90e8-4755-b8dd-2967131fe561"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированные токены:\n",
            "['человеческий', 'мозг', 'удивительный', 'компьютер', 'хранить', 'около', 'петабайт', 'информация', 'это', 'млн', 'час', 'видео', 'hd', 'качество', 'процесс', 'формирование', 'воспоминание', 'включать', 'стадия', 'кодирование', 'консолидация', 'извлечение', 'гиппокамп', 'играть', 'ключевой', 'роль', 'переводе', 'кратковременный', 'память', 'долговременный', 'memory', 'consolidation', 'происходить', 'преимущественно', 'время', 'сон', 'мозг', 'перезаписывать', 'дневный', 'впечатление', 'объяснять', 'доктор', 'миронов', 'году', 'эксперимент', 'использование', 'технология', 'optogenetics', 'позволить', 'учёный', 'mit', 'включать', 'выключать', 'конкретный', 'воспоминание', 'мышь', 'нейромедиатор', 'ацетилхолин', 'критически', 'важный', 'работа', 'память', 'дефицит', 'связать', 'болезнь', 'альцгеймера', 'современный', 'исследование', 'фокусироваться', 'разработка', 'метод', 'усиление', 'память', 'помощь', 'brain', 'computer', 'interfaces', 'представить', 'скоро', 'смочь', 'загружать', 'информация', 'прямо', 'мозг', 'фильм', 'матрица']\n",
            "\n",
            "Стемматизированнные токены:\n",
            "['человеческ', 'мозг', 'удивительн', 'компьютер', 'храня', 'окол', 'петабайт', 'информац', 'эт', 'млн', 'час', 'виде', 'hd', 'качеств', 'процесс', 'формирован', 'воспоминан', 'включа', 'стад', 'кодирован', 'консолидац', 'извлечен', 'гиппокамп', 'игра', 'ключев', 'рол', 'перевод', 'кратковремен', 'памят', 'долговремен', 'memory', 'consolidation', 'происход', 'преимуществен', 'врем', 'сна', 'мозг', 'перезаписыва', 'дневн', 'впечатлен', 'объясня', 'доктор', 'миронов', 'год', 'эксперимент', 'использован', 'технолог', 'optogenetics', 'позвол', 'учен', 'mit', 'включа', 'выключа', 'конкретн', 'воспоминан', 'мыш', 'нейромедиатор', 'ацетилхолин', 'критическ', 'важ', 'работ', 'памят', 'дефиц', 'связа', 'болезн', 'альцгеймер', 'современ', 'исследован', 'фокусир', 'разработк', 'метод', 'усилен', 'памят', 'помощ', 'brain', 'computer', 'interfaces', 'представьт', 'скор', 'смож', 'загружа', 'информац', 'прям', 'мозг', 'фильм', 'матриц']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Задание 6. Напишите функцию для препроцессинга текста**"
      ],
      "metadata": {
        "id": "R_R2xPrh4bW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Объедините все шаги в одну функцию. Выведите результат с лемматизированным списком"
      ],
      "metadata": {
        "id": "drJR_Rff4j_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ваш код\n",
        "def preprocessing(text):\n",
        "    # Нормализация текста.\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = re.sub(r'\\d+', ' ', text)\n",
        "    text = re.sub(r'\\n', ' ', text)\n",
        "    text = re.sub(r'[^\\w\\s]', ' ', text)\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Токенизация\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Удаление стоп-слов\n",
        "    filtered_tokens = [token for token in tokens if token not in russian_stopwords]\n",
        "\n",
        "    # Лемматизация\n",
        "    new_tokens = ' '.join(filtered_tokens)\n",
        "    doc = nlp_ru(new_tokens)\n",
        "    lemmas = [token.lemma_ for token in doc if token.is_alpha]\n",
        "    print(f'Лемматизированный список токенов:\\n{lemmas}')\n",
        "\n",
        "preprocessing(text)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "RUE3jJp940iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1645d5a1-7ab0-4e28-e8f3-cba0f4f99013"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Лемматизированный список токенов:\n",
            "['человеческий', 'мозг', 'удивительный', 'компьютер', 'хранить', 'около', 'петабайт', 'информация', 'это', 'млн', 'час', 'видео', 'hd', 'качество', 'процесс', 'формирование', 'воспоминание', 'включать', 'стадия', 'кодирование', 'консолидация', 'извлечение', 'гиппокамп', 'играть', 'ключевой', 'роль', 'переводе', 'кратковременный', 'память', 'долговременный', 'memory', 'consolidation', 'происходить', 'преимущественно', 'время', 'сон', 'мозг', 'перезаписывать', 'дневный', 'впечатление', 'объяснять', 'доктор', 'миронов', 'году', 'эксперимент', 'использование', 'технология', 'optogenetics', 'позволить', 'учёный', 'mit', 'включать', 'выключать', 'конкретный', 'воспоминание', 'мышь', 'нейромедиатор', 'ацетилхолин', 'критически', 'важный', 'работа', 'память', 'дефицит', 'связать', 'болезнь', 'альцгеймера', 'современный', 'исследование', 'фокусироваться', 'разработка', 'метод', 'усиление', 'память', 'помощь', 'brain', 'computer', 'interfaces', 'представить', 'скоро', 'смочь', 'загружать', 'информация', 'прямо', 'мозг', 'фильм', 'матрица']\n"
          ]
        }
      ]
    }
  ]
}