{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "owKq2QMu-_V-"
      },
      "source": [
        "Эта тетрадка содержит примеры и упражнения по двум основным методам векторизации текста: Bag of Words (мешок слов) и TF-IDF (Term Frequency-Inverse Document Frequency)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "C-tsQpvKAdW9"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WEJQsdB__X3v"
      },
      "source": [
        "**Часть 1: Исходные данные**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "d4n2kaiB-y9C"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Документ 1: Машинное обучение - это подраздел искусственного интеллекта.\n",
            "Документ 2: Нейронные сети широко используются в машинном обучении.\n",
            "Документ 3: Глубокое обучение основано на многослойных нейронных сетях.\n",
            "Документ 4: Искусственный интеллект имитирует когнитивные функции человека.\n",
            "Документ 5: Компьютерное зрение и обработка языка - примеры применения ИИ.\n"
          ]
        }
      ],
      "source": [
        "# Наш корпус документов\n",
        "documents = [\n",
        "    \"Машинное обучение - это подраздел искусственного интеллекта.\",\n",
        "    \"Нейронные сети широко используются в машинном обучении.\",\n",
        "    \"Глубокое обучение основано на многослойных нейронных сетях.\",\n",
        "    \"Искусственный интеллект имитирует когнитивные функции человека.\",\n",
        "    \"Компьютерное зрение и обработка языка - примеры применения ИИ.\"\n",
        "]\n",
        "\n",
        "# Вывод документов с номерами\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"Документ {i}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tApzmP2y_80E"
      },
      "source": [
        "**Часть 2: Предобработка**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "h8f1OVNaAAeP"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Предобработанные документы:\n",
            "Документ 1: машинное обучение  это подраздел искусственного интеллекта\n",
            "Документ 2: нейронные сети широко используются в машинном обучении\n",
            "Документ 3: глубокое обучение основано на многослойных нейронных сетях\n",
            "Документ 4: искусственный интеллект имитирует когнитивные функции человека\n",
            "Документ 5: компьютерное зрение и обработка языка  примеры применения ии\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(documents)):\n",
        "    # Приведение к нижнему регистру\n",
        "    documents[i] = documents[i].lower()\n",
        "    # Удаление пунктуации\n",
        "    documents[i] = re.sub(r'[^\\w\\s]', '', documents[i])\n",
        "\n",
        "  # Вывод предобработанных документов\n",
        "print(\"Предобработанные документы:\")\n",
        "for i, doc in enumerate(documents, 1):\n",
        "    print(f\"Документ {i}: {doc}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XakwSM_Z_dqJ"
      },
      "source": [
        "**Часть 3: Модель Bag of Words (мешок слов)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "2pdp6lFOB3mC"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting scikit-learn\n",
            "  Downloading scikit_learn-1.8.0-cp312-cp312-win_amd64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: numpy>=1.24.1 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from scikit-learn) (2.3.4)\n",
            "Collecting scipy>=1.10.0 (from scikit-learn)\n",
            "  Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
            "Requirement already satisfied: joblib>=1.3.0 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from scikit-learn) (1.5.2)\n",
            "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
            "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
            "Downloading scikit_learn-1.8.0-cp312-cp312-win_amd64.whl (8.0 MB)\n",
            "   ---------------------------------------- 0.0/8.0 MB ? eta -:--:--\n",
            "   --- ------------------------------------ 0.8/8.0 MB 5.6 MB/s eta 0:00:02\n",
            "   --------- ------------------------------ 1.8/8.0 MB 5.3 MB/s eta 0:00:02\n",
            "   ------------------ --------------------- 3.7/8.0 MB 7.3 MB/s eta 0:00:01\n",
            "   -------------------------------- ------- 6.6/8.0 MB 8.8 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 8.0/8.0 MB 9.0 MB/s eta 0:00:00\n",
            "Downloading scipy-1.16.3-cp312-cp312-win_amd64.whl (38.6 MB)\n",
            "   ---------------------------------------- 0.0/38.6 MB ? eta -:--:--\n",
            "   -- ------------------------------------- 2.4/38.6 MB 12.2 MB/s eta 0:00:03\n",
            "   ---- ----------------------------------- 4.7/38.6 MB 11.9 MB/s eta 0:00:03\n",
            "   ------- -------------------------------- 6.8/38.6 MB 11.0 MB/s eta 0:00:03\n",
            "   --------- ------------------------------ 9.2/38.6 MB 11.0 MB/s eta 0:00:03\n",
            "   ----------- ---------------------------- 11.5/38.6 MB 11.1 MB/s eta 0:00:03\n",
            "   -------------- ------------------------- 13.9/38.6 MB 11.2 MB/s eta 0:00:03\n",
            "   ---------------- ----------------------- 16.0/38.6 MB 10.9 MB/s eta 0:00:03\n",
            "   ------------------ --------------------- 18.1/38.6 MB 10.8 MB/s eta 0:00:02\n",
            "   -------------------- ------------------- 20.2/38.6 MB 10.6 MB/s eta 0:00:02\n",
            "   ----------------------- ---------------- 22.3/38.6 MB 10.5 MB/s eta 0:00:02\n",
            "   ------------------------- -------------- 24.4/38.6 MB 10.5 MB/s eta 0:00:02\n",
            "   --------------------------- ------------ 26.7/38.6 MB 10.5 MB/s eta 0:00:02\n",
            "   ----------------------------- ---------- 28.3/38.6 MB 10.3 MB/s eta 0:00:01\n",
            "   ------------------------------- -------- 30.4/38.6 MB 10.3 MB/s eta 0:00:01\n",
            "   --------------------------------- ------ 32.8/38.6 MB 10.3 MB/s eta 0:00:01\n",
            "   ------------------------------------ --- 34.9/38.6 MB 10.3 MB/s eta 0:00:01\n",
            "   -------------------------------------- - 37.2/38.6 MB 10.4 MB/s eta 0:00:01\n",
            "   ---------------------------------------- 38.6/38.6 MB 10.2 MB/s eta 0:00:00\n",
            "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
            "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
            "Successfully installed scikit-learn-1.8.0 scipy-1.16.3 threadpoolctl-3.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install scikit-learn\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Для создания Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PHP7yxKQCEpb"
      },
      "source": [
        "3.1 Создание матрицы Bag of Words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "scdI5kc4B4-E"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Матрица Bag of Words:\n",
            "            глубокое  зрение  ии  имитирует  интеллект  интеллекта  \\\n",
            "Документ 1         0       0   0          0          0           1   \n",
            "Документ 2         0       0   0          0          0           0   \n",
            "Документ 3         1       0   0          0          0           0   \n",
            "Документ 4         0       0   0          1          1           0   \n",
            "Документ 5         0       1   1          0          0           0   \n",
            "\n",
            "            искусственного  искусственный  используются  когнитивные  ...  \\\n",
            "Документ 1               1              0             0            0  ...   \n",
            "Документ 2               0              0             1            0  ...   \n",
            "Документ 3               0              0             0            0  ...   \n",
            "Документ 4               0              1             0            1  ...   \n",
            "Документ 5               0              0             0            0  ...   \n",
            "\n",
            "            подраздел  применения  примеры  сети  сетях  функции  человека  \\\n",
            "Документ 1          1           0        0     0      0        0         0   \n",
            "Документ 2          0           0        0     1      0        0         0   \n",
            "Документ 3          0           0        0     0      1        0         0   \n",
            "Документ 4          0           0        0     0      0        1         1   \n",
            "Документ 5          0           1        1     0      0        0         0   \n",
            "\n",
            "            широко  это  языка  \n",
            "Документ 1       0    1      0  \n",
            "Документ 2       1    0      0  \n",
            "Документ 3       0    0      0  \n",
            "Документ 4       0    0      0  \n",
            "Документ 5       0    0      1  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Создание векторизатора\n",
        "count_vectorizer = CountVectorizer()\n",
        "\n",
        "# Обучение векторизатора и преобразование документов\n",
        "bow_matrix = count_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Получение списка фичей (слов)\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "# Преобразование разреженной матрицы в плотную для наглядности\n",
        "bow_df = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=feature_names,\n",
        "    index=[f'Документ {i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "# Вывод матрицы Bag of Words\n",
        "print(\"Матрица Bag of Words:\")\n",
        "print(bow_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMK9JL-1C4Dn"
      },
      "source": [
        "Получение списка фичей (слов)\n",
        "\n",
        "feature_names = count_vectorizer.get_feature_names_out()\n",
        "Что происходит:\n",
        "Этот шаг извлекает из обученного CountVectorizer все уникальные слова (или n-граммы), которые были использованы при создании матрицы Bag of Words.\n",
        "\n",
        "Зачем это нужно:\n",
        "* Интерпретация результатов: Без этого шага у нас была бы только числовая матрица, но мы не знали бы, какой столбец соответствует какому слову.\n",
        "* Понимание словаря: Метод get_feature_names_out() возвращает список всех уникальных терминов, которые векторизатор извлек из документов и включил в свой словарь.\n",
        "* Подготовка к визуализации: Имена признаков необходимы для создания понятной таблицы с метками строк и столбцов."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MtVs8JuDTt6"
      },
      "source": [
        "**Часть 4: Модель TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TyAEZkC_Dmrx"
      },
      "source": [
        "4.1 Создание матрицы TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "HUpUVp_iD2xf"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "nauTaaSvDonR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Матрица TF-IDF:\n",
            "            глубокое    зрение        ии  имитирует  интеллект  интеллекта  \\\n",
            "Документ 1  0.000000  0.000000  0.000000   0.000000   0.000000    0.420669   \n",
            "Документ 2  0.000000  0.000000  0.000000   0.000000   0.000000    0.000000   \n",
            "Документ 3  0.387757  0.000000  0.000000   0.000000   0.000000    0.000000   \n",
            "Документ 4  0.000000  0.000000  0.000000   0.408248   0.408248    0.000000   \n",
            "Документ 5  0.000000  0.377964  0.377964   0.000000   0.000000    0.000000   \n",
            "\n",
            "            искусственного  искусственный  используются  когнитивные  ...  \\\n",
            "Документ 1        0.420669       0.000000      0.000000     0.000000  ...   \n",
            "Документ 2        0.000000       0.000000      0.408248     0.000000  ...   \n",
            "Документ 3        0.000000       0.000000      0.000000     0.000000  ...   \n",
            "Документ 4        0.000000       0.408248      0.000000     0.408248  ...   \n",
            "Документ 5        0.000000       0.000000      0.000000     0.000000  ...   \n",
            "\n",
            "            подраздел  применения   примеры      сети     сетях   функции  \\\n",
            "Документ 1   0.420669    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Документ 2   0.000000    0.000000  0.000000  0.408248  0.000000  0.000000   \n",
            "Документ 3   0.000000    0.000000  0.000000  0.000000  0.387757  0.000000   \n",
            "Документ 4   0.000000    0.000000  0.000000  0.000000  0.000000  0.408248   \n",
            "Документ 5   0.000000    0.377964  0.377964  0.000000  0.000000  0.000000   \n",
            "\n",
            "            человека    широко       это     языка  \n",
            "Документ 1  0.000000  0.000000  0.420669  0.000000  \n",
            "Документ 2  0.000000  0.408248  0.000000  0.000000  \n",
            "Документ 3  0.000000  0.000000  0.000000  0.000000  \n",
            "Документ 4  0.408248  0.000000  0.000000  0.000000  \n",
            "Документ 5  0.000000  0.000000  0.000000  0.377964  \n",
            "\n",
            "[5 rows x 31 columns]\n"
          ]
        }
      ],
      "source": [
        "# Создание TF-IDF векторизатора\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Обучение векторизатора и преобразование документов\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(documents)\n",
        "\n",
        "# Преобразование в DataFrame\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_vectorizer.get_feature_names_out(),\n",
        "    index=[f'Документ {i+1}' for i in range(len(documents))]\n",
        ")\n",
        "\n",
        "# Вывод матрицы TF-IDF\n",
        "print(\"Матрица TF-IDF:\")\n",
        "print(tfidf_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "keRydTjVHBdq"
      },
      "source": [
        "**Часть 5. Сравнение результатов**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "SjuAChaKPomZ"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "GAbzPK8YL_KR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== Топ-3 слова по всей коллекции ===\n",
            "\n",
            "Топ-3 слова по BOW (самые частые в коллекции):\n",
            "- обучение: 2 раз\n",
            "- глубокое: 1 раз\n",
            "- зрение: 1 раз\n",
            "\n",
            "Топ-3 слова по TF-IDF (с наибольшим весом по всей коллекции):\n",
            "- обучение: 0.6522\n",
            "- интеллекта: 0.4207\n",
            "- искусственного: 0.4207\n",
            "\n",
            "Сравнение топ-3 слов:\n",
            "Общие слова в обоих топ-3: обучение\n",
            "Только в BOW топ-3: глубокое, зрение\n",
            "Только в TF-IDF топ-3: интеллекта, искусственного\n"
          ]
        }
      ],
      "source": [
        "# Суммируем значения по всем документам для BOW\n",
        "# Для BOW просто суммируем частоты слов по всем документам\n",
        "bow_sum = np.sum(bow_matrix.toarray(), axis=0)\n",
        "\n",
        "# Создаем словарь слово -> суммарная частота\n",
        "word_bow_dict = dict(zip(feature_names, bow_sum))\n",
        "\n",
        "# Находим топ-3 слова по BOW (самые частотные в коллекции)\n",
        "top_bow_words = sorted(word_bow_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "print(\"=== Топ-3 слова по всей коллекции ===\\n\")\n",
        "print(\"Топ-3 слова по BOW (самые частые в коллекции):\")\n",
        "for word, count in top_bow_words:\n",
        "    print(f\"- {word}: {count} раз\")\n",
        "\n",
        "# Для TF-IDF нужно суммировать значения по всем документам\n",
        "tfidf_sum = np.sum(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "# Создаем словарь слово -> суммарный TF-IDF вес\n",
        "word_tfidf_dict = dict(zip(feature_names, tfidf_sum))\n",
        "\n",
        "# Находим топ-3 слова по TF-IDF (с наибольшим суммарным весом по всей коллекции)\n",
        "top_tfidf_words = sorted(word_tfidf_dict.items(), key=lambda x: x[1], reverse=True)[:3]\n",
        "\n",
        "print(\"\\nТоп-3 слова по TF-IDF (с наибольшим весом по всей коллекции):\")\n",
        "for word, score in top_tfidf_words:\n",
        "    print(f\"- {word}: {score:.4f}\")\n",
        "\n",
        "# Показываем разницу между списками\n",
        "print(\"\\nСравнение топ-3 слов:\")\n",
        "bow_words = [word for word, _ in top_bow_words]\n",
        "tfidf_words = [word for word, _ in top_tfidf_words]\n",
        "\n",
        "common_words = set(bow_words) & set(tfidf_words)\n",
        "bow_only = set(bow_words) - common_words\n",
        "tfidf_only = set(tfidf_words) - common_words\n",
        "\n",
        "if common_words:\n",
        "    print(f\"Общие слова в обоих топ-3: {', '.join(common_words)}\")\n",
        "if bow_only:\n",
        "    print(f\"Только в BOW топ-3: {', '.join(bow_only)}\")\n",
        "if tfidf_only:\n",
        "    print(f\"Только в TF-IDF топ-3: {', '.join(tfidf_only)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ycGpAdTzQHs_"
      },
      "source": [
        "**Самостоятельная работа**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLOsSo-xQO_i"
      },
      "source": [
        "ЗАДАНИЕ:\n",
        "1. Предобработайте эти документы (удалите стоп-слова, приведите к нижнему регистру)\n",
        "2. Создайте матрицу Bag of Words\n",
        "3. Создайте матрицу TF-IDF\n",
        "4. Найдите и выведите топ-3 важных слова для каждого документа по Bag of Words\n",
        "5. Найдите и выведите топ-3 важных слова для каждого документа по TF-IDF\n",
        "6. Проанализируйте разницу между результатами и объясните её\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "csxiQZNhQWHb"
      },
      "outputs": [],
      "source": [
        "# Создаем коллекцию документов\n",
        "exercise_documents = [\n",
        "    \"Япония поражает сочетанием древних традиций и футуристических технологий. Токио с его небоскребами и неоновыми огнями соседствует с тихими храмами и садами. Весной цветение сакуры превращает страну в розовое облако. Японская кухня, от суши до рамена, предлагает уникальные вкусовые ощущения. Синкансэны позволяют комфортно перемещаться между городами.\",\n",
        "\n",
        "    \"Исландия — страна потрясающих природных контрастов. Ледники соседствуют с действующими вулканами, а горячие гейзеры бьют среди снежных равнин. Северное сияние зимой и незаходящее солнце летом создают ощущение другой планеты. Голубая лагуна с ее геотермальными водами — идеальное место для расслабления после долгих пеших походов по национальным паркам.\",\n",
        "\n",
        "    \"Таиланд привлекает путешественников белоснежными пляжами и кристально чистой водой. Острова Пхукет и Самуи предлагают роскошные курорты и активный отдых. Бангкок поражает контрастами: золотые храмы соседствуют с оживленными рынками и современными торговыми центрами. Тайская кухня с ее острыми ароматами и экзотическими фруктами — отдельное гастрономическое путешествие.\",\n",
        "\n",
        "    \"Италия — настоящий музей под открытым небом. Рим хранит наследие древней империи в Колизее и Форуме. Венеция очаровывает каналами и гондолами. Флоренция — сокровищница искусства эпохи Возрождения. Побережье Амальфи и озеро Комо предлагают живописные пейзажи. Итальянская кухня, от пасты до джелато, заслуженно считается одной из лучших в мире.\",\n",
        "\n",
        "    \"Перу хранит тайны древних цивилизаций. Мачу-Пикчу, затерянный город инков, привлекает туристов со всего мира. Линии Наска, гигантские рисунки на плато, до сих пор остаются загадкой. Озеро Титикака поражает своими плавучими островами, на которых живут местные племена. Перуанская кухня, с ее свежими морепродуктами и разнообразием картофеля, переживает всемирное признание.\",\n",
        "\n",
        "    \"Марокко — это калейдоскоп красок и ароматов. Медины Феса и Марракеша с их узкими улочками и шумными базарами погружают в атмосферу арабских сказок. Пустыня Сахара предлагает незабываемые ночи под звездами в берберских лагерях. Атласские горы привлекают любителей трекинга. Марокканская кухня славится тажинами и кус-кусом, приправленными экзотическими специями.\",\n",
        "\n",
        "    \"Новая Зеландия — рай для любителей природы и активного отдыха. Фьорды Милфорд-Саунд, ледники Южных Альп и гейзеры Роторуа демонстрируют разнообразие ландшафтов. Хоббитон, декорации из фильмов «Властелин колец», привлекают поклонников Толкина. Маори, коренное население, сохраняет свою уникальную культуру. Адреналиновые развлечения, от банджи-джампинга до рафтинга, доступны по всей стране.\",\n",
        "\n",
        "    \"Кения предлагает классическое африканское сафари. Масаи-Мара, Амбосели и Цаво — национальные парки, где можно увидеть «большую пятерку» африканских животных в их естественной среде. Ежегодная миграция антилоп гну — одно из самых впечатляющих природных зрелищ. Пляжи Момбасы с коралловыми рифами идеальны для дайвинга и снорклинга. Племена масаи и самбуру сохраняют традиционный образ жизни.\",\n",
        "\n",
        "    \"Вьетнам сочетает богатую историю и динамичное настоящее. Бухта Халонг с ее карстовыми островами — природное чудо. Хойан очаровывает древними улочками и бумажными фонариками. Дельта Меконга предлагает возможность познакомиться с сельской жизнью. Вьетнамская кухня, от фо до свежих спринг-роллов, покоряет своей свежестью. Система туннелей Ку-Чи напоминает о недавней войне.\",\n",
        "\n",
        "    \"Антарктида — последний неосвоенный континент, привлекающий самых отважных путешественников. Круизы из Ушуаи позволяют увидеть айсберги, пингвинов и китов. Пересечение пролива Дрейка — настоящее испытание для морских путешественников. Исследовательские станции разных стран ведут научную работу в суровых условиях. Полуночное солнце летом создает сюрреалистичные пейзажи ледяной пустыни.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Топ-3 слова по Bag of Words:\n",
            "\n",
            "Документ 1: [('цветение', 1), ('городами', 1), ('уникальные', 1)]\n",
            "Документ 2: [('северное', 1), ('создают', 1), ('равнин', 1)]\n",
            "Документ 3: [('чистой', 1), ('отдых', 1), ('отдельное', 1)]\n",
            "Документ 4: [('возрождения', 1), ('древней', 1), ('джелато', 1)]\n",
            "Документ 5: [('своими', 1), ('переживает', 1), ('поражает', 1)]\n",
            "Документ 6: [('сказок', 1), ('базарами', 1), ('шумными', 1)]\n",
            "Документ 7: [('стране', 1), ('сохраняет', 1), ('банджиджампинга', 1)]\n",
            "Документ 8: [('африканское', 1), ('среде', 1), ('пятерку', 1)]\n",
            "Документ 9: [('свежих', 1), ('сочетает', 1), ('предлагает', 1)]\n",
            "Документ 10: [('путешественников', 2), ('пересечение', 1), ('пейзажи', 1)]\n",
            "\n",
            "Топ-3 слова по TF-IDF:\n",
            "\n",
            "Документ 1: [('цветение', 0.175), ('городами', 0.175), ('уникальные', 0.175)]\n",
            "Документ 2: [('северное', 0.171), ('создают', 0.171), ('равнин', 0.171)]\n",
            "Документ 3: [('острыми', 0.173), ('оживленными', 0.173), ('храмы', 0.173)]\n",
            "Документ 4: [('возрождения', 0.169), ('древней', 0.169), ('джелато', 0.169)]\n",
            "Документ 5: [('своими', 0.164), ('переживает', 0.164), ('город', 0.164)]\n",
            "Документ 6: [('сказок', 0.17), ('базарами', 0.17), ('шумными', 0.17)]\n",
            "Документ 7: [('стране', 0.165), ('сохраняет', 0.165), ('банджиджампинга', 0.165)]\n",
            "Документ 8: [('африканское', 0.162), ('среде', 0.162), ('пятерку', 0.162)]\n",
            "Документ 9: [('свежих', 0.165), ('сочетает', 0.165), ('возможность', 0.165)]\n",
            "Документ 10: [('путешественников', 0.276), ('пересечение', 0.162), ('исследовательские', 0.162)]\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = set(stopwords.words('russian'))\n",
        "\n",
        "for i in range(len(exercise_documents)):\n",
        "    exercise_documents[i] = exercise_documents[i].lower()\n",
        "    exercise_documents[i] = re.sub(r'[^\\w\\s]', '', exercise_documents[i])\n",
        "\n",
        "exercise_documents = [\n",
        "    \" \".join([word for word in doc.split() if word not in stop_words])\n",
        "    for doc in exercise_documents\n",
        "]\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_matrix = count_vectorizer.fit_transform(exercise_documents)\n",
        "bow_feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "print(\"\\nТоп-3 слова по Bag of Words:\\n\")\n",
        "\n",
        "for doc_idx in range(bow_matrix.shape[0]):\n",
        "    bow_values = bow_matrix[doc_idx].toarray()[0]\n",
        "    top_indices = np.argsort(bow_values)[-3:][::-1]\n",
        "\n",
        "    top_words = [(bow_feature_names[i], int(bow_values[i]))\n",
        "                 for i in top_indices if bow_values[i] > 0]\n",
        "    print(f\"Документ {doc_idx + 1}: {top_words}\")\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(exercise_documents)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "def tfidf_top3_words(tfidf_matrix, feature_names, top_n=3):\n",
        "    print(\"\\nТоп-3 слова по TF-IDF:\\n\")\n",
        "\n",
        "    for doc_idx in range(tfidf_matrix.shape[0]):\n",
        "        tfidf_values = tfidf_matrix[doc_idx].toarray()[0]\n",
        "        top_indices = np.argsort(tfidf_values)[-top_n:][::-1]\n",
        "\n",
        "        top_words = [(feature_names[i], round(float(tfidf_values[i]), 3))\n",
        "                     for i in top_indices if tfidf_values[i] > 0]\n",
        "        \n",
        "        print(f\"Документ {doc_idx + 1}: {top_words}\")\n",
        "\n",
        "tfidf_top3_words(tfidf_matrix, tfidf_feature_names)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
