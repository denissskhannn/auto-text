{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76va5BKKNKcx"
      },
      "source": [
        "**Задание 1: Векторизация текста с использованием Мешка слов (BoW) и TF-IDF**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EFPwzZZ4NMIT"
      },
      "source": [
        "1. Создайте корпус из минимум 10 текстовых документов (можно взять новостные статьи, отзывы на товары, статьи по одной тематике).\n",
        "2. Реализуйте предобработку текста, включающую:\n",
        "* Приведение к нижнему регистру\n",
        "* Токенизацию\n",
        "* Удаление стоп-слов и пунктуации\n",
        "* Лемматизацию/стемминг\n",
        "3. Реализуйте модель Мешка слов (BoW) с использованием CountVectorizer из scikit-learn.\n",
        "4. Реализуйте модель TF-IDF с использованием TfidfVectorizer из scikit-learn.\n",
        "5. Найдите 10 самых значимых терминов для каждого документа по обоим подходам и сравните результаты.\n",
        "6. Визуализируйте сходство документов с помощью метрики косинусного расстояния для обоих подходов.\n",
        "7. Прокомментируйте разницу в результатах между BoW и TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7q9vo0RuMxPX"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Предобработанные синопсисы:\n",
            "1: реальн жизн редк дел черн бел люд исключительн плох хорош дорам поведа истор двух совершен разн люд ëл плох парен потеря вер человечеств счита отличн полицейск успешн раскрыва одн дел друг однак встреч полн противоположн праведн человек скрыва псевдоним ке творя ужасн вещ полност меня ход событ ëл получ лиш аттестат средн образован однак помеша харизматичн обаятельн молод человек дослуж полиц ведущ сотрудник отдел борьб распространен запрещен веществ однак движет геро стремлен справедлив мужчин амбициоз предпочита закрыва глаз преступлен раскрыт котор способн принест продвижен карьерн лестниц одобрен начальств сам момент горизонт замаяч долгожда повышен перспективн мест город объяв сумасшедш преступник светл будущ стал ускольза ëл гот выйт троп войн стал разобра враг котор помеша план случ интерес обо пересекут стремлен одн то цел\n",
            "2: сюжет дорам мыш раскрыва захватыва истор действ котор происход мир котор люд могут идентифицирова психопат помощ днктест плод утроб матер чон рым честн добр новичок полицейск веря справедлив столкновен серийн убийцейпсихопат чьи безжалостн убийств навел ужас стран жизн полност меня опытн детект родител котор убит тех пор цел жизн найт убийц отомст смерт родител поимк преступник гот прибегнут люб средств забот закон правил бон проблемн старшеклассниц живущ бабушк разбира боев искусств кажд сталкива соседомполицейск чон рым начина забавн препира друг друг чхве хон чжу талантлив режиссерпродюсер известн шерлок хон чжу такж хран секрет котор детств связа убийц\n",
            "3: молод парен чон род небольш деревн получа предложен работ сво друг сеул упуска так возможн гер собира пак чемода девушк давн живет работа сует больш город цен намн выш привычн оставля чон друг выбор засел сам дешев общежит котор наход черт рог дорам незнакомц ад поведа сойт ум жив стран сосед котор похож извращенц маньяк умалишен чон насторажива владелиц кажет приятн женщин реша оста полгод пок накоп достаточн средств съемн квартир одн неприятн личност явля банд джун котор кажет гер сам адекватн человек однажд кухн пыта предупред чон нужн бежа мест законч разговор уда близнец отклонен устраива переполох тих жизн явн главн гер пишет книг атмосфер царя вокруг мал способств написан утр отправ работ небольш компан директор явля друг чон вид город друг краск желан возвраща дом работ парн отсутств идет вып коллег замеча драк улиц реша вмеша оказыва гер будуч солдат арм сталкива похож ситуац психологическ травм добра сво комнат чон наход стран ежедневник котор предположительн принадлеж бывш жильц котор соверш самоубийств нем наход стран записи…\n",
            "4: главн героин работа маркетолог молод компан качеств член команд амбициозн стартап девушк приход работа благ дел цел дням труд офис вечер подрабатыва каф миц принадлежа отц однажд тяжел дня героин ослабля бдительн случайн теря смартфон автобус девушк осозна случ пуска поиск пропаж оказыва слишк поздн нект неизвестн успева подобра телефон девушк сдает реша позвон сво телефон номер подруг попрос нашедш вернут собствен неожида вор соглаша отда телефон предлага забра телефон мастерск отда девушк подозрев подвох отправля встреч однак парен имен джун нашедш телефон замысл недобр получа доступ парол девушк вид починк устанавлива телефон шпионск программн обеспечен телефон возвраща закон владелиц начина след кажд действ включ звонк сообщен передвижен девушк медлен верн джун проника жизн времен станов причин ссор девушк отц однак эт лиш нача истор догадыв происходя обраща нов знаком помощ исправлен ошибк телефон джун внов станов причин пробл котор оказыва вовлеч сам близк люд девушк времен полиц расслед сер леденя душ убийств следств связыва действ одн маньяк отличительн черт котор коллекционир телефон сво жертв подозрева дел проход нек джун\n",
            "5: эт истор мест котор соверша человек натиск обстоятельств люд боя пойт прот систем перв очеред эт истор чжи ен курсант обуча полицейск академ юност гангстер напа уб мам жесток разочарова справедлив правосуд сво стран преступник уб родител ушл наказан пойма сотрудник правоохранительн систем чжи реша взят правосуд сво рук поступа полицейск академ найт преступник лиш сем днем обычн студент котор прилежн уч свободн врем превраща человек котор власт цел занима выслеживан преступник безнаказа живущ обществ мстител найдет тех ком уда уйт правосуд либ наказан слишк снисходительн деятельн начина привлека очен вниман посторон люд нем начина говор высокопоставлен личност тайн войн преступн мир пишут стат разн газет борьб несправедлив станов довольн обсужда обществ однажд репортер чхвэ уда пойма мстител поличн врем охот преступник однак вмест преда личност охотник бандит огласк журналист реша помоч поиск правосуд злоде личност линчевател начина интересова сеульск следовател чон хон явля глав столичн следствен отдел отслежива существован темн геро намерева найт всем доступн способ\n",
            "6: апокалиптическ триллер действ котор происход врем инфекцион заболеван стал норм дорам показа классов дискриминац тонк психологическ войн ожесточен борьб выживан происходя многоэтажн жил комплекс изолирова внешн мир изз нов вид инфекцион заболеван бом агент специальн отряд полиц решительн поэт быстр принима решен колебан претворя жизн осуществ мечт переезд нов квартир столкнет беспрецедентн кризис чон хен умн честн детект отдел насильствен преступлен работа благ обществ личн выгод влюбл одноклассницудруг бом чон хен попада водоворот событ принима неожида предложен бом хан сок руководител медицинск подразделен вооружен сил владеет ключ нов инфекцион заболеван поруч определ причин нов инфекцион заболеван предотврат распространен хан сок персонаж котор мгновен трансформир поэт трудн прочита\n",
            "7: кан бит проблематичн суд котор скептическ относ неискрен раскаян преступник способн сопережива бол страдан жертв суд кан красив имеет элитн происхожден истин сущност демон пришедш ад мисс котор состо наказа тех испытыва угрызен совест несет ответствен смерт друг люд долг отправ уб суд кан знаком детектив он человек тепл характер котор став благополуч жертв перв мест помога кан бит осозна важност сочувств сострадан\n",
            "8: пор упуска сам важн жизн кажд человек хочет прост жит ощуща ценност сво жизн чувствова существован деятельн жизнен позиц преодолен препятств устремлен будущ несут какойт смысл обыден ситуац редк осозна нам нужн жизн сам осмыслен естествен сво проблем котор явля предпосылк причин обесцениван сам важн жизн имен человек переста получа удовольств жизн появля психологическ расстройств чхве джэ несчастн человек иб вся жизн полк неудач гореч долг год поиск работ принос никак плод столкнувш мошенник сфер биткойн приход распроща всем сбережен том любя девушк внезапн оставля одн трудн минут обременен тяжел мысл справля давлен требовательн обществ неудачник прибега самоубийств случа умерет прост получа оскорблен легкомыслен отношен смерт заявля зам смерт наказа юнош проступок смерт заманива смертоносн игр откуд жив практическ выйт гер предсто переж смерт снов снов друг жизн джэ сможет найт способ переж неминуем смерт наступа кажд удаст останов черед реинкарнац прож жизн перв жизн ужасн провал насчет остальн сможет осозна значен существован найт верн пут счаст любв придет дальш испытыва сер неудач пок попадет\n",
            "9: ким бывш офицер спецназ работа водител служб такс уникальн таксист сочета врожден интуиц здравомысл смелост такж навык боев искусств котор могут помоч выстоя прот множеств противник кан прокурор котор бульдозер борет справедлив вооружен элитн образован честност не недостаток склонност проявля агресс столкновен несправедлив врем расследован исчезновен бывш заключен не возника подозрен относительн личност таинствен таксист ким чан сон чхол генеральн директор таинствен служб такс возглавля операц наказан тех соверша незакон действ польз преимуществ слеп зон государствен систем правосуд тепл ктолиб относ жертв абсолютн жесток безжал отношен преступник хакер служб такс умеющ наход личн информац люд явля эксперт использован цифров устройств сист видеонаблюден смартфон компьютер действова глаз уш ким\n",
            "10: сем девушк сан вынужд посел южнокорейск уезд мудж изз финансов пробл отц переезд ник дал легк ещ выясн прода дом натуральн развалин дорам спас ве городок словн паутин окутыва влиян религиозн сект предводительств скользк тип называ свят отц сраз полож глаз сан подсоб отц жил жизн снов налажива сан вмест братомблизнец идет школ встреча хорош люд однак брат везет юнош сталкива травл сторон одноклассник конц конц настольк довод соверша самоубийств\n",
            "\n",
            "Матрица Bag of Words:\n",
            "\n",
            "             ëл  абсолютн  автобус  агент  агресс  ад  адекватн  академ  \\\n",
            "Синопсис 1    3         0        0      0       0   0         0       0   \n",
            "Синопсис 2    0         0        0      0       0   0         0       0   \n",
            "Синопсис 3    0         0        0      0       0   1         1       0   \n",
            "Синопсис 4    0         0        1      0       0   0         0       0   \n",
            "Синопсис 5    0         0        0      0       0   0         0       2   \n",
            "Синопсис 6    0         0        0      1       0   0         0       0   \n",
            "Синопсис 7    0         0        0      0       0   1         0       0   \n",
            "Синопсис 8    0         0        0      0       0   0         0       0   \n",
            "Синопсис 9    0         1        0      0       1   0         0       0   \n",
            "Синопсис 10   0         0        0      0       0   0         0       0   \n",
            "\n",
            "             амбициоз  амбициозн  ...  школ  шпионск  эксперт  элитн  эт  \\\n",
            "Синопсис 1          1          0  ...     0        0        0      0   0   \n",
            "Синопсис 2          0          0  ...     0        0        0      0   0   \n",
            "Синопсис 3          0          0  ...     0        0        0      0   0   \n",
            "Синопсис 4          0          1  ...     0        1        0      0   1   \n",
            "Синопсис 5          0          0  ...     0        0        0      0   2   \n",
            "Синопсис 6          0          0  ...     0        0        0      0   0   \n",
            "Синопсис 7          0          0  ...     0        0        0      1   0   \n",
            "Синопсис 8          0          0  ...     0        0        0      0   0   \n",
            "Синопсис 9          0          0  ...     0        0        1      1   0   \n",
            "Синопсис 10         0          0  ...     1        0        0      0   0   \n",
            "\n",
            "             южнокорейск  юност  юнош  явля  явн  \n",
            "Синопсис 1             0      0     0     0    0  \n",
            "Синопсис 2             0      0     0     0    0  \n",
            "Синопсис 3             0      0     0     2    1  \n",
            "Синопсис 4             0      0     0     0    0  \n",
            "Синопсис 5             0      1     0     1    0  \n",
            "Синопсис 6             0      0     0     0    0  \n",
            "Синопсис 7             0      0     0     0    0  \n",
            "Синопсис 8             0      0     1     1    0  \n",
            "Синопсис 9             0      0     0     1    0  \n",
            "Синопсис 10            1      0     1     0    0  \n",
            "\n",
            "[10 rows x 786 columns]\n",
            "\n",
            "Матрица TF-IDF: \n",
            "\n",
            "                   ëл  абсолютн   автобус     агент    агресс        ад  \\\n",
            "Синопсис 1   0.274874  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Синопсис 2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Синопсис 3   0.000000  0.000000  0.000000  0.000000  0.000000  0.062527   \n",
            "Синопсис 4   0.000000  0.000000  0.061727  0.000000  0.000000  0.000000   \n",
            "Синопсис 5   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Синопсис 6   0.000000  0.000000  0.000000  0.084224  0.000000  0.000000   \n",
            "Синопсис 7   0.000000  0.000000  0.000000  0.000000  0.000000  0.102317   \n",
            "Синопсис 8   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "Синопсис 9   0.000000  0.091782  0.000000  0.000000  0.091782  0.000000   \n",
            "Синопсис 10  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
            "\n",
            "             адекватн    академ  амбициоз  амбициозн  ...      школ   шпионск  \\\n",
            "Синопсис 1   0.000000  0.000000  0.091625   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 2   0.000000  0.000000  0.000000   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 3   0.073553  0.000000  0.000000   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 4   0.000000  0.000000  0.000000   0.061727  ...  0.000000  0.061727   \n",
            "Синопсис 5   0.000000  0.155074  0.000000   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 6   0.000000  0.000000  0.000000   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 7   0.000000  0.000000  0.000000   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 8   0.000000  0.000000  0.000000   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 9   0.000000  0.000000  0.000000   0.000000  ...  0.000000  0.000000   \n",
            "Синопсис 10  0.000000  0.000000  0.000000   0.000000  ...  0.116747  0.000000   \n",
            "\n",
            "              эксперт     элитн        эт  южнокорейск     юност      юнош  \\\n",
            "Синопсис 1   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
            "Синопсис 2   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
            "Синопсис 3   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
            "Синопсис 4   0.000000  0.000000  0.052474     0.000000  0.000000  0.000000   \n",
            "Синопсис 5   0.000000  0.000000  0.131827     0.000000  0.077537  0.000000   \n",
            "Синопсис 6   0.000000  0.000000  0.000000     0.000000  0.000000  0.000000   \n",
            "Синопсис 7   0.000000  0.102317  0.000000     0.000000  0.000000  0.000000   \n",
            "Синопсис 8   0.000000  0.000000  0.000000     0.000000  0.000000  0.062387   \n",
            "Синопсис 9   0.091782  0.078023  0.000000     0.000000  0.000000  0.000000   \n",
            "Синопсис 10  0.000000  0.000000  0.000000     0.116747  0.000000  0.099246   \n",
            "\n",
            "                 явля       явн  \n",
            "Синопсис 1   0.000000  0.000000  \n",
            "Синопсис 2   0.000000  0.000000  \n",
            "Синопсис 3   0.097271  0.073553  \n",
            "Синопсис 4   0.000000  0.000000  \n",
            "Синопсис 5   0.051270  0.000000  \n",
            "Синопсис 6   0.000000  0.000000  \n",
            "Синопсис 7   0.000000  0.000000  \n",
            "Синопсис 8   0.048527  0.000000  \n",
            "Синопсис 9   0.060689  0.000000  \n",
            "Синопсис 10  0.000000  0.000000  \n",
            "\n",
            "[10 rows x 786 columns]\n",
            "\n",
            "Топ-10 BoW по отзывам:\n",
            "\n",
            "Отзыв 1:\n",
            "  - ëл 3\n",
            "  - однак 3\n",
            "  - люд 2\n",
            "  - котор 2\n",
            "  - человек 2\n",
            "  - дел 2\n",
            "  - одн 2\n",
            "  - плох 2\n",
            "  - стал 2\n",
            "  - стремлен 2\n",
            "\n",
            "Отзыв 2:\n",
            "  - котор 4\n",
            "  - чжу 2\n",
            "  - хон 2\n",
            "  - родител 2\n",
            "  - рым 2\n",
            "  - чон 2\n",
            "  - убийц 2\n",
            "  - жизн 2\n",
            "  - друг 2\n",
            "  - старшеклассниц 1\n",
            "\n",
            "Отзыв 3:\n",
            "  - чон 6\n",
            "  - котор 5\n",
            "  - друг 4\n",
            "  - гер 4\n",
            "  - работ 3\n",
            "  - стран 3\n",
            "  - наход 3\n",
            "  - явля 2\n",
            "  - реша 2\n",
            "  - сам 2\n",
            "\n",
            "Отзыв 4:\n",
            "  - телефон 9\n",
            "  - девушк 8\n",
            "  - джун 4\n",
            "  - героин 2\n",
            "  - действ 2\n",
            "  - дел 2\n",
            "  - времен 2\n",
            "  - нашедш 2\n",
            "  - работа 2\n",
            "  - однак 2\n",
            "\n",
            "Отзыв 5:\n",
            "  - правосуд 4\n",
            "  - преступник 4\n",
            "  - начина 3\n",
            "  - личност 3\n",
            "  - котор 3\n",
            "  - уб 2\n",
            "  - сво 2\n",
            "  - человек 2\n",
            "  - реша 2\n",
            "  - эт 2\n",
            "\n",
            "Отзыв 6:\n",
            "  - нов 4\n",
            "  - инфекцион 4\n",
            "  - заболеван 4\n",
            "  - бом 3\n",
            "  - хен 2\n",
            "  - чон 2\n",
            "  - принима 2\n",
            "  - хан 2\n",
            "  - сок 2\n",
            "  - поэт 2\n",
            "\n",
            "Отзыв 7:\n",
            "  - кан 4\n",
            "  - суд 3\n",
            "  - котор 3\n",
            "  - жертв 2\n",
            "  - бит 2\n",
            "  - помога 1\n",
            "  - сострадан 1\n",
            "  - элитн 1\n",
            "  - состо 1\n",
            "  - став 1\n",
            "\n",
            "Отзыв 8:\n",
            "  - жизн 9\n",
            "  - смерт 5\n",
            "  - человек 3\n",
            "  - сам 3\n",
            "  - джэ 2\n",
            "  - важн 2\n",
            "  - осозна 2\n",
            "  - неудач 2\n",
            "  - получа 2\n",
            "  - существован 2\n",
            "\n",
            "Отзыв 9:\n",
            "  - такс 3\n",
            "  - служб 3\n",
            "  - ким 3\n",
            "  - таксист 2\n",
            "  - таинствен 2\n",
            "  - не 2\n",
            "  - котор 2\n",
            "  - бывш 2\n",
            "  - сон 1\n",
            "  - элитн 1\n",
            "\n",
            "Отзыв 10:\n",
            "  - сан 3\n",
            "  - отц 3\n",
            "  - конц 2\n",
            "  - ве 1\n",
            "  - братомблизнец 1\n",
            "  - влиян 1\n",
            "  - вынужд 1\n",
            "  - везет 1\n",
            "  - брат 1\n",
            "  - выясн 1\n",
            "\n",
            "Топ-10 TF-IDF по отзывам:\n",
            "\n",
            "Отзыв 1:\n",
            "  - ëл 0.2749\n",
            "  - стремлен 0.1832\n",
            "  - плох 0.1832\n",
            "  - помеша 0.1832\n",
            "  - однак 0.1818\n",
            "  - стал 0.1558\n",
            "  - дел 0.1558\n",
            "  - одн 0.1212\n",
            "  - человек 0.1088\n",
            "  - вер 0.0916\n",
            "\n",
            "Отзыв 2:\n",
            "  - чжу 0.2138\n",
            "  - рым 0.2138\n",
            "  - убийц 0.2138\n",
            "  - хон 0.1817\n",
            "  - родител 0.1817\n",
            "  - котор 0.1731\n",
            "  - чон 0.1414\n",
            "  - друг 0.1269\n",
            "  - талантлив 0.1069\n",
            "  - отомст 0.1069\n",
            "\n",
            "Отзыв 3:\n",
            "  - чон 0.2918\n",
            "  - гер 0.2501\n",
            "  - работ 0.1876\n",
            "  - наход 0.1876\n",
            "  - друг 0.1747\n",
            "  - стран 0.1641\n",
            "  - котор 0.1489\n",
            "  - небольш 0.1471\n",
            "  - кажет 0.1471\n",
            "  - похож 0.1471\n",
            "\n",
            "Отзыв 4:\n",
            "  - телефон 0.5555\n",
            "  - девушк 0.3265\n",
            "  - джун 0.2099\n",
            "  - времен 0.1235\n",
            "  - героин 0.1235\n",
            "  - нашедш 0.1235\n",
            "  - отда 0.1235\n",
            "  - отц 0.1049\n",
            "  - станов 0.1049\n",
            "  - оказыва 0.1049\n",
            "\n",
            "Отзыв 5:\n",
            "  - правосуд 0.2637\n",
            "  - преступник 0.1842\n",
            "  - начина 0.173\n",
            "  - личност 0.173\n",
            "  - мстител 0.1551\n",
            "  - пойма 0.1551\n",
            "  - академ 0.1551\n",
            "  - чжи 0.1551\n",
            "  - систем 0.1318\n",
            "  - эт 0.1318\n",
            "\n",
            "Отзыв 6:\n",
            "  - инфекцион 0.3369\n",
            "  - заболеван 0.3369\n",
            "  - нов 0.2864\n",
            "  - бом 0.2527\n",
            "  - хен 0.1684\n",
            "  - сок 0.1684\n",
            "  - хан 0.1684\n",
            "  - принима 0.1684\n",
            "  - поэт 0.1684\n",
            "  - чон 0.1114\n",
            "\n",
            "Отзыв 7:\n",
            "  - кан 0.4093\n",
            "  - суд 0.3611\n",
            "  - бит 0.2407\n",
            "  - жертв 0.179\n",
            "  - котор 0.1462\n",
            "  - совест 0.1204\n",
            "  - имеет 0.1204\n",
            "  - сочувств 0.1204\n",
            "  - проблематичн 0.1204\n",
            "  - помога 0.1204\n",
            "\n",
            "Отзыв 8:\n",
            "  - жизн 0.322\n",
            "  - смерт 0.2729\n",
            "  - джэ 0.1468\n",
            "  - неудач 0.1468\n",
            "  - важн 0.1468\n",
            "  - сможет 0.1468\n",
            "  - прост 0.1468\n",
            "  - переж 0.1468\n",
            "  - сам 0.1456\n",
            "  - человек 0.1307\n",
            "\n",
            "Отзыв 9:\n",
            "  - ким 0.2753\n",
            "  - служб 0.2753\n",
            "  - такс 0.2753\n",
            "  - таинствен 0.1836\n",
            "  - таксист 0.1836\n",
            "  - не 0.1836\n",
            "  - бывш 0.156\n",
            "  - прокурор 0.0918\n",
            "  - заключен 0.0918\n",
            "  - устройств 0.0918\n",
            "\n",
            "Отзыв 10:\n",
            "  - сан 0.3502\n",
            "  - отц 0.2977\n",
            "  - конц 0.2335\n",
            "  - ве 0.1167\n",
            "  - влиян 0.1167\n",
            "  - вынужд 0.1167\n",
            "  - брат 0.1167\n",
            "  - братомблизнец 0.1167\n",
            "  - называ 0.1167\n",
            "  - выясн 0.1167\n",
            "\n",
            "Косинусное сходство BoW:\n",
            "       0      1      2      3      4      5      6      7      8      9\n",
            "0  1.000  0.174  0.137  0.123  0.185  0.095  0.119  0.127  0.069  0.079\n",
            "1  0.174  1.000  0.259  0.097  0.228  0.140  0.169  0.187  0.138  0.049\n",
            "2  0.137  0.259  1.000  0.154  0.173  0.137  0.145  0.183  0.103  0.046\n",
            "3  0.123  0.097  0.154  1.000  0.106  0.085  0.063  0.130  0.051  0.113\n",
            "4  0.185  0.228  0.173  0.106  1.000  0.082  0.150  0.103  0.185  0.043\n",
            "5  0.095  0.140  0.137  0.085  0.082  1.000  0.049  0.068  0.060  0.043\n",
            "6  0.119  0.169  0.145  0.063  0.150  0.049  1.000  0.110  0.162  0.011\n",
            "7  0.127  0.187  0.183  0.130  0.103  0.068  0.110  1.000  0.020  0.089\n",
            "8  0.069  0.138  0.103  0.051  0.185  0.060  0.162  0.020  1.000  0.028\n",
            "9  0.079  0.049  0.046  0.113  0.043  0.043  0.011  0.089  0.028  1.000\n",
            "\n",
            "Косинусное сходство TF-IDF:\n",
            "       0      1      2      3      4      5      6      7      8      9\n",
            "0  1.000  0.083  0.066  0.072  0.098  0.055  0.044  0.067  0.027  0.039\n",
            "1  0.083  1.000  0.119  0.047  0.129  0.063  0.055  0.100  0.067  0.020\n",
            "2  0.066  0.119  1.000  0.099  0.098  0.065  0.054  0.125  0.053  0.031\n",
            "3  0.072  0.047  0.099  1.000  0.064  0.052  0.027  0.082  0.022  0.071\n",
            "4  0.098  0.129  0.098  0.064  1.000  0.041  0.065  0.073  0.115  0.026\n",
            "5  0.055  0.063  0.065  0.052  0.041  1.000  0.010  0.030  0.027  0.027\n",
            "6  0.044  0.055  0.054  0.027  0.065  0.010  1.000  0.075  0.090  0.003\n",
            "7  0.067  0.100  0.125  0.082  0.073  0.030  0.075  1.000  0.010  0.045\n",
            "8  0.027  0.067  0.053  0.022  0.115  0.027  0.090  0.010  1.000  0.014\n",
            "9  0.039  0.020  0.031  0.071  0.026  0.027  0.003  0.045  0.014  1.000\n",
            "\n",
            "ТОП-10 BoW по корпусу:\n",
            " 1. котор           24\n",
            " 2. жизн            16\n",
            " 3. девушк          11\n",
            " 4. чон             11\n",
            " 5. друг            9\n",
            " 6. люд             9\n",
            " 7. телефон         9\n",
            " 8. человек         9\n",
            " 9. преступник      8\n",
            "10. сво             8\n",
            "\n",
            "ТОП-10 TF-IDF по корпусу:\n",
            " 1. котор           0.8590\n",
            " 2. жизн            0.6347\n",
            " 3. чон             0.5958\n",
            " 4. телефон         0.5555\n",
            " 5. девушк          0.5009\n",
            " 6. кан             0.4873\n",
            " 7. друг            0.4711\n",
            " 8. человек         0.4468\n",
            " 9. смерт           0.4419\n",
            "10. преступник      0.4280\n",
            "\n",
            "Общие слова (BoW и TF-IDF): девушк, друг, жизн, котор, преступник, телефон, человек, чон\n",
            "Только BoW: люд, сво\n",
            "Только TF-IDF: кан, смерт\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "texts = [\n",
        "        \"Реальная жизнь редко делится на «чёрное» и «белое», так же, как и люди — на исключительно «плохих» или «хороших» ... Дорама поведает историю двух совершенно разных людей. Су Ëль — «плохой парень», потерявший веру в человечество, но при этом считающийся отличным полицейским, успешно раскрывающим одно дело за другим. Однако встреча с его полной противоположностью — «праведным» человеком, скрывающимся за псевдонимом «Кей», но творящим ужасные вещи, — полностью меняет ход событий... Су Ëль получил лишь аттестат о среднем образовании, что, однако, не помешало харизматичному и обаятельному молодому человеку дослужиться в полиции до ведущего сотрудника отдела по борьбе с распространением запрещённых веществ. Однако движет героем совсем не стремление к справедливости: мужчина амбициозен и предпочитает закрывать глаза на те преступления, раскрытие которых не способно принести ему продвижения по карьерной лестнице и одобрения от начальства. Но в тот самый момент, когда на горизонте замаячило долгожданное повышение на перспективное место, в городе объявился сумасшедший преступник, светлое будущее стало ускользать от него... Теперь же Су Ëль готов выйти на тропу войны и во что бы то ни стало разобраться с врагом, который помешал его планам! Но что случится, если интересы обоих пересекутся в стремлении к одной и той же цели?\",\n",
        "        \"Сюжет дорамы Мышь раскрывает захватывающую историю, действие которой происходит в мире, в котором люди могут идентифицировать психопатов с помощью ДНК-теста плода в утробе матери. Чон Ба Рым - честный и добрый новичок полицейский, верящий в справедливость. После столкновения с серийным убийцей-психопатом, чьи безжалостные убийства навели ужас на всю страну, его жизнь полностью меняется. Го Му Чи - опытный детектив, родители которого были убиты, когда он был юн. С тех пор цель его жизни – найти убийцу и отомстить за смерть родителей. Для поимки преступников Го Му Чи готов прибегнуть к любым средствам, не заботясь ни о законе, ни о правилах. О Бон И - проблемная старшеклассница, живущая с бабушкой и хорошо разбирающаяся в боевых искусствах. Каждый раз, когда она сталкивается с соседом-полицейским Чон Ба Рымом, они начинают забавно препираться друг с другом. Чхве Хон Чжу - талантливый режиссёр-продюсер, известная как «Шерлок Хон Чжу». Она также хранит секрет, который в детстве связал её с убийцей.\",\n",
        "        \"Молодой парень Юн Чон Ву родом из небольшой деревни получает предложение о работе от своего друга из Сеула. Упускать такую возможность герой не собирается и пакует чемоданы, тем более что его девушка давно живет и работает здесь. Суета большого города и цены намного выше привычных не оставляют Юн Чон Ву другого выбора, как заселиться в самое дешевое общежитие, которое находится у черта на рогах. Дорама «Незнакомцы из ада» поведает вам, как не сойти с ума, живя со странными соседями, которые похожи на извращенцев, маньяков и умалишённых. Юн Чон Ву они настораживают, но владелица кажется ему приятной женщиной, и он решает остаться на полгода, пока не накопит достаточно средств для съемной квартиры. Еще одной неприятной личностью является бандит Ан Хи Джуном, который кажется герою самым адекватным человеком. Однажды на кухне он пытается предупредить Юн Чон Ву о том, что нужно бежать из этого места, но закончить разговор им не удается. Близнецы с отклонениями устраивают переполох, и тихой жизни здесь явно не будет. Между тем главный герой пишет книгу, а атмосфера, царящая вокруг него, мало способствует для ее написания. На утро отправившись на работу в небольшую компанию, где директором является его друг, Юн Чон Ву видит город в других красках. Желание возвращаться домой после работы у парня отсутствует, и он идет выпить с коллегами, но замечает драку на улице и решает вмешаться. Оказывается, герой, будучи солдатом в армии сталкивался с похожей ситуацией и теперь у него психологическая травма. Добравшись до своей комнаты, Юн Чон Ву находит странный ежедневник, который предположительно принадлежит бывшему жильцу, который совершил самоубийство. В нем он находит странные записи…\",\n",
        "        \"Главная героиня Ли На Ми работает маркетологом в молодой компании. В качестве члена команды амбициозного стартапа девушке приходится много работать на благо дела. Целыми днями она трудится в офисе, а по вечерам подрабатывает в кафе «Мици», принадлежащем её отцу. Однажды после тяжёлого дня героиня ослабляет бдительность и случайно теряет смартфон в автобусе. Когда девушка осознаёт случившееся и пускается на поиски пропажи, оказывается уже слишком поздно: некто неизвестный успевает подобрать телефон девушки. Но Ли На Ми не сдаётся и решает позвонить на свой телефон с номера подруги для того, чтобы попросить нашедшего вернуть ей собственность. Неожиданно для На Ми, вор соглашается отдать ей телефон и предлагает забрать телефон из мастерской, куда он отдал его. Девушка, не подозревая подвоха, отправляется на встречу. Однако парень по имени О Джун Ëн, нашедший телефон, уже замыслил недоброе. Он получает доступ к паролю девушки и под видом починки устанавливает в телефон шпионское программное обеспечение. И как только телефон возвращается к законной владелице, то тут же начинает следить за каждым действием На Ми, включая звонки, сообщения и передвижения девушки. Медленно, но верно Джун Ëн проникает в жизнь На Ми. А со временем и становится причиной ссоры между девушкой и её отцом. Однако это лишь начало истории. Не догадываясь о происходящем, На Ми обращается к новому знакомому за помощью в исправлении ошибки телефона. И Джун Ëн вновь становится причиной её проблем, в которые оказывается вовлечены и самые близкие люди девушки. Тем временем полиция расследует серию леденящих душу убийств. Следствие связывает их с действиями одного и того же маньяка, отличительной чертой который коллекционирует телефоны своих жертв. А подозреваемым по делу проходит некий О Джун Ëн...\",\n",
        "        \"Эта история о мести, которую совершает человек под натиском обстоятельств. О людях, не боящихся пойти против системы. В первую очередь это история Чжи Ёна, курсанта, обучающегося в полицейской академии. В юности гангстеры напали и убили его маму. Он жестоко разочаровался в справедливости правосудия своей страны. Ведь преступники, убившие его родителей, ушли от наказания. Они так и не были пойманы сотрудниками правоохранительной системы. Тогда Чжи Ён решает взять правосудие в свои руки. Поступает в полицейскую академию, чтобы найти преступников, лишивших его семьи. Днём он обычный студент, который прилежно учится. В свободное время он превращается в человека, у которого нет власти, но есть цель. Он занимается выслеживанием преступников, безнаказанно живущих в обществе. Мститель найдёт тех, кому удалось уйти от правосудия, либо наказание для них было слишком снисходительным. Но его деятельность начинает привлекать очень много внимания посторонних людей. О нём начинают говорить высокопоставленные личности. Про тайную войну с преступным миром пишут статьи в разных газетах. Борьба с несправедливостью становится довольно обсуждаемой в обществе. Однажды репортёру Чхвэ Ми Рё удаётся поймать мстителя с поличным во время его охоты на преступников. Однако вместо того, чтобы предать личность охотника на бандитов огласке. Журналист решает помочь ему в поиске правосудия для злодеев. Но так же личностью «Линчевателя» начинает интересоваться Сеульский следователь — Чон Хон. Он является главой столичного следственного отдела. Отслеживает существование тёмного героя и намеревается его найти всеми доступными способами.\",\n",
        "        \"Апокалиптический триллер, действие которого происходит во времена, когда инфекционные заболевания стали нормой. В дораме показана классовая дискриминация, тонкие психологические войны и ожесточённая борьба за выживание, происходящие в многоэтажном жилом комплексе, изолированном от внешнего мира из-за нового вида инфекционного заболевания. Юн Сэ Бом – агент специального отряда полиции. Она решительна, поэтому быстро принимает решения и без колебаний претворяет их в жизнь. Но как только она осуществит свою мечту о переезде в новую квартиру, она столкнётся с беспрецедентным кризисом. Чон И Хён – умный и честный детектив из отдела по насильственным преступлениям, работающий на благо общества, а не для личной выгоды. Он всегда был влюблён в свою одноклассницу-друга Юн Сэ Бом. Чон И Хён попадает в водоворот событий после того, как принимает неожиданное предложение от Юн Сэ Бом. Хан Тэ Сок – руководитель медицинского подразделения вооружённых сил. Он тот, кто владеет «ключом» к новому инфекционному заболеванию. Ему было поручено определить причину нового инфекционного заболевания и предотвратить его распространение. Хан Тэ Сок - персонаж, который мгновенно трансформируется, поэтому его трудно «прочитать».\",\n",
        "        \"Кан Бит На – проблематичная судья, которая скептически относится к неискреннему раскаянию преступников и не способна сопереживать боли и страданиям жертв. Судья Кан красива и имеет элитное происхождение, но её истинная сущность – демон, пришедший из ада, миссия которого состоит в том, чтобы наказать тех, кто не испытывает угрызений совести, но несёт ответственность за смерть других людей. Её долг – отправить их в ад, убив. Судья Кан знакомится с детективом Ха Да Оном, человеком с теплым характером, который ставит благополучие жертвы на первое место и помогает Кан Бит На осознать важность сочувствия и сострадания.\",\n",
        "        \"Порой, все мы упускаем самое важное в жизни. Ведь каждый человек хочет не просто жить, но и ощущать ценность своей жизни, чувствовать, что его существование, его деятельность, жизненные позиции, преодоление препятствий, устремлённость в будущее несут какой-то смысл. В обыденных ситуациях мы редко осознаём, что нам нужна не только жизнь сама по себе, но и её осмысленность. Естественно, что у всех своя проблема, которая является предпосылкой, причиной обесценивания самого важного — жизни. Именно когда человек перестаёт получать удовольствие от жизни, у него появляются психологические расстройства. Чхве И Джэ — несчастный человек, ибо вся его жизнь полка неудач и горечи. Долгие годы поисков работы не приносят никаких плодов. Столкнувшись с мошенниками в сфере биткойнов, ему приходится распрощаться со всеми сбережениями. К тому же и любящая девушка внезапно оставляет его одного в трудную минуту. Обременённый тяжёлыми мыслями, не справляющийся с давлением требовательного общества неудачник прибегает к самоубийству. Но в его случае умереть так просто не получается. Оскорблённая его легкомысленным отношением к смерти, заявляется зама «Смерть», чтобы наказать юношу за такой проступок. Смерть заманивает его в свою смертоносную игру, откуда живым практически не выйти. Герою предстоит пережить свою смерть снова и снова в 13ти других жизнях. Если И Джэ сможет найти способ пережить неминуемую смерть, наступающую каждый раз, ему удастся остановить череду реинкарнаций и прожить свою жизнь. Его первая жизнь была ужасна и провалена, а как насчёт остальных? Сможет ли он осознать значение существования и найти верный путь для счастья и любви? Или ему придётся и дальше испытывать серию неудач, пока не попадёт в ад?\",\n",
        "        \"Ким До Ги - бывший офицер спецназа, работающий водителем в службе такси. Он уникальный таксист, сочетающий в себе врождённую интуицию, здравомыслие, смелость, а также навыки боевых искусств, которые могут помочь ему выстоять против множества противников. Кан Ха На - прокурор, которая, как бульдозер, борется за справедливость. Вооружённая элитным образованием и честностью, у неё есть один недостаток - склонность проявлять агрессию при столкновении с несправедливостью. Во время расследования исчезновения бывшего заключенного у неё возникают подозрения относительно личности таинственного таксиста Ким До Ги. Чан Сон Чхоль - генеральный директор таинственной службы такси. Он возглавляет операцию по наказанию тех, кто совершает незаконные действия и пользуется преимуществами в слепой зоне государственной системы правосудия. Он теплее, чем кто-либо другой относится к жертвам, но абсолютно жесток и безжалостен по отношению к преступникам. Го Ын - хакер в службе такси, умеющая находить личную информацию людей. Она является экспертом в использовании всех цифровых устройств, от систем видеонаблюдения, смартфонов до компьютеров, и будет действовать как глаза и уши Ким До Ги.\",\n",
        "        \"Семья девушки Сан Ми была вынуждена поселиться в южнокорейском уезде Муджу из-за финансовых проблем отца. Переезд никому не дался легко, а по потом ещё и выяснилось, что им продали не дом, а натуральные развалины. В дораме «Спаси меня» весь городок словно паутиной окутывает влияние религиозной секты под предводительством скользкого типа, называющего себя святым отцом. Он сразу положил глаз на Сан Ми и даже подсобил её отцу с жильём. Жизнь снова налаживается, когда Сан Ми вместе с братом-близнецом идёт в школу, где встречает хороших людей. Однако брату не везет: юноша сталкивается с травлей со стороны одноклассников. В конце концов они его настолько доводят, что тот совершает самоубийство.\"\n",
        "]\n",
        "\n",
        "russian_stopwords = stopwords.words('russian')\n",
        "stemmer = SnowballStemmer('russian')\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = text.replace('\\n', ' ')\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text = text.translate(translator)\n",
        "    tokens = word_tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in russian_stopwords and len(t) > 2]\n",
        "    stemmed = [stemmer.stem(t) for t in tokens]\n",
        "    return \" \".join(stemmed)\n",
        "\n",
        "preprocessed_texts = [preprocess_text(doc) for doc in texts]\n",
        "\n",
        "print(\"Предобработанные синопсисы:\")\n",
        "for i, doc in enumerate(preprocessed_texts, 1):\n",
        "    print(f\"{i}: {doc}\")\n",
        "\n",
        "count_vectorizer = CountVectorizer()\n",
        "bow_matrix = count_vectorizer.fit_transform(preprocessed_texts)\n",
        "bow_feature_names = count_vectorizer.get_feature_names_out()\n",
        "\n",
        "bow_df = pd.DataFrame(\n",
        "    bow_matrix.toarray(),\n",
        "    columns=bow_feature_names,\n",
        "    index=[f\"Синопсис {i}\" for i in range(1, 11)]\n",
        ")\n",
        "\n",
        "print(\"\\nМатрица Bag of Words:\\n\")\n",
        "print(bow_df)\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(preprocessed_texts)\n",
        "tfidf_feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "\n",
        "tfidf_df = pd.DataFrame(\n",
        "    tfidf_matrix.toarray(),\n",
        "    columns=tfidf_feature_names,\n",
        "    index=[f\"Синопсис {i}\" for i in range(1, 11)]\n",
        ")\n",
        "print(\"\\nМатрица TF-IDF: \\n\")\n",
        "print(tfidf_df)\n",
        "\n",
        "def top_terms_per_doc(matrix, feature_names, top_n=10, is_tfidf=False):\n",
        "    tops = {}\n",
        "    for i in range(matrix.shape[0]):\n",
        "        row = matrix[i].toarray().flatten()\n",
        "        idx_sorted = np.argsort(row)[::-1]\n",
        "        top_idx = idx_sorted[:top_n]\n",
        "        if is_tfidf:\n",
        "            tops[i] = [(feature_names[j], round(row[j], 4)) for j in top_idx if row[j] > 0]\n",
        "        else:\n",
        "            tops[i] = [(feature_names[j], int(row[j])) for j in top_idx if row[j] > 0]\n",
        "    return tops\n",
        "\n",
        "bow_top_per_doc = top_terms_per_doc(bow_matrix, bow_feature_names)\n",
        "tfidf_top_per_doc = top_terms_per_doc(tfidf_matrix, tfidf_feature_names, is_tfidf=True)\n",
        "\n",
        "print(\"\\nТоп-10 BoW по отзывам:\")\n",
        "for i, terms in bow_top_per_doc.items():\n",
        "    print(f\"\\nОтзыв {i+1}:\")\n",
        "    for w, v in terms:\n",
        "        print(\"  -\", w, v)\n",
        "\n",
        "print(\"\\nТоп-10 TF-IDF по отзывам:\")\n",
        "for i, terms in tfidf_top_per_doc.items():\n",
        "    print(f\"\\nОтзыв {i+1}:\")\n",
        "    for w, v in terms:\n",
        "        print(\"  -\", w, v)\n",
        "\n",
        "bow_cosine = cosine_similarity(bow_matrix)\n",
        "tfidf_cosine = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "print(\"\\nКосинусное сходство BoW:\")\n",
        "print(pd.DataFrame(bow_cosine.round(3)))\n",
        "\n",
        "print(\"\\nКосинусное сходство TF-IDF:\")\n",
        "print(pd.DataFrame(tfidf_cosine.round(3)))\n",
        "\n",
        "bow_sum = np.sum(bow_matrix.toarray(), axis=0)\n",
        "tfidf_sum = np.sum(tfidf_matrix.toarray(), axis=0)\n",
        "\n",
        "bow_top_corpus = sorted(zip(bow_feature_names, bow_sum), key=lambda x: x[1], reverse=True)[:10]\n",
        "tfidf_top_corpus = sorted(zip(tfidf_feature_names, tfidf_sum), key=lambda x: x[1], reverse=True)[:10]\n",
        "\n",
        "print(\"\\nТОП-10 BoW по корпусу:\")\n",
        "for i, (w, f) in enumerate(bow_top_corpus, 1):\n",
        "    print(f\"{i:2d}. {w:<15} {f}\")\n",
        "\n",
        "print(\"\\nТОП-10 TF-IDF по корпусу:\")\n",
        "for i, (w, s) in enumerate(tfidf_top_corpus, 1):\n",
        "    print(f\"{i:2d}. {w:<15} {s:.4f}\")\n",
        "\n",
        "bow_words = {w for w, _ in bow_top_corpus}\n",
        "tfidf_words = {w for w, _ in tfidf_top_corpus}\n",
        "common = bow_words & tfidf_words\n",
        "bow_only = bow_words - tfidf_words\n",
        "tfidf_only = tfidf_words - bow_words\n",
        "\n",
        "print(\"\\nОбщие слова (BoW и TF-IDF):\", \", \".join(sorted(common)))\n",
        "print(\"Только BoW:\", \", \".join(sorted(bow_only)))\n",
        "print(\"Только TF-IDF:\", \", \".join(sorted(tfidf_only)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj7D5CC2NguY"
      },
      "source": [
        "**Задание 2: Морфологическая разметка текста**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xx3HfIUpNxj1"
      },
      "source": [
        "1. Возьмите отрывок текста (минимум 300 слов) на русском и английском языке.\n",
        "2. Для русского языка используйте PyMorphy2 или PyMorphy3, для английского - NLTK или SpaCy для проведения морфологического анализа.\n",
        "3. Выполните следующие операции:\n",
        "* Определите части речи для каждого слова в тексте\n",
        "* Для существительных определите падеж, род и число\n",
        "* Для глаголов определите время, лицо и число\n",
        "* Создайте частотный словарь частей речи в тексте\n",
        "4. Разработайте функцию, которая будет автоматически изменять текст, заменяя все существительные на их форму множественного числа (где возможно).\n",
        "5. Результаты морфологического анализа должны быть представлены в виде таблицы.\n",
        "6. Оцените и прокомментируйте точность определения морфологических характеристик"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Fi83NngNNjJ0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: inflect in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (7.5.0)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from inflect) (10.8.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from inflect) (4.4.4)\n",
            "Requirement already satisfied: typing_extensions>=4.14.0 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from typeguard>=4.0.1->inflect) (4.15.0)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Home\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "RUS tagged tokens:\n",
            "['В_PREP', 'подтверждение_NOUN_nomn_neut_sing', 'отхожу_VERB_1per_futr_sing', 'от_PREP', 'моего_ADJF', 'прежнего_ADJF', 'дома_NOUN_gent_masc_sing', 'и_CONJ', 'бреду_NOUN_accs_femn_sing', 'в_PREP', 'центр_NOUN_nomn_masc_sing', 'Гейл_NOUN_nomn_masc_sing', 'хотел_VERB_None_past_sing', 'высадиться_INFN', 'вместе_ADVB', 'со_PREP', 'мной_NPRO', 'но_CONJ', 'не_PRCL', 'настаивал_VERB_None_past_sing', 'когда_CONJ', 'я_NPRO', 'отказалась_VERB_None_past_sing', 'Понимает_VERB_3per_pres_sing', 'любая_ADJF', 'компания_NOUN_nomn_femn_sing', 'мне_NPRO', 'сегодня_ADVB', 'в_PREP', 'тягость_NOUN_accs_femn_sing', 'Даже_PRCL', 'его_NPRO', 'Есть_INFN', 'дороги_NOUN_gent_femn_sing', 'которые_ADJF', 'нужно_PRED', 'пройти_INFN', 'в_PREP', 'одиночку_NOUN_accs_femn_sing', 'Лето_NOUN_accs_neut_sing', 'выдалось_VERB_None_past_sing', 'сухим_ADJF', 'и_CONJ', 'знойным_ADJF', 'Редкие_ADJF', 'дожди_NOUN_nomn_masc_plur', 'не_PRCL', 'смыли_VERB_None_past_plur', 'кучи_NOUN_gent_femn_sing', 'золы_NOUN_gent_femn_sing', 'Она_NPRO', 'вспархивает_VERB_3per_pres_sing', 'от_PREP', 'моих_ADJF', 'шагов_NOUN_gent_masc_plur', 'и_CONJ', 'тут_ADVB', 'же_PRCL', 'оседает_VERB_3per_pres_sing', 'Ветра_NOUN_gent_masc_sing', 'нет_PRED', 'Я_NPRO', 'внимательно_ADVB', 'смотрю_VERB_1per_pres_sing', 'под_PREP', 'ноги_NOUN_accs_femn_plur', 'Здесь_ADVB', 'раньше_COMP', 'была_VERB_None_past_sing', 'дорога_NOUN_nomn_femn_sing', 'Высадившись_GRND', 'на_PREP', 'Луговине_NOUN_datv_femn_sing', 'я_NPRO', 'споткнулась_VERB_None_past_sing', 'о_PREP', 'камень_NOUN_nomn_masc_sing', 'Только_ADVB', 'оказалось_VERB_None_past_sing', 'что_CONJ', 'это_PRCL', 'не_PRCL', 'камень_NOUN_nomn_masc_sing', 'а_CONJ', 'человеческий_ADJF', 'череп_NOUN_accs_masc_sing', 'Он_NPRO', 'откатился_VERB_None_past_sing', 'зияющие_PRTF', 'глазницы_NOUN_nomn_femn_plur', 'уставились_VERB_None_past_plur', 'в_PREP', 'небо_NOUN_nomn_neut_sing', 'Я_NPRO', 'долго_ADVB', 'не_PRCL', 'могла_VERB_None_past_sing', 'отвести_INFN', 'взгляд_NOUN_accs_masc_sing', 'от_PREP', 'зубов_NOUN_gent_masc_plur', 'гадая_GRND', 'кому_NPRO', 'они_NPRO', 'принадлежали_VERB_None_past_plur', 'и_CONJ', 'представляя_GRND', 'как_CONJ', 'выглядели_VERB_None_past_plur', 'бы_PRCL', 'мои_ADJF', 'По_PREP', 'привычке_NOUN_datv_femn_sing', 'стараюсь_VERB_1per_pres_sing', 'придерживаться_INFN', 'дороги_NOUN_gent_femn_sing', 'но_CONJ', 'она_NPRO', 'усеяна_PRTS', 'останками_NOUN_ablt_None_plur', 'людей_NOUN_gent_masc_plur', 'пытавшихся_PRTF', 'спастись_INFN', 'бегством_NOUN_ablt_neut_sing', 'Некоторые_ADJF', 'сгорели_VERB_None_past_plur', 'полностью_ADVB', 'другие_ADJF', 'избежав_GRND', 'огня_NOUN_gent_masc_sing', 'задохнулись_VERB_None_past_plur', 'от_PREP', 'дыма_NOUN_gent_masc_sing', 'На_PREP', 'полуразложившихся_PRTF', 'трупах_NOUN_loct_masc_plur', 'кишат_VERB_3per_pres_plur', 'мухи_NOUN_nomn_femn_plur', 'Вас_NPRO', 'всех_ADJF', 'убила_VERB_None_past_sing', 'я._UNKN', 'И_CONJ', 'тебя_NPRO', 'И_CONJ', 'тебя_NPRO', 'И_CONJ', 'тебя…_UNKN', 'Потому_ADVB', 'что_CONJ', 'так_CONJ', 'и_CONJ', 'есть_INFN', 'Моя_ADJF', 'стрела_NOUN_nomn_femn_sing', 'разрушившая_PRTF', 'силовое_ADJF', 'поле_NOUN_loct_neut_sing', 'вызвала_VERB_None_past_sing', 'огненный_ADJF', 'шквал_NOUN_nomn_masc_sing', 'возмездия_NOUN_gent_neut_sing', 'и_CONJ', 'повергла_VERB_None_past_sing', 'в_PREP', 'хаос_NOUN_nomn_masc_sing', 'весь_ADJF', 'Панем_ADJS', 'В_PREP', 'голове_NOUN_loct_femn_sing', 'звучат_VERB_3per_pres_plur', 'слова_NOUN_gent_neut_sing', 'президента_NOUN_gent_masc_sing', 'Сноу_NOUN_nomn_None_sing', 'сказанные_PRTF', 'мне_NPRO', 'перед_PREP', 'туром_NOUN_ablt_masc_sing', 'победителей_NOUN_gent_masc_plur', 'Вы_NPRO', 'Огненная_ADJF', 'Китнисс_NOUN_gent_femn_plur', 'бросили_VERB_None_past_plur', 'искру_NOUN_accs_femn_sing', 'способную_ADJF', 'разгореться_INFN', 'в_PREP', 'адское_ADJF', 'пламя_NOUN_nomn_neut_sing', 'которое_ADJF', 'уничтожит_VERB_3per_futr_sing', 'Панем_ADJS', 'Он_NPRO', 'не_PRCL', 'преувеличивал_VERB_None_past_sing', 'Возможно_CONJ', 'он_NPRO', 'искренне_ADVB', 'пытался_VERB_None_past_sing', 'заручиться_INFN', 'моей_ADJF', 'поддержкой_NOUN_ablt_femn_sing', 'хотя_CONJ', 'от_PREP', 'меня_NPRO', 'уже_ADVB', 'ничего_ADVB', 'не_PRCL', 'зависело_VERB_None_past_sing', 'Пожар_NOUN_nomn_masc_sing', 'все_PRCL', 'еще_ADVB', 'продолжается_VERB_3per_pres_sing', 'машинально_ADVB', 'отмечаю_VERB_1per_pres_sing', 'я._UNKN', 'Угольные_ADJF', 'шахты_NOUN_nomn_femn_plur', 'вдали_ADVB', 'изрыгают_VERB_3per_pres_plur', 'клубы_NOUN_accs_masc_plur', 'черного_ADJF', 'дыма_NOUN_gent_masc_sing', 'Впрочем_CONJ', 'кому_NPRO', 'какое_ADJF', 'дело_NOUN_nomn_neut_sing', 'Почти_ADVB', 'все_PRCL', 'жители_NOUN_nomn_masc_plur', 'дистрикта_NOUN_gent_masc_sing', 'погибли_VERB_None_past_plur', 'Оставшиеся_PRTF', 'восемь_NUMR', 'процентов_NOUN_gent_masc_plur', 'укрылись_VERB_None_past_plur', 'в_PREP', 'Тринадцатом_ADJF', 'дистрикте_NOUN_loct_masc_sing', 'бесприютные_ADJF', 'беженцы_NOUN_nomn_masc_plur', 'навсегда_ADVB', 'лишившиеся_PRTF', 'родины_NOUN_gent_femn_sing', 'Конечно_CONJ', 'так_CONJ', 'думать_INFN', 'не_PRCL', 'следует_VERB_3per_pres_sing', 'Я_NPRO', 'должна_ADJS', 'быть_INFN', 'благодарна_ADJS', 'за_PREP', 'то_CONJ', 'что_CONJ', 'нас_NPRO', 'приняли_VERB_None_past_plur', 'Больных_ADJF', 'израненных_PRTF', 'голодных_ADJF', 'нищих_ADJF', 'Однако_CONJ', 'мне_NPRO', 'не_PRCL', 'дает_VERB_3per_pres_sing', 'покоя_NOUN_gent_masc_sing', 'мысль_NOUN_nomn_femn_sing', 'что_CONJ', 'если_CONJ', 'бы_PRCL', 'не_PRCL', 'мятежники_NOUN_nomn_masc_plur', 'Тринадцатого_ADJF', 'дистрикта_NOUN_gent_masc_sing', 'Двенадцатый_ADJF', 'был_VERB_None_past_sing', 'бы_PRCL', 'цел_ADJS', 'Это_PRCL', 'не_PRCL', 'снимает_VERB_3per_pres_sing', 'вины_NOUN_gent_femn_sing', 'с_PREP', 'меня_NPRO', 'ее_NPRO', 'хватит_VERB_None_futr_sing', 'на_PREP', 'всех_ADJF', 'Но_CONJ', 'что_CONJ', 'бы_PRCL', 'я_NPRO', 'смогла_VERB_None_past_sing', 'без_PREP', 'них_NPRO', 'Жители_NOUN_nomn_masc_plur', 'Двенадцатого_ADJF', 'дистрикта_NOUN_gent_masc_sing', 'вообще_ADVB', 'ни_PRCL', 'в_PREP', 'чем_CONJ', 'не_PRCL', 'виноваты_ADJS', 'им_NPRO', 'просто_PRCL', 'не_PRCL', 'повезло_VERB_None_past_sing', 'что_CONJ', 'у_PREP', 'них_NPRO', 'есть_INFN', 'я._UNKN', 'И_CONJ', 'все_PRCL', 'же_PRCL', 'некоторые_ADJF', 'выжившие_PRTF', 'рады_NOUN_gent_femn_sing', 'что_CONJ', 'вырвались_VERB_None_past_plur', 'оттуда_ADVB', 'Прочь_ADVB', 'от_PREP', 'голода_NOUN_gent_masc_sing', 'и_CONJ', 'гнета_NOUN_gent_masc_sing', 'гибельных_ADJF', 'шахт_NOUN_gent_femn_plur', 'от_PREP', 'плети_NOUN_accs_femn_plur', 'начальника_NOUN_gent_masc_sing', 'миротворцев_NOUN_gent_masc_plur', 'Ромулуса_NOUN_gent_masc_sing', 'Треда_NOUN_gent_masc_sing', 'Крыша_NOUN_nomn_femn_sing', 'над_PREP', 'головой_NOUN_ablt_femn_sing', 'уже_ADVB', 'счастье_NOUN_nomn_neut_sing', 'ведь_PRCL', 'до_PREP', 'недавнего_ADJF', 'времени_NOUN_gent_neut_sing', 'мы_NPRO', 'и_CONJ', 'не_PRCL', 'подозревали_VERB_None_past_plur', 'о_PREP', 'существовании_NOUN_loct_neut_sing', 'Тринадцатого_ADJF', 'дистрикта_NOUN_gent_masc_sing']\n",
            "ENG tagged tokens:\n",
            "['To_TO', 'reinforce_VB', 'this_DT', ',_,', 'I_PRP', 'begin_VBP', 'to_TO', 'move_VB', 'away_RB', 'from_IN', 'my_PRP$', 'old_JJ', 'house_NN', 'and_CC', 'in_IN', 'toward_IN', 'the_DT', 'town_NN', '._.', 'Gale_NNP', 'asked_VBD', 'to_TO', 'be_VB', 'dropped_VBN', 'off_IN', 'in_IN', '12_CD', 'with_IN', 'me_PRP', ',_,', 'but_CC', 'he_PRP', 'did_VBD', \"n't_RB\", 'force_VB', 'the_DT', 'issue_NN', 'when_WRB', 'I_PRP', 'refused_VBD', 'his_PRP$', 'company_NN', '._.', 'He_PRP', 'understands_VBZ', 'I_PRP', 'do_VBP', \"n't_RB\", 'want_VB', 'anyone_NN', 'with_IN', 'me_PRP', 'today_NN', '._.', 'Not_RB', 'even_RB', 'him_PRP', '._.', 'Some_DT', 'walks_NNS', 'you_PRP', 'have_VBP', 'to_TO', 'take_VB', 'alone_RB', '._.', 'The_DT', 'summer_NN', \"'s_POS\", 'been_VBN', 'scorching_VBG', 'hot_JJ', 'and_CC', 'dry_JJ', 'as_IN', 'a_DT', 'bone_NN', '._.', 'There_EX', \"'s_VBZ\", 'been_VBN', 'next_JJ', 'to_TO', 'no_DT', 'rain_NN', 'to_TO', 'disturb_VB', 'the_DT', 'piles_NNS', 'of_IN', 'ash_NN', 'left_VBN', 'by_IN', 'the_DT', 'attack_NN', '._.', 'They_PRP', 'shift_VBP', 'here_RB', 'and_CC', 'there_RB', ',_,', 'in_IN', 'reaction_NN', 'to_TO', 'my_PRP$', 'footsteps_NNS', '._.', 'No_DT', 'breeze_NN', 'to_TO', 'scatter_VB', 'them_PRP', '._.', 'I_PRP', 'keep_VBP', 'my_PRP$', 'eyes_NNS', 'on_IN', 'what_WP', 'I_PRP', 'remember_VBP', 'as_IN', 'the_DT', 'road_NN', ',_,', 'because_IN', 'when_WRB', 'I_PRP', 'first_RB', 'landed_VBD', 'in_IN', 'the_DT', 'Meadow_NNP', ',_,', 'I_PRP', 'was_VBD', \"n't_RB\", 'careful_JJ', 'and_CC', 'I_PRP', 'walked_VBD', 'right_RB', 'into_IN', 'a_DT', 'rock_NN', '._.', 'Only_RB', 'it_PRP', 'was_VBD', \"n't_RB\", 'a_DT', 'rock_NN', '--_:', 'it_PRP', 'was_VBD', 'someone_NN', \"'s_POS\", 'skull_NN', '._.', 'It_PRP', 'rolled_VBD', 'over_RB', 'and_CC', 'over_RB', 'and_CC', 'landed_JJ', 'faceup_NN', ',_,', 'and_CC', 'for_IN', 'a_DT', 'long_JJ', 'time_NN', 'I_PRP', 'could_MD', \"n't_RB\", 'stop_VB', 'looking_VBG', 'at_IN', 'the_DT', 'teeth_NN', ',_,', 'wondering_VBG', 'whose_WP$', 'they_PRP', 'were_VBD', ',_,', 'thinking_VBG', 'of_IN', 'how_WRB', 'mine_NN', 'would_MD', 'probably_RB', 'look_VB', 'the_DT', 'same_JJ', 'way_NN', 'under_IN', 'similar_JJ', 'circumstances_NNS', '._.', 'I_PRP', 'stick_VBP', 'to_TO', 'the_DT', 'road_NN', 'out_IN', 'of_IN', 'habit_NN', ',_,', 'but_CC', 'it_PRP', \"'s_VBZ\", 'a_DT', 'bad_JJ', 'choice_NN', ',_,', 'because_IN', 'it_PRP', \"'s_VBZ\", 'full_JJ', 'of_IN', 'the_DT', 'remains_NNS', 'of_IN', 'those_DT', 'who_WP', 'tried_VBD', 'to_TO', 'flee_VB', '._.', 'Some_DT', 'were_VBD', 'incinerated_VBN', 'entirely_RB', '._.', 'But_CC', 'others_NNS', ',_,', 'probably_RB', 'overcome_VB', 'with_IN', 'smoke_NN', ',_,', 'escaped_VBD', 'the_DT', 'worst_JJS', 'of_IN', 'the_DT', 'flames_NNS', 'and_CC', 'now_RB', 'lie_VB', 'reeking_VBG', 'in_IN', 'various_JJ', 'states_NNS', 'of_IN', 'decomposition_NN', ',_,', 'carrion_NN', 'for_IN', 'scavengers_NNS', ',_,', 'blanketed_VBN', 'by_IN', 'flies_NNS', '._.', 'I_PRP', 'killed_VBD', 'you_PRP', ',_,', 'I_PRP', 'think_VBP', 'as_IN', 'I_PRP', 'pass_VBP', 'a_DT', 'pile_NN', '._.', 'And_CC', 'you_PRP', '._.', 'And_CC', 'you_PRP', '._.', 'Because_IN', 'I_PRP', 'did_VBD', '._.', 'It_PRP', 'was_VBD', 'my_PRP$', 'arrow_NN', ',_,', 'aimed_VBN', 'at_IN', 'the_DT', 'chink_NN', 'in_IN', 'the_DT', 'force_NN', 'field_NN', 'surrounding_VBG', 'the_DT', 'arena_NN', ',_,', 'that_WDT', 'brought_VBD', 'on_IN', 'this_DT', 'firestorm_NN', 'of_IN', 'retribution_NN', '._.', 'That_WDT', 'sent_VBD', 'the_DT', 'whole_JJ', 'country_NN', 'of_IN', 'Panem_NNP', 'into_IN', 'chaos_NN', '._.', 'In_IN', 'my_PRP$', 'head_NN', 'I_PRP', 'hear_VBP', 'President_NNP', 'Snow_NNP', \"'s_POS\", 'words_NNS', ',_,', 'spoken_VBD', 'the_DT', 'morning_NN', 'I_PRP', 'was_VBD', 'to_TO', 'begin_VB', 'the_DT', 'Victory_NNP', 'Tour_NNP', '._.', '``_``', 'Katniss_NNP', 'Everdeen_NNP', ',_,', 'the_DT', 'girl_NN', 'who_WP', 'was_VBD', 'on_IN', 'fire_NN', ',_,', 'you_PRP', 'have_VBP', 'provided_VBN', 'a_DT', 'spark_NN', 'that_IN', ',_,', 'left_VBD', 'unattended_JJ', ',_,', 'may_MD', 'grow_VB', 'to_TO', 'an_DT', 'inferno_NN', 'that_WDT', 'destroys_VBZ', 'Panem_NNP', '._.', \"''_''\", 'It_PRP', 'turns_VBZ', 'out_RP', 'he_PRP', 'was_VBD', \"n't_RB\", 'exaggerating_VBG', 'or_CC', 'simply_RB', 'trying_VBG', 'to_TO', 'scare_VB', 'me_PRP', '._.']\n",
            "\n",
            "RUS POS frequency:\n",
            "   pos_tag  count\n",
            "0     NOUN     82\n",
            "1     VERB     46\n",
            "2     ADJF     38\n",
            "3     CONJ     35\n",
            "4     PREP     32\n",
            "5     NPRO     31\n",
            "6     PRCL     28\n",
            "7     ADVB     21\n",
            "8     INFN     12\n",
            "9     PRTF      9\n",
            "10    ADJS      6\n",
            "11    GRND      4\n",
            "12    UNKN      4\n",
            "13    PRED      2\n",
            "14    COMP      1\n",
            "15    PRTS      1\n",
            "16    NUMR      1\n",
            "\n",
            "ENG POS frequency:\n",
            "   pos_tag  count\n",
            "0       NN     45\n",
            "1       IN     41\n",
            "2      PRP     38\n",
            "3       DT     35\n",
            "4       RB     22\n",
            "5      VBD     22\n",
            "6       VB     16\n",
            "7       JJ     14\n",
            "8       CC     14\n",
            "9       TO     13\n",
            "10     VBP     13\n",
            "11     NNS     11\n",
            "12     NNP     10\n",
            "13     VBN      8\n",
            "14     VBG      8\n",
            "15    PRP$      6\n",
            "16     VBZ      6\n",
            "17     WDT      4\n",
            "18     WRB      3\n",
            "19     POS      3\n",
            "20      WP      3\n",
            "21      MD      3\n",
            "22      CD      1\n",
            "23      EX      1\n",
            "24     WP$      1\n",
            "25     JJS      1\n",
            "26      RP      1\n",
            "\n",
            "RUS tagged table:\n",
            "             token pos_tag  case gender number person tense\n",
            "0                В    PREP  None   None   None   None  None\n",
            "1    подтверждение    NOUN  nomn   neut   sing   None  None\n",
            "2           отхожу    VERB  None   None   sing   1per  futr\n",
            "3               от    PREP  None   None   None   None  None\n",
            "4            моего    ADJF  gent   neut   sing   None  None\n",
            "..             ...     ...   ...    ...    ...    ...   ...\n",
            "348    подозревали    VERB  None   None   plur   None  past\n",
            "349              о    PREP  None   None   None   None  None\n",
            "350  существовании    NOUN  loct   neut   sing   None  None\n",
            "351   Тринадцатого    ADJF  gent   masc   sing   None  None\n",
            "352      дистрикта    NOUN  gent   masc   sing   None  None\n",
            "\n",
            "[353 rows x 7 columns]\n",
            "\n",
            "ENG tagged table:\n",
            "         token pos_tag\n",
            "0           To      TO\n",
            "1    reinforce      VB\n",
            "2         this      DT\n",
            "3            ,       ,\n",
            "4            I     PRP\n",
            "..         ...     ...\n",
            "387     trying     VBG\n",
            "388         to      TO\n",
            "389      scare      VB\n",
            "390         me     PRP\n",
            "391          .       .\n",
            "\n",
            "[392 rows x 2 columns]\n",
            "\n",
            "Russian pluralized text:\n",
            "В подтверждения отхожу от моего прежнего домов и бреду в центры . гейлы хотел высадиться вместе со мной, но не настаивал, когда я отказалась . Понимает: любая компании мне сегодня в тягости . Даже его . Есть дорог, которые нужно пройти в одиночки . Лето выдалось сухим и знойным . Редкие дожди не смыли куч зол . Она вспархивает от моих шагов и тут же оседает . ветров нет . Я внимательно смотрю под ноги . Здесь раньше была дороги . Высадившись на луговинам, я споткнулась о камни . Только оказалось, что это не камни, а человеческий черепа . Он откатился, зияющие глазницы уставились в небеса . Я долго не могла отвести взгляды от зубов, гадая, кому они принадлежали, и представляя, как выглядели бы мои . По привычкам стараюсь придерживаться дорог, но она усеяна останками людей, пытавшихся спастись бегствами . Некоторые сгорели полностью, другие, избежав огней, задохнулись от дымов . На полуразложившихся трупах кишат мухи . Вас всех убила я. И тебя . И тебя . И тебя… Потому что так и есть . Моя стрелы, разрушившая силовое полях, вызвала огненный шквалы возмездий и повергла в хаосы весь Панем . В головах звучат слов президентов сноу, сказанные мне перед турами победителей: « Вы, Огненная Китнисс, бросили искры, способную разгореться в адское пламена, которое уничтожит Панем » . Он не преувеличивал . Возможно, он искренне пытался заручиться моей поддержками, хотя от меня уже ничего не зависело . пожары все еще продолжается, машинально отмечаю я. Угольные шахты вдали изрыгают клубы черного дымов . Впрочем, кому какое дела? Почти все жители дистриктов погибли . Оставшиеся восемь процентов укрылись в Тринадцатом дистриктах — бесприютные беженцы, навсегда лишившиеся родин . Конечно, так думать не следует . Я должна быть благодарна за то, что нас приняли . Больных, израненных, голодных, нищих . Однако мне не дает покоев мысли, что, если бы не мятежники Тринадцатого дистриктов, Двенадцатый был бы цел . Это не снимает вин с меня — ее хватит на всех . Но что бы я смогла без них? Жители Двенадцатого дистриктов вообще ни в чем не виноваты, им просто не повезло, что у них есть я. И все же некоторые выжившие рад, что вырвались оттуда . Прочь от голодов и гнётов, гибельных шахт, от плети начальников миротворцев ромулусов тредов . крыши над головами — уже счастья, ведь до недавнего времён мы и не подозревали о существованиях Тринадцатого дистриктов.\n",
            "\n",
            "English pluralized text:\n",
            "To reinforce this, I begin to move away from my old houses and in toward the towns. Gales asked to be dropped off in 12 with me, but he didn't force the issues when I refused his companies. He understands I don't want anyones with me todays. Not even him. Some walks you have to take alone. The summers's been scorching hot and dry as a bones. There's been next to no rains to disturb the piles of ashes left by the attacks. They shift here and there, in reactions to my footsteps. No breezes to scatter them. I keep my eyes on what I remember as the roads, because when I first landed in the Meadows, I wasn't careful and I walked right into a rocks. Only it wasn't a rocks--it was someones's skulls. It rolled over and over and landed faceups, and for a long times I couldn't stop looking at the teeth, wondering whose they were, thinking of how mine would probably look the same ways under similar circumstances. I stick to the roads out of habits, but it's a bad choices, because it's full of the remains of those who tried to flee. Some were incinerated entirely. But others, probably overcome with smokes, escaped the worst of the flames and now lie reekings in various states of decompositions, carrions for scavengers, blanketed by flies. I killed you, I think as I pass a piles. And you. And you. Because I did. It was my arrows, aimed at the chinks in the forces fields surrounding the arenas, that brought on this firestorms of retributions. That sent the whole countries of Panems into chaoses. In my heads I hear Presidents Snows's words, spoken the mornings I was to begin the Victorys Tours. \"Katnisses Everdeens, the girls who was on fires, you have provided a sparks that, left unattended, may grow to an infernos that destroys Panems.\" It turns out he wasn't exaggerating or simply trying to scare me. \n"
          ]
        }
      ],
      "source": [
        "!pip install inflect\n",
        "\n",
        "import re\n",
        "import nltk\n",
        "import pymorphy3\n",
        "import inflect\n",
        "import spacy\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import pos_tag\n",
        "from collections import Counter\n",
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "nltk.download('punkt')\n",
        "\n",
        "russian_text = \"\"\"В подтверждение отхожу от моего прежнего дома и бреду в центр. Гейл хотел высадиться вместе со мной, но не настаивал, когда я отказалась. Понимает: любая компания мне сегодня в тягость. Даже его. Есть дороги, которые нужно пройти в одиночку.\n",
        "Лето выдалось сухим и знойным. Редкие дожди не смыли кучи золы. Она вспархивает от моих шагов и тут же оседает. Ветра нет. Я внимательно смотрю под ноги. Здесь раньше была дорога. Высадившись на Луговине, я споткнулась о камень. Только оказалось, что это не камень, а человеческий череп. Он откатился, зияющие глазницы уставились в небо. Я долго не могла отвести взгляд от зубов, гадая, кому они принадлежали, и представляя, как выглядели бы мои.\n",
        "По привычке стараюсь придерживаться дороги, но она усеяна останками людей, пытавшихся спастись бегством. Некоторые сгорели полностью, другие, избежав огня, задохнулись от дыма. На полуразложившихся трупах кишат мухи. Вас всех убила я. И тебя. И тебя. И тебя…\n",
        "Потому что так и есть. Моя стрела, разрушившая силовое поле, вызвала огненный шквал возмездия и повергла в хаос весь Панем.\n",
        "В голове звучат слова президента Сноу, сказанные мне перед туром победителей: «Вы, Огненная Китнисс, бросили искру, способную разгореться в адское пламя, которое уничтожит Панем». Он не преувеличивал. Возможно, он искренне пытался заручиться моей поддержкой, хотя от меня уже ничего не зависело.\n",
        "Пожар все еще продолжается, машинально отмечаю я. Угольные шахты вдали изрыгают клубы черного дыма. Впрочем, кому какое дело? Почти все жители дистрикта погибли. Оставшиеся восемь процентов укрылись в Тринадцатом дистрикте — бесприютные беженцы, навсегда лишившиеся родины.\n",
        "Конечно, так думать не следует. Я должна быть благодарна за то, что нас приняли. Больных, израненных, голодных, нищих. Однако мне не дает покоя мысль, что, если бы не мятежники Тринадцатого дистрикта, Двенадцатый был бы цел. Это не снимает вины с меня — ее хватит на всех. Но что бы я смогла без них?\n",
        "Жители Двенадцатого дистрикта вообще ни в чем не виноваты, им просто не повезло, что у них есть я. И все же некоторые выжившие рады, что вырвались оттуда. Прочь от голода и гнета, гибельных шахт, от плети начальника миротворцев Ромулуса Треда. Крыша над головой — уже счастье, ведь до недавнего времени мы и не подозревали о существовании Тринадцатого дистрикта.\"\"\"\n",
        "\n",
        "english_text = \"\"\"To reinforce this, I begin to move away from my old house and in toward the town. Gale asked to be dropped off in 12 with me, but he didn't force the issue when I refused his company. He understands I don't want anyone with me today. Not even him. Some walks you have to take alone. The summer's been scorching hot and dry as a bone. There's been next to no rain to disturb the piles of ash left by the attack. They shift here and there, in reaction to my footsteps. No breeze to scatter them. I keep my eyes on what I remember as the road, because when I first landed in the Meadow, I wasn't careful and I walked right into a rock. Only it wasn't a rock--it was someone's skull. It rolled over and over and landed faceup, and for a long time I couldn't stop looking at the teeth, wondering whose they were, thinking of how mine would probably look the same way under similar circumstances. I stick to the road out of habit, but it's a bad choice, because it's full of the remains of those who tried to flee. Some were incinerated entirely. But others, probably overcome with smoke, escaped the worst of the flames and now lie reeking in various states of decomposition, carrion for scavengers, blanketed by flies. I killed you, I think as I pass a pile. And you. And you. Because I did. It was my arrow, aimed at the chink in the force field surrounding the arena, that brought on this firestorm of retribution. That sent the whole country of Panem into chaos. In my head I hear President Snow's words, spoken the morning I was to begin the Victory Tour. \"Katniss Everdeen, the girl who was on fire, you have provided a spark that, left unattended, may grow to an inferno that destroys Panem.\" It turns out he wasn't exaggerating or simply trying to scare me. \"\"\"\n",
        "\n",
        "eng_tokens = word_tokenize(english_text)\n",
        "eng_tagged = pos_tag(eng_tokens)\n",
        "\n",
        "eng_tagged_joined = [\"_\".join(pair) for pair in eng_tagged]\n",
        "\n",
        "eng_tokens_np = [t for t in eng_tokens if re.search(r\"[A-Za-z0-9]\", t)]\n",
        "eng_pos_counts = Counter(tag for _, tag in pos_tag(eng_tokens_np))\n",
        "\n",
        "eng_df = pd.DataFrame(eng_tagged, columns=[\"token\", \"pos_tag\"])\n",
        "eng_freq_df = pd.DataFrame(\n",
        "    eng_pos_counts.most_common(),\n",
        "    columns=[\"pos_tag\", \"count\"]\n",
        ")\n",
        "\n",
        "morph = pymorphy3.MorphAnalyzer()\n",
        "rus_tokens = word_tokenize(russian_text)\n",
        "\n",
        "rus_list_of_tagged = []\n",
        "rus_pos_counts = Counter()\n",
        "rus_rows = []\n",
        "\n",
        "for word in rus_tokens:\n",
        "    if all(not ch.isalnum() and ch not in \"ёЁ\" for ch in word):\n",
        "        continue\n",
        "\n",
        "    p = morph.parse(word)[0]\n",
        "    pos = p.tag.POS or \"UNKN\"\n",
        "    rus_pos_counts[pos] += 1\n",
        "\n",
        "    if pos == \"NOUN\":\n",
        "        rus_list_of_tagged.append(\n",
        "            f\"{word}_{pos}_{p.tag.case}_{p.tag.gender}_{p.tag.number}\"\n",
        "        )\n",
        "    elif pos == \"VERB\":\n",
        "        rus_list_of_tagged.append(\n",
        "            f\"{word}_{pos}_{p.tag.person}_{p.tag.tense}_{p.tag.number}\"\n",
        "        )\n",
        "    else:\n",
        "        rus_list_of_tagged.append(f\"{word}_{pos}\")\n",
        "\n",
        "    rus_rows.append({\n",
        "        \"token\": word,\n",
        "        \"pos_tag\": pos,\n",
        "        \"case\": p.tag.case,\n",
        "        \"gender\": p.tag.gender,\n",
        "        \"number\": p.tag.number,\n",
        "        \"person\": p.tag.person,\n",
        "        \"tense\": p.tag.tense,\n",
        "    })\n",
        "\n",
        "rus_df = pd.DataFrame(rus_rows)\n",
        "rus_freq_df = pd.DataFrame(\n",
        "    rus_pos_counts.most_common(),\n",
        "    columns=[\"pos_tag\", \"count\"]\n",
        ")\n",
        "\n",
        "p_inflect = inflect.engine()\n",
        "nlp_en = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def pluralize_nouns_pymorphy():\n",
        "    new_tokens = []\n",
        "\n",
        "    for w in rus_tokens:\n",
        "        if any(ch.isalpha() for ch in w):\n",
        "            p = morph.parse(w)[0]\n",
        "            if p.tag.POS == \"NOUN\" and p.tag.number == \"sing\":\n",
        "                gramm = {\"plur\"}\n",
        "                if p.tag.case:\n",
        "                    gramm.add(p.tag.case)\n",
        "                if p.tag.animacy:\n",
        "                    gramm.add(p.tag.animacy)\n",
        "\n",
        "                inf = p.inflect(gramm)\n",
        "                if inf:\n",
        "                    w = inf.word\n",
        "        new_tokens.append(w)\n",
        "\n",
        "    return new_tokens\n",
        "\n",
        "russian_pluralized = detok.detokenize(pluralize_nouns_pymorphy())\n",
        "\n",
        "def pluralize_nouns_spacy(text: str) -> str:\n",
        "    doc = nlp_en(text)\n",
        "    out = []\n",
        "\n",
        "    for tok in doc:\n",
        "        if tok.tag_ in (\"NN\", \"NNP\"):\n",
        "            pl = p_inflect.plural_noun(tok.text) or p_inflect.plural(tok.text)\n",
        "\n",
        "            if tok.text.isupper():\n",
        "                pl = pl.upper()\n",
        "            elif tok.text.istitle():\n",
        "                pl = pl.title()\n",
        "\n",
        "            out.append(pl + tok.whitespace_)\n",
        "        else:\n",
        "            out.append(tok.text_with_ws)\n",
        "\n",
        "    return \"\".join(out)\n",
        "\n",
        "english_pluralized = pluralize_nouns_spacy(english_text)\n",
        "\n",
        "detok = TreebankWordDetokenizer()\n",
        "\n",
        "print(\"\\nRUS tagged tokens:\")\n",
        "print(rus_list_of_tagged)\n",
        "\n",
        "print(\"ENG tagged tokens:\")\n",
        "print(eng_tagged_joined)\n",
        "\n",
        "print(\"\\nRUS POS frequency:\")\n",
        "print(rus_freq_df)\n",
        "\n",
        "\n",
        "print(\"\\nENG POS frequency:\")\n",
        "print(eng_freq_df)\n",
        "\n",
        "print(\"\\nRUS tagged table:\")\n",
        "print(rus_df)\n",
        "\n",
        "print(\"\\nENG tagged table:\")\n",
        "print(eng_df)\n",
        "\n",
        "print(\"\\nRussian pluralized text:\")\n",
        "print(russian_pluralized)\n",
        "\n",
        "print(\"\\nEnglish pluralized text:\")\n",
        "print(english_pluralized)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azGFuWTkOCnX"
      },
      "source": [
        "**Задание 3: Синтаксический анализ предложений**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MWWlpPkjOFpg"
      },
      "source": [
        "1. Возьмите 2 простых и 3 сложных предложений на русском и английском языке (всего 10 предложений).\n",
        "2. Используйте SpaCy для построения синтаксических деревьев этих предложений.\n",
        "3. Для каждого предложения:\n",
        "* Визуализируйте синтаксическое дерево\n",
        "* Выделите все подлежащие и сказуемые\n",
        "* Найдите все пары слов, связанные отношением определения (прилагательное-существительное)\n",
        "4. Разработайте функцию для извлечения всех объектных и субъектных отношений из предложения в формате (субъект, предикат, объект).\n",
        "5. Объясните, какие трудности возникают при синтаксическом анализе сложных предложений"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "pHi7VumCOcqr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting ru-core-news-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/ru_core_news_md-3.8.0/ru_core_news_md-3.8.0-py3-none-any.whl (41.9 MB)\n",
            "     ---------------------------------------- 0.0/41.9 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/41.9 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/41.9 MB ? eta -:--:--\n",
            "      --------------------------------------- 0.5/41.9 MB 1.9 MB/s eta 0:00:23\n",
            "     - -------------------------------------- 1.6/41.9 MB 3.6 MB/s eta 0:00:12\n",
            "     -- ------------------------------------- 2.6/41.9 MB 4.6 MB/s eta 0:00:09\n",
            "     --- ------------------------------------ 3.1/41.9 MB 3.8 MB/s eta 0:00:11\n",
            "     ----- ---------------------------------- 5.2/41.9 MB 5.2 MB/s eta 0:00:08\n",
            "     ----- ---------------------------------- 6.0/41.9 MB 5.3 MB/s eta 0:00:07\n",
            "     -------- ------------------------------- 8.7/41.9 MB 6.1 MB/s eta 0:00:06\n",
            "     ---------- ----------------------------- 11.3/41.9 MB 6.8 MB/s eta 0:00:05\n",
            "     ------------ --------------------------- 13.1/41.9 MB 7.0 MB/s eta 0:00:05\n",
            "     -------------- ------------------------- 15.2/41.9 MB 7.2 MB/s eta 0:00:04\n",
            "     ---------------- ----------------------- 17.3/41.9 MB 7.5 MB/s eta 0:00:04\n",
            "     ------------------ --------------------- 18.9/41.9 MB 7.6 MB/s eta 0:00:04\n",
            "     ------------------- -------------------- 20.7/41.9 MB 7.7 MB/s eta 0:00:03\n",
            "     --------------------- ------------------ 22.8/41.9 MB 7.8 MB/s eta 0:00:03\n",
            "     ----------------------- ---------------- 24.9/41.9 MB 7.9 MB/s eta 0:00:03\n",
            "     ------------------------- -------------- 26.7/41.9 MB 8.0 MB/s eta 0:00:02\n",
            "     --------------------------- ------------ 28.8/41.9 MB 8.1 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 30.9/41.9 MB 8.2 MB/s eta 0:00:02\n",
            "     ------------------------------- -------- 33.0/41.9 MB 8.3 MB/s eta 0:00:02\n",
            "     --------------------------------- ------ 35.1/41.9 MB 8.4 MB/s eta 0:00:01\n",
            "     ----------------------------------- ---- 37.5/41.9 MB 8.5 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 38.5/41.9 MB 8.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  40.9/41.9 MB 8.6 MB/s eta 0:00:01\n",
            "     ---------------------------------------  41.7/41.9 MB 8.3 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 41.9/41.9 MB 8.2 MB/s eta 0:00:00\n",
            "Requirement already satisfied: pymorphy3>=1.0.0 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from ru-core-news-md==3.8.0) (2.0.6)\n",
            "Requirement already satisfied: dawg2-python>=0.8.0 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (0.9.0)\n",
            "Requirement already satisfied: pymorphy3-dicts-ru in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (2.4.417150.4580142)\n",
            "Requirement already satisfied: setuptools>=68.2.2 in c:\\users\\home\\lancsboxx\\miniconda\\lib\\site-packages (from pymorphy3>=1.0.0->ru-core-news-md==3.8.0) (75.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('ru_core_news_md')\n",
            "Collecting en-core-web-md==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.0/33.5 MB ? eta -:--:--\n",
            "     ---------------------------------------- 0.3/33.5 MB ? eta -:--:--\n",
            "     - -------------------------------------- 1.3/33.5 MB 3.2 MB/s eta 0:00:11\n",
            "     -- ------------------------------------- 2.4/33.5 MB 4.5 MB/s eta 0:00:07\n",
            "     ---- ----------------------------------- 3.9/33.5 MB 5.1 MB/s eta 0:00:06\n",
            "     -------- ------------------------------- 6.8/33.5 MB 6.9 MB/s eta 0:00:04\n",
            "     ---------- ----------------------------- 8.7/33.5 MB 7.2 MB/s eta 0:00:04\n",
            "     ------------ --------------------------- 10.2/33.5 MB 7.4 MB/s eta 0:00:04\n",
            "     -------------- ------------------------- 12.1/33.5 MB 7.5 MB/s eta 0:00:03\n",
            "     ---------------- ----------------------- 13.6/33.5 MB 7.5 MB/s eta 0:00:03\n",
            "     ------------------ --------------------- 15.7/33.5 MB 7.7 MB/s eta 0:00:03\n",
            "     -------------------- ------------------- 17.3/33.5 MB 7.7 MB/s eta 0:00:03\n",
            "     ---------------------- ----------------- 19.1/33.5 MB 7.8 MB/s eta 0:00:02\n",
            "     ------------------------- -------------- 21.0/33.5 MB 7.9 MB/s eta 0:00:02\n",
            "     --------------------------- ------------ 23.1/33.5 MB 8.0 MB/s eta 0:00:02\n",
            "     ----------------------------- ---------- 24.9/33.5 MB 8.1 MB/s eta 0:00:02\n",
            "     -------------------------------- ------- 27.0/33.5 MB 8.1 MB/s eta 0:00:01\n",
            "     ---------------------------------- ----- 28.8/33.5 MB 8.2 MB/s eta 0:00:01\n",
            "     ------------------------------------ --- 30.7/33.5 MB 8.3 MB/s eta 0:00:01\n",
            "     -------------------------------------- - 32.5/33.5 MB 8.4 MB/s eta 0:00:01\n",
            "     ---------------------------------------- 33.5/33.5 MB 8.2 MB/s eta 0:00:00\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_md')\n",
            "Визуализация синтакисческого дерева для русского языка\n",
            "\n",
            "Предложение 1\n"
          ]
        },
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'display' from 'IPython.core.display' (c:\\Users\\Home\\LancsBoxX\\miniconda\\Lib\\site-packages\\IPython\\core\\display.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m     doc_ru = nlp_ru(sentence)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mПредложение \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrussian_sentences.index(sentence)\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m     \u001b[43mdisplacy\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrender\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc_ru\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstyle\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjupyter\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdistance\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompact\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mВизуализация синтакисческого дерева для английского языка\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m english_sentences:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Home\\LancsBoxX\\miniconda\\Lib\\site-packages\\spacy\\displacy\\__init__.py:69\u001b[39m, in \u001b[36mrender\u001b[39m\u001b[34m(docs, style, page, minify, jupyter, options, manual)\u001b[39m\n\u001b[32m     65\u001b[39m     html = RENDER_WRAPPER(html)\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mor\u001b[39;00m (jupyter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m is_in_jupyter()):\n\u001b[32m     67\u001b[39m     \u001b[38;5;66;03m# return HTML rendered by IPython display()\u001b[39;00m\n\u001b[32m     68\u001b[39m     \u001b[38;5;66;03m# See #4840 for details on span wrapper to disable mathjax\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mIPython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdisplay\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTML, display\n\u001b[32m     71\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m display(HTML(\u001b[33m'\u001b[39m\u001b[33m<span class=\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtex2jax_ignore\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[33m</span>\u001b[39m\u001b[33m'\u001b[39m.format(html)))\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m html\n",
            "\u001b[31mImportError\u001b[39m: cannot import name 'display' from 'IPython.core.display' (c:\\Users\\Home\\LancsBoxX\\miniconda\\Lib\\site-packages\\IPython\\core\\display.py)"
          ]
        }
      ],
      "source": [
        "!python -m spacy download ru_core_news_md\n",
        "!python -m spacy download en_core_web_md\n",
        "\n",
        "nlp_ru = spacy.load(\"ru_core_news_md\")\n",
        "nlp_en = spacy.load(\"en_core_web_md\")\n",
        "from spacy import displacy\n",
        "\n",
        "russian_sentences = [\n",
        "    \"Я гуляю по улице.\",\n",
        "    \"Я люблю хорошую погоду.\",\n",
        "    \"Когда я вышел на улицу, начался сильный дождь, хотя утром светило солнце.\",\n",
        "    \"Девушка, которая учится в университете, написала интересную статью.\",\n",
        "    \"Если бы он знал правду, то не принял бы такое решение, но теперь уже ничего не изменить.\"\n",
        "]\n",
        "\n",
        "english_sentences = [\n",
        "    \"The cat sleeps on the mat.\",\n",
        "    \"She reads interesting books.\",\n",
        "    \"Although it was raining heavily, we decided to go for a walk because we had promised our friends.\",\n",
        "    \"The book that I read yesterday, which was written by a famous author, contains many fascinating stories about ancient civilizations.\",\n",
        "    \"If I had known about the meeting earlier, I would have prepared better, but unfortunately nobody informed me in time.\"\n",
        "]\n",
        "\n",
        "print('Визуализация синтакисческого дерева для русского языка\\n')\n",
        "for sentence in russian_sentences:\n",
        "    doc_ru = nlp_ru(sentence)\n",
        "\n",
        "    print(f\"Предложение {russian_sentences.index(sentence) + 1}\")\n",
        "    displacy.render(doc_ru, style=\"dep\", jupyter=True, options={\"distance\": 100, \"compact\": True})\n",
        "\n",
        "print('Визуализация синтакисческого дерева для английского языка\\n')\n",
        "for sentence in english_sentences:\n",
        "    doc_en = nlp_en(sentence)\n",
        "    print(f\"Sentence {english_sentences.index(sentence) + 1}\")\n",
        "    displacy.render(doc_en, style=\"dep\", jupyter=True, options={\"distance\": 100, \"compact\": True})\n",
        "\n",
        "def find_subj_pred_rus(doc):\n",
        "    root = None\n",
        "    pred_tokens = []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"ROOT\":\n",
        "            root = token\n",
        "            pred_tokens.append(token)\n",
        "            break\n",
        "\n",
        "    if not root:\n",
        "        return [], []\n",
        "\n",
        "    changed = True\n",
        "    while changed:\n",
        "        changed = False\n",
        "        for t in doc:\n",
        "            if t.dep_ == \"conj\" and t.head in pred_tokens and t.pos_ in {\"VERB\", \"AUX\"}:\n",
        "                if t not in pred_tokens:\n",
        "                    pred_tokens.append(t)\n",
        "                    changed = True\n",
        "\n",
        "    def pred_text(p):\n",
        "        parts = {p}\n",
        "        for ch in p.children:\n",
        "            if ch.dep_ in {\"aux\", \"auxpass\", \"cop\", \"neg\"}:\n",
        "                parts.add(ch)\n",
        "            if ch.dep_ == \"xcomp\" and ch.pos_ in {\"VERB\", \"AUX\"}:\n",
        "                parts.add(ch)\n",
        "        return \" \".join(tok.text for tok in sorted(parts, key=lambda x: x.i))\n",
        "\n",
        "    def subjects(p):\n",
        "        subs = [ch for ch in p.children if ch.dep_ in {\"nsubj\", \"nsubjpass\", \"nsubj:pass\"}]\n",
        "        if subs:\n",
        "            return subs\n",
        "        if p.dep_ == \"conj\":\n",
        "            return subjects(p.head)\n",
        "        return []\n",
        "\n",
        "    all_subj = []\n",
        "    all_pred = []\n",
        "    for p in sorted(pred_tokens, key=lambda x: x.i):\n",
        "        all_pred.append(pred_text(p))\n",
        "        all_subj.extend([s.text for s in subjects(p)])\n",
        "\n",
        "    return all_subj, all_pred\n",
        "for text in russian_sentences:\n",
        "  doc = nlp_ru(text)\n",
        "  subject, predicate = find_subj_pred_rus(doc)\n",
        "  print(f\"Предложение: {text}\")\n",
        "  print(f\"Подлежащее: {subject}, Сказуемое: {predicate}\")\n",
        "  print()\n",
        "\n",
        "\n",
        "def find_subj_pred_eng(doc):\n",
        "    subj = []\n",
        "    pred_tokens = []\n",
        "    root = None\n",
        "    for token in doc:\n",
        "        if token.dep_ == \"ROOT\":\n",
        "            root = token\n",
        "            pred_tokens.append(token)\n",
        "            break\n",
        "    if not root:\n",
        "        return [], []\n",
        "\n",
        "    for token in doc:\n",
        "        if token.dep_ in [\"aux\", \"auxpass\"] and token.head in [root] + [t for t in pred_tokens]:\n",
        "            pred_tokens.append(token)\n",
        "    pred = [doc[i].text for i in sorted([t.i for t in pred_tokens])]\n",
        "    for token in doc:\n",
        "        if token.dep_ in [\"nsubj\", \"nsubjpass\"]:\n",
        "            current = token\n",
        "            while current.head != current:\n",
        "                if current.head in pred_tokens or current.head == root:\n",
        "                    subj.append(token.text)\n",
        "                    break\n",
        "                current = current.head\n",
        "    if not subj:\n",
        "        for token in doc:\n",
        "            if token.dep_ in [\"nsubj\", \"nsubj:pass\"]:\n",
        "                subj.append(token.text)\n",
        "    return subj, pred\n",
        "for text in english_sentences:\n",
        "    doc = nlp_en(text)\n",
        "    subject, predicate = find_subj_pred_eng(doc)\n",
        "    print(f\"Предложение: {text}\")\n",
        "    print(f\"Подлежащее: {subject}, Сказуемое: {predicate}\")\n",
        "    print()\n",
        "\n",
        "print(\"Отношение прилагательное-существительное в русских предложениях:\")\n",
        "for sentence in russian_sentences:\n",
        "    doc_ru = nlp_ru(sentence)\n",
        "    for token in doc_ru:\n",
        "        if token.dep_ == \"amod\":\n",
        "            print(f\"Объект '{token.text}' зависит от существительного '{token.head.text}'\")\n",
        "\n",
        "print(\"\\nОтношение прилагательное-существительное в английских предложениях:\")\n",
        "for sentence in english_sentences:\n",
        "    doc_en = nlp_en(sentence)\n",
        "    for token in doc_en:\n",
        "        if token.dep_ == \"amod\":\n",
        "            print(f\"Объект '{token.text}' зависит от существительного '{token.head.text}'\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz9fITygOcK7"
      },
      "source": [
        "**Задание 4: Распознавание именованных сущностей (Named Entity Recognition)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bBTyWvAOojo"
      },
      "source": [
        "1. Подготовьте корпус из 10 новостных текстов, содержащий различные типы именованных сущностей (имена людей, организации, географические названия, даты и т.д.) на английском или русском языке.\n",
        "2. Используйте SpaCy для автоматического распознавания именованных сущностей.\n",
        "3. Реализуйте свой простой метод для распознавания имен людей и географических названий с помощью регулярных выражений и словарей.\n",
        "4. Сравните результаты работы SpaCy и вашего метода:\n",
        "* Рассчитайте точность (precision), полноту (recall) и F1-меру для вашего метода относительно результатов SpaCy\n",
        "* Проанализируйте ошибки обоих подходов, какие типы ошибок характерны для каждого подхода\n",
        "5. Представьте сравнение результатов в виде таблицы"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "HFxhvnXJPI4d"
      },
      "outputs": [],
      "source": [
        "documents = [\n",
        "    \"Friends and Family of Valentino Bid Farewell to Fashion Giant. Famous faces from the worlds of fashion and film descended on Rome on Friday to bid farewell to legendary Italian designer Valentino Garavani, who died on Monday at 93.\"\n",
        "    \"For his latest tour, Harry Styles is coming to Madison Square Garden. A lot. Styles, who is set to release his fourth solo album, “Kiss All the Time. Disco, Occasionally.,” on March 6, will perform a 30-show residency at the Garden, appearing every Wednesday, Friday and Saturday from Aug. 26 to Oct. 31, Live Nation, the tour’s promoter, announced on Thursday. The residency will be the only United States dates this year on Styles’s 50-stop world tour, called Together, Together, which is set to begin in Amsterdam in May. Information about ticket sales is available on Styles’s website. The full tour involves shows in seven cities, also including São Paulo, Mexico City, Melbourne, Sydney and London, where he will play six nights at Wembley Stadium. Opening acts, which vary by city, include Jamie xx, Robyn and Shania Twain.\"\n",
        "    \"ASAP Rocky Grew Up, Settled Down (with Rihanna) and Returned to Rap. The artist reflects on how the chaotic eight years since his last release — including three kids and two trials — led to his latest album, Don’t Be Dumb. In the time since ASAP Rocky released his last album, “Testing,” in 2018, he’s acted in two Hollywood movies, become romantic partners and a father of three with Rihanna, faced two criminal prosecutions and seen dozens of his unfinished songs let loose onto the internet by leakers. It has been a sometimes agonizing, sometimes thrilling stretch of time defined by upheaval, and it sometimes seemed like Rocky might never put out an album again. “I never accepted that,” he said on Popcast, The New York Times pop culture show, in an interview last week. “That’s my core. I consider myself a Renaissance man, so it starts with music.” His fourth album, “Don’t Be Dumb,” released Friday, is a return to some of Rocky’s familiar musical influences, with gothic takes on Southern rap styles (“Playa,” “Helicopter”) and experimental production that dabbles in cloudy abstraction and psychedelic rock (“Punk Rocky”). Typical of the rapper’s preferred eclecticism, the album features appearances by rappers like Doechii and Tyler, the Creator, alongside vocals from the folk singer Jessica Pratt and Damon Albarn.\"\n",
        "    \"On the cover of Danny Brown’s new album, “Stardust,” the rapper’s face appears in shadow, but his well-defined abs are clearly visible. Is the 44-year-old excited to show off the results of his hard work at the gym? “Totally, totally,” he said. In 2023, Brown entered rehab, and he’s eager to discuss sobriety’s positive impact on his physical and mental health, his relationships and his artmaking process. Brown is a Detroit native with a classicist’s appreciation for language, but an ear for beats that are anything but traditional. Years ago he heard the glitchy sounds of 100 gecs and thought that he could rap over that, or something like it, and began to explore the experimental pop scene. His collaborators on his sixth album — including the genre-resistant New Jersey radical Jane Remover, the sibling duo Frost Children and the alt-rock-adjacent artist Underscores — have little history of working with a rapper like Brown. They all joined a group chat where he would share Dropbox links to in-progress material. His only prompt: “What would you want a Danny Brown song to sound like in the mix of these?”\"\n",
        "    \"If the first two episodes of “The End of an Era” — the documentary series about Taylor Swift’s record-breaking Eras Tour — were defined by how Swift weathered the brutal moments that befell her road show, the final four are marked by the pop star’s joy. The last batch of episodes, now all available to stream on Disney+, chart how Swift rebounded from not one but two heartbreaks and embarked on a relationship with her fiancé, Travis Kelce. Along the way, she recorded yet another album in secret and completed the record-breaking concert extravaganza. Though the Kelce details are perhaps the most tantalizing, Swift also offers more insight into the complexities and challenges of mounting a tour of that scale.\"\n",
        "    \"Brazil’s Latest Icon Is New to Film. Now, She’s a Star of an Oscar Contender. Clad in florals and puffing on a cigarette, Tânia Maria has captivated audiences with a striking, if brief, performance in “The Secret Agent,” Brazil’s latest Oscar nominee.\"\n",
        "    \"Despite Trump’s Words, China and Russia Are Not Threatening Greenland U.S. and European officials say they are unaware of any intelligence that shows China and Russia are endangering the island, which is protected by the NATO security umbrella.\"\n",
        "    \"McLaren awarded more than $12m in legal case against IndyCar champion Alex Palou. As previously reported by The Athletic, the dispute dates back to October 2022, when Palou agreed to sign for McLaren’s IndyCar team, Arrow McLaren. Under that deal, it was agreed Palou would take up a position as McLaren’s reserve Formula 1 driver in 2023, with the possibility of promotion, before racing for their IndyCar team across the 2024, 2025 and 2026 seasons. He would also continue to drive for his existing Chip Ganassi Racing (CGR) team in the 2023 season. However, in August 2023, Palou opted against making the move, instead choosing to stay with CGR. McLaren later sued Palou, claiming in excess of $20m in losses, according to Friday’s judgement. In total, McLaren was awarded $10.2m, with a further amount of between $2m and $2.5m dependent on the outcome of expert evidence.\"\n",
        "    \"The K-pop band NewJeans plunged deeper into turmoil this week after its management label said it had dropped one member of the five-person girl group, one of the genre’s most influential acts in recent years. The announcement on Monday by the group’s label, Ador, that it was terminating the contract of Danielle Marsh, who performs as Danielle, is the latest flashpoint in a more than yearlong legal battle between the label and the band. The label did not provide a reason for dropping Danielle. It said that three NewJeans members — Haerin, Hyein and Hanni — would continue working while it continued “discussions” with Minji, a fourth member. Neither Danielle nor Minji responded immediately to requests for comment on Tuesday. It is unclear when or whether NewJeans will record music or perform as a group again. Even though K-pop labels often let go of artists, Danielle’s dismissal has prompted intense scrutiny and news coverage in South Korea. That’s because it illustrates the complexities of management structures and contracts in a largely top-down industry that is highly restrictive and led by a handful of powerful entertainment conglomerates.\"\n",
        "    \"Rebecca Hall had never worked with Ryan Murphy when he pitched her what she called “the most wild and fun and subversive wacky premise” for a new show titled The Beauty. Hall admitted to existing in a realm of “permanent tension” between her desire to age gracefully and naturally, and appreciating that she works in an industry where how she looks matters. In a video call from her home in upstate New York, Hall spoke about her mother’s piano, the brown and green cocktails her husband Morgan Spector makes, and things that go bump in the night. These are edited excerpts from the conversation.\"\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_md\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Сущности, которые распознал каждый метод:\n",
            "\n",
            "\n",
            "================================================================================\n",
            "Документ 1:\n",
            "Friends and Family of Valentino Bid Farewell to Fashion Giant. Famous faces from the worlds of fashion and film descended on Rome on Friday to bid far...\n",
            "\n",
            "SpaCy (166 сущностей):\n",
            "  Rome                                     [GPE]\n",
            "  Friday                                   [DATE]\n",
            "  Italian                                  [NORP]\n",
            "  Valentino Garavani                       [PERSON]\n",
            "  Monday                                   [DATE]\n",
            "  93.For                                   [DATE]\n",
            "  Harry Styles                             [PERSON]\n",
            "  Madison Square Garden                    [FAC]\n",
            "  fourth                                   [ORDINAL]\n",
            "  Kiss All the Time                        [WORK_OF_ART]\n",
            "  Disco                                    [PERSON]\n",
            "  March 6                                  [DATE]\n",
            "  30                                       [CARDINAL]\n",
            "  Wednesday, Friday and                    [DATE]\n",
            "  Saturday                                 [DATE]\n",
            "  Aug. 26 to Oct. 31                       [DATE]\n",
            "  Live Nation                              [ORG]\n",
            "  Thursday                                 [DATE]\n",
            "  United States                            [GPE]\n",
            "  this year                                [DATE]\n",
            "  Styles                                   [ORG]\n",
            "  50                                       [CARDINAL]\n",
            "  Amsterdam                                [GPE]\n",
            "  May                                      [DATE]\n",
            "  Styles                                   [ORG]\n",
            "  seven                                    [CARDINAL]\n",
            "  São Paulo                                [PERSON]\n",
            "  Mexico City                              [GPE]\n",
            "  Melbourne                                [GPE]\n",
            "  Sydney                                   [GPE]\n",
            "  London                                   [GPE]\n",
            "  six nights                               [DATE]\n",
            "  Wembley Stadium                          [FAC]\n",
            "  Jamie xx                                 [PERSON]\n",
            "  Robyn                                    [PERSON]\n",
            "  Shania Twain                             [PERSON]\n",
            "  Rihanna                                  [PERSON]\n",
            "  eight years                              [DATE]\n",
            "  three                                    [CARDINAL]\n",
            "  two                                      [CARDINAL]\n",
            "  ASAP Rocky                               [ORG]\n",
            "  2018                                     [DATE]\n",
            "  two                                      [CARDINAL]\n",
            "  Hollywood                                [GPE]\n",
            "  three                                    [CARDINAL]\n",
            "  Rihanna                                  [PERSON]\n",
            "  two                                      [CARDINAL]\n",
            "  dozens                                   [CARDINAL]\n",
            "  Rocky                                    [ORG]\n",
            "  Popcast                                  [ORG]\n",
            "  The New York Times                       [ORG]\n",
            "  last week                                [DATE]\n",
            "  Renaissance                              [ORG]\n",
            "  fourth                                   [ORDINAL]\n",
            "  Friday                                   [DATE]\n",
            "  Rocky                                    [ORG]\n",
            "  Southern                                 [NORP]\n",
            "  Playa                                    [WORK_OF_ART]\n",
            "  Punk Rocky                               [WORK_OF_ART]\n",
            "  Doechii                                  [PERSON]\n",
            "  Tyler                                    [PERSON]\n",
            "  Jessica Pratt                            [PERSON]\n",
            "  Damon Albarn                             [PERSON]\n",
            "  Danny Brown’s                            [PERSON]\n",
            "  Stardust                                 [PERSON]\n",
            "  44-year-old                              [DATE]\n",
            "  2023                                     [CARDINAL]\n",
            "  Brown                                    [PERSON]\n",
            "  Brown                                    [PERSON]\n",
            "  Detroit                                  [GPE]\n",
            "  Years ago                                [DATE]\n",
            "  100                                      [CARDINAL]\n",
            "  sixth                                    [ORDINAL]\n",
            "  New Jersey                               [GPE]\n",
            "  Jane Remover                             [PERSON]\n",
            "  Frost Children                           [ORG]\n",
            "  Brown                                    [PERSON]\n",
            "  Dropbox                                  [ORG]\n",
            "  Danny Brown                              [PERSON]\n",
            "  first                                    [ORDINAL]\n",
            "  two                                      [CARDINAL]\n",
            "  The End of an Era                        [WORK_OF_ART]\n",
            "  Taylor Swift                             [PERSON]\n",
            "  Eras Tour                                [ORG]\n",
            "  Swift                                    [ORG]\n",
            "  four                                     [CARDINAL]\n",
            "  Swift                                    [ORG]\n",
            "  one                                      [CARDINAL]\n",
            "  two                                      [CARDINAL]\n",
            "  Travis Kelce                             [PERSON]\n",
            "  Kelce                                    [PERSON]\n",
            "  Swift                                    [ORG]\n",
            "  Brazil                                   [GPE]\n",
            "  Latest Icon                              [ORG]\n",
            "  Tânia Maria                              [PERSON]\n",
            "  The Secret Agent                         [WORK_OF_ART]\n",
            "  Brazil                                   [GPE]\n",
            "  Oscar                                    [PERSON]\n",
            "  Trump’s Words                            [ORG]\n",
            "  China                                    [GPE]\n",
            "  Russia                                   [GPE]\n",
            "  U.S.                                     [GPE]\n",
            "  European                                 [NORP]\n",
            "  China                                    [GPE]\n",
            "  Russia                                   [GPE]\n",
            "  NATO                                     [ORG]\n",
            "  McLaren                                  [PERSON]\n",
            "  more than $12                            [MONEY]\n",
            "  IndyCar                                  [ORG]\n",
            "  Alex Palou                               [PERSON]\n",
            "  Athletic                                 [ORG]\n",
            "  October 2022                             [DATE]\n",
            "  Palou                                    [PERSON]\n",
            "  McLaren                                  [PERSON]\n",
            "  IndyCar                                  [ORG]\n",
            "  Arrow McLaren                            [PERSON]\n",
            "  Palou                                    [GPE]\n",
            "  McLaren                                  [PERSON]\n",
            "  Formula 1                                [PRODUCT]\n",
            "  2023                                     [CARDINAL]\n",
            "  IndyCar                                  [ORG]\n",
            "  2024                                     [CARDINAL]\n",
            "  2025                                     [DATE]\n",
            "  2026 seasons                             [DATE]\n",
            "  Chip Ganassi Racing                      [PERSON]\n",
            "  CGR                                      [ORG]\n",
            "  the 2023 season                          [DATE]\n",
            "  August 2023                              [DATE]\n",
            "  Palou                                    [GPE]\n",
            "  CGR                                      [ORG]\n",
            "  McLaren                                  [PERSON]\n",
            "  Palou                                    [GPE]\n",
            "  20                                       [MONEY]\n",
            "  Friday                                   [DATE]\n",
            "  McLaren                                  [PERSON]\n",
            "  $10.2m                                   [MONEY]\n",
            "  between $2m                              [MONEY]\n",
            "  2.5                                      [MONEY]\n",
            "  NewJeans                                 [ORG]\n",
            "  this week                                [DATE]\n",
            "  one                                      [CARDINAL]\n",
            "  five                                     [CARDINAL]\n",
            "  one                                      [CARDINAL]\n",
            "  recent years                             [DATE]\n",
            "  Monday                                   [DATE]\n",
            "  Danielle Marsh                           [ORG]\n",
            "  Danielle                                 [GPE]\n",
            "  Danielle                                 [PERSON]\n",
            "  three                                    [CARDINAL]\n",
            "  NewJeans                                 [ORG]\n",
            "  Haerin                                   [NORP]\n",
            "  Hyein                                    [PERSON]\n",
            "  Hanni                                    [PERSON]\n",
            "  Minji                                    [PERSON]\n",
            "  fourth                                   [ORDINAL]\n",
            "  Danielle                                 [PERSON]\n",
            "  Minji                                    [PERSON]\n",
            "  Tuesday                                  [DATE]\n",
            "  NewJeans                                 [ORG]\n",
            "  Danielle                                 [ORG]\n",
            "  South Korea                              [GPE]\n",
            "  Rebecca Hall                             [PERSON]\n",
            "  Ryan Murphy                              [PERSON]\n",
            "  The Beauty.                              [WORK_OF_ART]\n",
            "  New York                                 [GPE]\n",
            "  Morgan Spector                           [ORG]\n",
            "\n",
            "Простой метод (90 сущностей):\n",
            "  ASAP Rocky                               [PERSON]\n",
            "  Valentino Garavani                       [PERSON]\n",
            "  Harry Styles                             [PERSON]\n",
            "  Shania Twain                             [PERSON]\n",
            "  Jane Remover                             [PERSON]\n",
            "  Danny Brown                              [PERSON]\n",
            "  Frost Children                           [PERSON]\n",
            "  Underscores                              [PERSON]\n",
            "  Taylor Swift                             [PERSON]\n",
            "  Travis Kelce                             [PERSON]\n",
            "  Tânia Maria                              [PERSON]\n",
            "  Trump                                    [PERSON]\n",
            "  NATO                                     [PERSON]\n",
            "  Arrow McLaren                            [PERSON]\n",
            "  Alex Palou                               [PERSON]\n",
            "  Danielle Marsh                           [PERSON]\n",
            "  Ador                                     [PERSON]\n",
            "  Haerin                                   [PERSON]\n",
            "  Hyein                                    [PERSON]\n",
            "  Hanni                                    [PERSON]\n",
            "  Minji                                    [PERSON]\n",
            "  Rebecca Hall                             [PERSON]\n",
            "  Ryan Murphy                              [PERSON]\n",
            "  Morgan Spector                           [PERSON]\n",
            "  Rihanna                                  [PERSON]\n",
            "  Doechii                                  [PERSON]\n",
            "  Tyler, the Creator                       [PERSON]\n",
            "  Jessica Pratt                            [PERSON]\n",
            "  Damon Albarn                             [PERSON]\n",
            "  New York                                 [LOC]\n",
            "  China                                    [LOC]\n",
            "  Russia                                   [LOC]\n",
            "  South Korea                              [LOC]\n",
            "  Detroit                                  [LOC]\n",
            "  New Jersey                               [LOC]\n",
            "  São Paulo                                [LOC]\n",
            "  Mexico City                              [LOC]\n",
            "  Melbourne                                [LOC]\n",
            "  Sydney                                   [LOC]\n",
            "  London                                   [LOC]\n",
            "  Brazil                                   [LOC]\n",
            "  Greenland                                [LOC]\n",
            "  United States                            [LOC]\n",
            "  Amsterdam                                [LOC]\n",
            "  Valentino Bid                            [PERSON]\n",
            "  Fashion Giant                            [PERSON]\n",
            "  Madison Square                           [PERSON]\n",
            "  Kiss All                                 [PERSON]\n",
            "  Live Nation                              [PERSON]\n",
            "  Wembley Stadium                          [PERSON]\n",
            "  Rocky Grew                               [PERSON]\n",
            "  Settled Down                             [PERSON]\n",
            "  Be Dumb                                  [PERSON]\n",
            "  The New                                  [PERSON]\n",
            "  York Times                               [PERSON]\n",
            "  Punk Rocky                               [PERSON]\n",
            "  The End                                  [PERSON]\n",
            "  Eras Tour                                [PERSON]\n",
            "  Latest Icon                              [PERSON]\n",
            "  Is New                                   [PERSON]\n",
            "  Oscar Contender                          [PERSON]\n",
            "  The Secret                               [PERSON]\n",
            "  Despite Trump                            [PERSON]\n",
            "  Russia Are                               [PERSON]\n",
            "  Not Threatening                          [PERSON]\n",
            "  The Athletic                             [PERSON]\n",
            "  Chip Ganassi                             [PERSON]\n",
            "  Neither Danielle                         [PERSON]\n",
            "  The Beauty                               [PERSON]\n",
            "  Valentino Bid Farewell                   [PERSON]\n",
            "  Madison Square Garden                    [PERSON]\n",
            "  Rocky Grew Up                            [PERSON]\n",
            "  The New York                             [PERSON]\n",
            "  Latest Icon Is                           [PERSON]\n",
            "  The Secret Agent                         [PERSON]\n",
            "  Russia Are Not                           [PERSON]\n",
            "  Chip Ganassi Racing                      [PERSON]\n",
            "  to Fashion                               [LOC]\n",
            "  to Madison                               [LOC]\n",
            "  from Aug                                 [LOC]\n",
            "  to Oct                                   [LOC]\n",
            "  in Amsterdam                             [LOC]\n",
            "  in May                                   [LOC]\n",
            "  at Wembley                               [LOC]\n",
            "  to Rap                                   [LOC]\n",
            "  to Film                                  [LOC]\n",
            "  to October                               [LOC]\n",
            "  in August                                [LOC]\n",
            "  to Friday                                [LOC]\n",
            "  in South                                 [LOC]\n",
            "\n",
            "Метрики для документа 1:\n",
            "  Precision: 0.522\n",
            "  Recall: 0.373\n",
            "  F1: 0.435\n",
            "  Совпадения (TP): 47\n",
            "  Ложные срабатывания (FP): 43\n",
            "  Пропущенные (FN): 79\n",
            "\n",
            "================================================================================\n",
            "Общая таблица полученных результатов:\n",
            "\n",
            "     Документ Precision Recall    F1  TP  FP  FN  Сущностей (spaCy)  Сущностей (простой)\n",
            "            1     0.522  0.373 0.435  47  43  79                166                   90\n",
            "Среднее/Итого     0.522  0.373 0.435  47  43  79                166                   90\n"
          ]
        }
      ],
      "source": [
        "def extract_spacy_entities(text):\n",
        "    doc = nlp(text)\n",
        "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "    return entities\n",
        "\n",
        "def simple_ner(text):\n",
        "    entities = []\n",
        "    known_names = [\n",
        "        'ASAP Rocky', 'Valentino Garavani', 'Harry Styles',\n",
        "        'Shania Twain', 'Jane Remover', 'Danny Brown',\n",
        "        'Frost Children', 'Underscores', 'Taylor Swift',\n",
        "        'Travis Kelce', 'Tânia Maria', 'Trump',\n",
        "        'NATO', 'Arrow McLaren', 'Alex Palou',\n",
        "        'New Jeans', 'Danielle Marsh', 'Ador',\n",
        "        'Haerin', 'Hyein', 'Hanni',\n",
        "        'Minji', 'Rebecca Hall', 'Ryan Murphy',\n",
        "        'Morgan Spector', 'Travis Kelce', 'Rihanna', 'Doechii',\n",
        "        'Tyler, the Creator', 'Jessica Pratt', 'Damon Albarn'\n",
        "    ]\n",
        "    known_locations = [\n",
        "        'New York', 'China', 'Russia', 'South Korea',\n",
        "        'Detroit', 'New Jersey', 'São Paulo', 'Mexico City',\n",
        "        'Melbourne', 'Sydney', 'London', 'Brazil', 'Greenland',\n",
        "        'United States', 'Amsterdam'\n",
        "    ]\n",
        "\n",
        "    name_patterns = [\n",
        "        r'\\b([A-Z][a-z]+\\s+[A-Z][a-z]+)\\b',\n",
        "        r'\\b([A-Z][a-z]+\\s+[A-Z]\\.\\s+[A-Z][a-z]+)\\b',\n",
        "        r'\\b([A-Z][a-z]+\\s+[A-Z][a-z]+\\s+[A-Z][a-z]+)\\b'\n",
        "    ]\n",
        "\n",
        "    location_patterns = [\n",
        "        r'\\b([A-Z][a-z]+\\s+[A-Z][a-z]+)\\b',\n",
        "        r'\\b(?:in|at|from|to)\\s+([A-Z][a-z]+)\\b',\n",
        "        r'\\b([A-Z][a-z]+\\s+(?:City|State|County|Island|River|Lake|Mount))\\b'\n",
        "    ]\n",
        "\n",
        "    for name in known_names:\n",
        "        if name in text:\n",
        "            entities.append((name, 'PERSON'))\n",
        "\n",
        "    for location in known_locations:\n",
        "        if location in text:\n",
        "            entities.append((location, 'LOC'))\n",
        "\n",
        "    for pattern in name_patterns:\n",
        "        for match in re.finditer(pattern, text):\n",
        "            name = match.group()\n",
        "            if name not in [e[0] for e in entities]:\n",
        "                entities.append((name, 'PERSON'))\n",
        "\n",
        "    for pattern in location_patterns:\n",
        "        for match in re.finditer(pattern, text):\n",
        "            location = match.group()\n",
        "            if location not in [e[0] for e in entities]:\n",
        "                entities.append((location, 'LOC'))\n",
        "\n",
        "    unique_entities = []\n",
        "    seen = set()\n",
        "    for ent in entities:\n",
        "        if ent[0] not in seen:\n",
        "            unique_entities.append(ent)\n",
        "            seen.add(ent[0])\n",
        "\n",
        "    return unique_entities\n",
        "\n",
        "def calculate_metrics(predicted, reference):\n",
        "    predicted_set = set([entity[0].lower() for entity in predicted])\n",
        "    reference_set = set([entity[0].lower() for entity in reference])\n",
        "\n",
        "    true_positives = len(predicted_set.intersection(reference_set))\n",
        "    false_positives = len(predicted_set - reference_set)\n",
        "    false_negatives = len(reference_set - predicted_set)\n",
        "\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    return precision, recall, f1, true_positives, false_positives, false_negatives\n",
        "\n",
        "spacy_results = []\n",
        "simple_results = []\n",
        "metrics = []\n",
        "\n",
        "print(\"\\nСущности, которые распознал каждый метод:\\n\")\n",
        "\n",
        "for i, document in enumerate(documents):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Документ {i+1}:\")\n",
        "    print(f\"{document[:150]}...\" if len(document) > 150 else document)\n",
        "\n",
        "    spacy_entities = extract_spacy_entities(document)\n",
        "    spacy_results.append(spacy_entities)\n",
        "\n",
        "    simple_entities = simple_ner(document)\n",
        "    simple_results.append(simple_entities)\n",
        "\n",
        "    print(f\"\\nSpaCy ({len(spacy_entities)} сущностей):\")\n",
        "    for entity, label in spacy_entities:\n",
        "        print(f\"  {entity:40} [{label}]\")\n",
        "\n",
        "    print(f\"\\nПростой метод ({len(simple_entities)} сущностей):\")\n",
        "    for entity, label in simple_entities:\n",
        "        print(f\"  {entity:40} [{label}]\")\n",
        "\n",
        "    precision, recall, f1, tp, fp, fn = calculate_metrics(simple_entities, spacy_entities)\n",
        "    metrics.append({\n",
        "        'precision': precision,\n",
        "        'recall': recall,\n",
        "        'f1': f1,\n",
        "        'tp': tp,\n",
        "        'fp': fp,\n",
        "        'fn': fn\n",
        "    })\n",
        "\n",
        "    print(f\"\\nМетрики для документа {i+1}:\")\n",
        "    print(f\"  Precision: {precision:.3f}\")\n",
        "    print(f\"  Recall: {recall:.3f}\")\n",
        "    print(f\"  F1: {f1:.3f}\")\n",
        "    print(f\"  Совпадения (TP): {tp}\")\n",
        "    print(f\"  Ложные срабатывания (FP): {fp}\")\n",
        "    print(f\"  Пропущенные (FN): {fn}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Общая таблица полученных результатов:\\n\")\n",
        "\n",
        "table_data = []\n",
        "for i in range(len(documents)):\n",
        "    table_data.append({\n",
        "        'Документ': i+1,\n",
        "        'Precision': f\"{metrics[i]['precision']:.3f}\",\n",
        "        'Recall': f\"{metrics[i]['recall']:.3f}\",\n",
        "        'F1': f\"{metrics[i]['f1']:.3f}\",\n",
        "        'TP': metrics[i]['tp'],\n",
        "        'FP': metrics[i]['fp'],\n",
        "        'FN': metrics[i]['fn'],\n",
        "        'Сущностей (spaCy)': len(spacy_results[i]),\n",
        "        'Сущностей (простой)': len(simple_results[i])\n",
        "    })\n",
        "\n",
        "avg_precision = sum(m['precision'] for m in metrics) / len(metrics)\n",
        "avg_recall = sum(m['recall'] for m in metrics) / len(metrics)\n",
        "avg_f1 = sum(m['f1'] for m in metrics) / len(metrics)\n",
        "total_tp = sum(m['tp'] for m in metrics)\n",
        "total_fp = sum(m['fp'] for m in metrics)\n",
        "total_fn = sum(m['fn'] for m in metrics)\n",
        "total_spacy = sum(len(r) for r in spacy_results)\n",
        "total_simple = sum(len(r) for r in simple_results)\n",
        "\n",
        "table_data.append({\n",
        "    'Документ': 'Среднее/Итого',\n",
        "    'Precision': f\"{avg_precision:.3f}\",\n",
        "    'Recall': f\"{avg_recall:.3f}\",\n",
        "    'F1': f\"{avg_f1:.3f}\",\n",
        "    'TP': total_tp,\n",
        "    'FP': total_fp,\n",
        "    'FN': total_fn,\n",
        "    'Сущностей (spaCy)': total_spacy,\n",
        "    'Сущностей (простой)': total_simple\n",
        "})\n",
        "\n",
        "df = pd.DataFrame(table_data)\n",
        "print(df.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
